{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fy0Rq7oGkzJ7",
        "outputId": "93e44db6-03d8-419f-9149-12119e80bd96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Video Processing Pipeline ===\n",
            "This code downloads videos from hugging face, processes them, and creates training datasets\n",
            "Step 1: Installing required dependencies...\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Checking required packages:\n",
            "âœ… cv2\n",
            "âœ… mediapipe\n",
            "âœ… librosa\n",
            "âœ… moviepy\n",
            "âœ… numpy\n",
            "âœ… json\n",
            "âœ… tqdm\n",
            "âœ… pathlib\n",
            "âœ… matplotlib\n",
            "âœ… huggingface_hub\n",
            "âœ… pandas\n",
            "âœ… python-dotenv\n",
            "\n",
            "ğŸ‰ All dependencies are ready!\n"
          ]
        }
      ],
      "source": [
        "# Hugging Face Video Processing Pipeline for Neural Network Training Dataset Creation\n",
        "# This notebook downloads videos from hugging face, processes them through the complete pipeline, and creates training datasets\n",
        "\n",
        "print(\"=== Video Processing Pipeline ===\")\n",
        "print(\"This code downloads videos from hugging face, processes them, and creates training datasets\")\n",
        "print(\"Step 1: Installing required dependencies...\")\n",
        "\n",
        "# Install yt-dlp for YouTube downloading\n",
        "%pip install yt-dlp --quiet\n",
        "%pip install python-dotenv --quiet\n",
        "\n",
        "# Check if all required libraries are available\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required_packages = [\n",
        "    #  we dont need yt-dlp for hugging face\n",
        "    'cv2', 'mediapipe', 'librosa', 'moviepy', \n",
        "    'numpy', 'json', 'tqdm', 'pathlib', 'matplotlib',\n",
        "    'huggingface_hub' , 'pandas' , 'python-dotenv'\n",
        "]\n",
        "\n",
        "print(\"\\nChecking required packages:\")\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        if package == 'python-dotenv':\n",
        "            __import__(\"dotenv\")\n",
        "        else:\n",
        "            __import__(package)\n",
        "        print(f\"âœ… {package}\")\n",
        "    except ImportError:\n",
        "        print(f\"âŒ {package} - has  not been installed please install it first\")\n",
        "\n",
        "print(\"\\nğŸ‰ All dependencies are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded all the URLs\n"
          ]
        }
      ],
      "source": [
        "# HUgging face Dataset URLs\n",
        "\n",
        "huggingface_test_dataset_urls = [\n",
        "    {\"name\":\"test_video\" , \"url\": \"https://huggingface.co/datasets/sanjuhs/test-video-dataset\"},\n",
        "]\n",
        "\n",
        "huggingface_actual_dataset_urls = [\n",
        "    {\"name\":\"ml_sanjay_5_15min_datasets\" , \"url\": \"https://huggingface.co/datasets/sanjuhs/ml_video_dataset\"},\n",
        "\n",
        "]\n",
        "\n",
        "# these are the  corresponding youtube URLs, the same has been uploaded to hugging face\n",
        "test_urls = [ {\"name\":\"test_video\" , \"url\": \"https://youtu.be/1wO0Rx9REAA\"}]\n",
        "\n",
        "actual_urls = [\n",
        "    {\"name\":\"test_video\" , \"url\": \"https://youtu.be/1wO0Rx9REAA\"},\n",
        "    {\"name\":\"ml_sanjay_assortmentSounds55_15min_dataset\" , \"url\": \"https://youtu.be/W_-ZsKm_MZc\"},\n",
        "    {\"name\":\"ml_sanjay_frutratedAngry_15min_dataset\" , \"url\": \"https://youtu.be/OnrWa45WCdI\"},\n",
        "    {\"name\":\"ml_sanjay_explanationRnn_15min_dataset\" , \"url\": \"https://youtu.be/iqrOrChYwlw\"},\n",
        "    {\"name\":\"ml_sanjay_commentary_15min_dataset\" , \"url\": \"https://youtu.be/mu3StD1YpR4\"},\n",
        "    {\"name\":\"ml_sanjay_calmSinging_15min_dataset\" , \"url\": \"https://youtu.be/V1xG5qYp98U\"},\n",
        "    ]\n",
        "\n",
        "print(\"loaded all the URLs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing Hugging Face Hub library...\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "ğŸš€ Initializing Hugging Face Dataset Downloader...\n",
            "ğŸ¤— Hugging Face Dataset Downloader initialized\n",
            "Download directory: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/hf_datasets\n",
            "\n",
            "ğŸ“‹ Available datasets:\n",
            "Test datasets:\n",
            "  - test_video: https://huggingface.co/datasets/sanjuhs/test-video-dataset\n",
            "\n",
            "Actual datasets:\n",
            "  - ml_sanjay_5_15min_datasets: https://huggingface.co/datasets/sanjuhs/ml_video_dataset\n",
            "\n",
            "âœ… Hugging Face downloader ready!\n"
          ]
        }
      ],
      "source": [
        "# Hugging Face Dataset Downloader\n",
        "# Install required dependencies for Hugging Face datasets\n",
        "\n",
        "print(\"Installing Hugging Face Hub library...\")\n",
        "%pip install huggingface_hub --quiet\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download, list_repo_files\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "class HuggingFaceDatasetDownloader:\n",
        "    def __init__(self, work_dir=\"hf_downloads\"):\n",
        "        \"\"\"\n",
        "        Initialize the Hugging Face dataset downloader\n",
        "        \n",
        "        Args:\n",
        "            work_dir: Directory to download datasets to\n",
        "        \"\"\"\n",
        "        self.work_dir = Path(work_dir)\n",
        "        self.work_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        print(f\"ğŸ¤— Hugging Face Dataset Downloader initialized\")\n",
        "        print(f\"Download directory: {self.work_dir.absolute()}\")\n",
        "    \n",
        "    def get_dataset_info(self, repo_id):\n",
        "        \"\"\"\n",
        "        Get information about a Hugging Face dataset\n",
        "        \n",
        "        Args:\n",
        "            repo_id: Repository ID (e.g., \"sanjuhs/test-video-dataset\")\n",
        "        \n",
        "        Returns:\n",
        "            dict: Dataset information\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get list of files in the repository\n",
        "            files = list_repo_files(repo_id, repo_type=\"dataset\")\n",
        "            \n",
        "            print(f\"ğŸ“‹ Dataset: {repo_id}\")\n",
        "            print(f\"ğŸ“ Files found: {len(files)}\")\n",
        "            \n",
        "            # Categorize files by type\n",
        "            video_files = [f for f in files if f.endswith(('.mp4', '.webm', '.mkv', '.avi'))]\n",
        "            json_files = [f for f in files if f.endswith('.json')]\n",
        "            other_files = [f for f in files if not f.endswith(('.mp4', '.webm', '.mkv', '.avi', '.json'))]\n",
        "            \n",
        "            info = {\n",
        "                'repo_id': repo_id,\n",
        "                'total_files': len(files),\n",
        "                'video_files': video_files,\n",
        "                'json_files': json_files,\n",
        "                'other_files': other_files,\n",
        "                'all_files': files\n",
        "            }\n",
        "            \n",
        "            print(f\"ğŸ¥ Video files: {len(video_files)}\")\n",
        "            print(f\"ğŸ“„ JSON files: {len(json_files)}\")\n",
        "            print(f\"ğŸ“ Other files: {len(other_files)}\")\n",
        "            \n",
        "            return info\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error getting dataset info: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def download_dataset_files(self, repo_id, dataset_name, file_types=['video', 'json']):\n",
        "        \"\"\"\n",
        "        Download specific file types from a Hugging Face dataset\n",
        "        \n",
        "        Args:\n",
        "            repo_id: Repository ID (e.g., \"sanjuhs/test-video-dataset\")\n",
        "            dataset_name: Local name for the dataset\n",
        "            file_types: List of file types to download ('video', 'json', 'all')\n",
        "        \n",
        "        Returns:\n",
        "            dict: Download results\n",
        "        \"\"\"\n",
        "        print(f\"\\nğŸ“¥ Downloading dataset: {repo_id}\")\n",
        "        \n",
        "        # Create dataset directory\n",
        "        dataset_dir = self.work_dir / dataset_name\n",
        "        dataset_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Get dataset info\n",
        "        dataset_info = self.get_dataset_info(repo_id)\n",
        "        if not dataset_info:\n",
        "            return None\n",
        "        \n",
        "        downloaded_files = []\n",
        "        failed_files = []\n",
        "        \n",
        "        # Determine which files to download\n",
        "        files_to_download = []\n",
        "        \n",
        "        if 'all' in file_types:\n",
        "            files_to_download = dataset_info['all_files']\n",
        "        else:\n",
        "            if 'video' in file_types:\n",
        "                files_to_download.extend(dataset_info['video_files'])\n",
        "            if 'json' in file_types:\n",
        "                files_to_download.extend(dataset_info['json_files'])\n",
        "            if 'other' in file_types:\n",
        "                files_to_download.extend(dataset_info['other_files'])\n",
        "        \n",
        "        print(f\"ğŸ“¦ Downloading {len(files_to_download)} files...\")\n",
        "        \n",
        "        # Download each file\n",
        "        for filename in tqdm(files_to_download, desc=\"Downloading files\"):\n",
        "            try:\n",
        "                print(f\"  ğŸ“¥ Downloading: {filename}\")\n",
        "                \n",
        "                # Download file to dataset directory\n",
        "                local_path = hf_hub_download(\n",
        "                    repo_id=repo_id,\n",
        "                    filename=filename,\n",
        "                    repo_type=\"dataset\",\n",
        "                    local_dir=dataset_dir,\n",
        "                    local_dir_use_symlinks=False\n",
        "                )\n",
        "                \n",
        "                downloaded_files.append({\n",
        "                    'filename': filename,\n",
        "                    'local_path': local_path,\n",
        "                    'size_mb': os.path.getsize(local_path) / (1024 * 1024)\n",
        "                })\n",
        "                \n",
        "                print(f\"    âœ… Downloaded to: {local_path}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    âŒ Failed to download {filename}: {e}\")\n",
        "                failed_files.append({'filename': filename, 'error': str(e)})\n",
        "        \n",
        "        # Create download summary\n",
        "        results = {\n",
        "            'repo_id': repo_id,\n",
        "            'dataset_name': dataset_name,\n",
        "            'dataset_dir': str(dataset_dir),\n",
        "            'total_files_requested': len(files_to_download),\n",
        "            'downloaded_files': downloaded_files,\n",
        "            'failed_files': failed_files,\n",
        "            'success_count': len(downloaded_files),\n",
        "            'failure_count': len(failed_files),\n",
        "            'total_size_mb': sum([f['size_mb'] for f in downloaded_files])\n",
        "        }\n",
        "        \n",
        "        # Save download summary\n",
        "        summary_file = dataset_dir / \"download_summary.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        \n",
        "        print(f\"\\nâœ… Download complete!\")\n",
        "        print(f\"ğŸ“ Dataset saved to: {dataset_dir}\")\n",
        "        print(f\"ğŸ“Š Downloaded: {results['success_count']}/{results['total_files_requested']} files\")\n",
        "        print(f\"ğŸ’¾ Total size: {results['total_size_mb']:.2f} MB\")\n",
        "        \n",
        "        if failed_files:\n",
        "            print(f\"âš ï¸  Failed downloads: {len(failed_files)}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def list_downloaded_datasets(self):\n",
        "        \"\"\"List all downloaded datasets\"\"\"\n",
        "        if not self.work_dir.exists():\n",
        "            print(\"No downloads directory found\")\n",
        "            return []\n",
        "        \n",
        "        datasets = []\n",
        "        for item in self.work_dir.iterdir():\n",
        "            if item.is_dir():\n",
        "                summary_file = item / \"download_summary.json\"\n",
        "                if summary_file.exists():\n",
        "                    with open(summary_file, 'r') as f:\n",
        "                        summary = json.load(f)\n",
        "                    datasets.append(summary)\n",
        "                else:\n",
        "                    # Directory exists but no summary - manual inspection\n",
        "                    files = list(item.glob(\"*\"))\n",
        "                    datasets.append({\n",
        "                        'dataset_name': item.name,\n",
        "                        'dataset_dir': str(item),\n",
        "                        'file_count': len(files),\n",
        "                        'files': [f.name for f in files]\n",
        "                    })\n",
        "        \n",
        "        return datasets\n",
        "\n",
        "# Initialize the downloader\n",
        "print(\"\\nğŸš€ Initializing Hugging Face Dataset Downloader...\")\n",
        "hf_downloader = HuggingFaceDatasetDownloader(work_dir=\"hf_datasets\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Available datasets:\")\n",
        "print(\"Test datasets:\")\n",
        "for dataset in huggingface_test_dataset_urls:\n",
        "    print(f\"  - {dataset['name']}: {dataset['url']}\")\n",
        "\n",
        "print(\"\\nActual datasets:\")  \n",
        "for dataset in huggingface_actual_dataset_urls:\n",
        "    print(f\"  - {dataset['name']}: {dataset['url']}\")\n",
        "\n",
        "print(\"\\nâœ… Hugging Face downloader ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸš€ DOWNLOADING HUGGING FACE DATASETS\n",
            "================================================================================\n",
            "\n",
            "1ï¸âƒ£ TEST DATASET:\n",
            "dataset object is {'name': 'ml_sanjay_5_15min_datasets', 'url': 'https://huggingface.co/datasets/sanjuhs/ml_video_dataset'}\n",
            "dataset name is ml_sanjay_5_15min_datasets data set url is https://huggingface.co/datasets/sanjuhs/ml_video_dataset\n",
            " Dl data sets one by one\n",
            " Now downloadingd ataset name is ml_sanjay_5_15min_datasets data set url is https://huggingface.co/datasets/sanjuhs/ml_video_dataset\n",
            "ğŸ” Getting info for test dataset: sanjuhs/ml_video_dataset\n",
            "ğŸ“‹ Dataset: sanjuhs/ml_video_dataset\n",
            "ğŸ“ Files found: 6\n",
            "ğŸ¥ Video files: 5\n",
            "ğŸ“„ JSON files: 0\n",
            "ğŸ“ Other files: 1\n",
            "dataset_info {'repo_id': 'sanjuhs/ml_video_dataset', 'total_files': 6, 'video_files': ['ml_sanjay_assortmentSounds55_15min_dataset.mp4', 'ml_sanjay_calmSinging_15min_dataset.mp4', 'ml_sanjay_commentary_15min_dataset.mp4', 'ml_sanjay_explanationRnn_15min_dataset.mp4', 'ml_sanjay_frutratedAngry_15min_dataset.mp4'], 'json_files': [], 'other_files': ['.gitattributes'], 'all_files': ['.gitattributes', 'ml_sanjay_assortmentSounds55_15min_dataset.mp4', 'ml_sanjay_calmSinging_15min_dataset.mp4', 'ml_sanjay_commentary_15min_dataset.mp4', 'ml_sanjay_explanationRnn_15min_dataset.mp4', 'ml_sanjay_frutratedAngry_15min_dataset.mp4']}\n",
            "\n",
            "ğŸ“¥ Downloading test dataset...\n",
            "\n",
            "ğŸ“¥ Downloading dataset: sanjuhs/ml_video_dataset\n",
            "ğŸ“‹ Dataset: sanjuhs/ml_video_dataset\n",
            "ğŸ“ Files found: 6\n",
            "ğŸ¥ Video files: 5\n",
            "ğŸ“„ JSON files: 0\n",
            "ğŸ“ Other files: 1\n",
            "ğŸ“¦ Downloading 6 files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ğŸ“¥ Downloading: .gitattributes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sanjayprasads/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "Downloading files:  17%|â–ˆâ–‹        | 1/6 [00:00<00:04,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/.gitattributes\n",
            "  ğŸ“¥ Downloading: ml_sanjay_assortmentSounds55_15min_dataset.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [04:17<10:04, 151.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_assortmentSounds55_15min_dataset.mp4\n",
            "  ğŸ“¥ Downloading: ml_sanjay_calmSinging_15min_dataset.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [09:00<10:35, 211.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_calmSinging_15min_dataset.mp4\n",
            "  ğŸ“¥ Downloading: ml_sanjay_commentary_15min_dataset.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [12:36<07:06, 213.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_commentary_15min_dataset.mp4\n",
            "  ğŸ“¥ Downloading: ml_sanjay_explanationRnn_15min_dataset.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [17:00<03:51, 231.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_explanationRnn_15min_dataset.mp4\n",
            "  ğŸ“¥ Downloading: ml_sanjay_frutratedAngry_15min_dataset.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [21:49<00:00, 218.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âœ… Downloaded to: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_frutratedAngry_15min_dataset.mp4\n",
            "\n",
            "âœ… Download complete!\n",
            "ğŸ“ Dataset saved to: hf_datasets/ml_sanjay_5_15min_datasets\n",
            "ğŸ“Š Downloaded: 6/6 files\n",
            "ğŸ’¾ Total size: 7677.63 MB\n",
            "\n",
            "âœ… Dataset downloaded successfully!\n",
            "ğŸ“ Location: hf_datasets/ml_sanjay_5_15min_datasets\n",
            "ğŸ“Š Files: 6 downloaded\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download and Use Hugging Face Datasets\n",
        "\n",
        "def download_dataset_hf( dataset_url ):\n",
        "    \"\"\"Download the test dataset from Hugging Face\"\"\"\n",
        "\n",
        "    for ds in dataset_url:\n",
        "        print(\"dataset object is\" , ds)\n",
        "        print(\"dataset name is\" , ds[\"name\"] , \"data set url is\" , ds[\"url\"])\n",
        "\n",
        "    for ds in dataset_url:\n",
        "        print(\" Dl data sets one by one\")\n",
        "        print(\" Now downloadingd ataset name is\" , ds[\"name\"] , \"data set url is\" , ds[\"url\"])\n",
        "    \n",
        "        repo_id = ds[\"url\"].split(\"/\")[-1]  # Extract repo name from URL\n",
        "        full_repo_id = f\"sanjuhs/{repo_id}\"  # Full repository ID\n",
        "        \n",
        "        print(f\"ğŸ” Getting info for test dataset: {full_repo_id}\")\n",
        "        \n",
        "        # Get dataset information first\n",
        "        dataset_info = hf_downloader.get_dataset_info(full_repo_id)\n",
        "        print(\"dataset_info\" , dataset_info)\n",
        "        \n",
        "        if dataset_info:\n",
        "            print(f\"\\nğŸ“¥ Downloading test dataset...\")\n",
        "            # Download all files (video + json)\n",
        "            results = hf_downloader.download_dataset_files(\n",
        "                repo_id=full_repo_id,\n",
        "                dataset_name=ds[\"name\"],\n",
        "                file_types=['all']  # Download everything\n",
        "            )\n",
        "            return results\n",
        "        else:\n",
        "            print(\"âŒ Could not get dataset information\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# Example usage - Download and process test dataset\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸš€ DOWNLOADING HUGGING FACE DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Option 1: Download and process test dataset\n",
        "print(\"\\n1ï¸âƒ£ TEST DATASET:\")\n",
        "# dataset_results = download_dataset_hf(huggingface_test_dataset_urls) # change test to actual to download actual dataset\n",
        "dataset_results = download_dataset_hf(huggingface_actual_dataset_urls) # change test to actual to download actual dataset\n",
        "\n",
        "\n",
        "if dataset_results:\n",
        "    print(f\"\\nâœ… Dataset downloaded successfully!\")\n",
        "    print(f\"ğŸ“ Location: {dataset_results['dataset_dir']}\")\n",
        "    print(f\"ğŸ“Š Files: {dataset_results['success_count']} downloaded\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GUEq0xUOk5DE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'repo_id': 'sanjuhs/ml_video_dataset', 'dataset_name': 'ml_sanjay_5_15min_datasets', 'dataset_dir': 'hf_datasets/ml_sanjay_5_15min_datasets', 'total_files_requested': 6, 'downloaded_files': [{'filename': '.gitattributes', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/.gitattributes', 'size_mb': 0.0023469924926757812}, {'filename': 'ml_sanjay_assortmentSounds55_15min_dataset.mp4', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_assortmentSounds55_15min_dataset.mp4', 'size_mb': 1513.3162021636963}, {'filename': 'ml_sanjay_calmSinging_15min_dataset.mp4', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_calmSinging_15min_dataset.mp4', 'size_mb': 1506.6638174057007}, {'filename': 'ml_sanjay_commentary_15min_dataset.mp4', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_commentary_15min_dataset.mp4', 'size_mb': 1493.8126392364502}, {'filename': 'ml_sanjay_explanationRnn_15min_dataset.mp4', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_explanationRnn_15min_dataset.mp4', 'size_mb': 1553.5173444747925}, {'filename': 'ml_sanjay_frutratedAngry_15min_dataset.mp4', 'local_path': 'hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_frutratedAngry_15min_dataset.mp4', 'size_mb': 1610.3213396072388}], 'failed_files': [], 'success_count': 6, 'failure_count': 0, 'total_size_mb': 7677.633689880371}\n",
            "will now process step 0 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_assortmentSounds55_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Starting video analysis...\n",
            "============================================================\n",
            "VIDEO ANALYSIS REPORT\n",
            "============================================================\n",
            "File: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_assortmentSounds55_15min_dataset.mp4\n",
            "File Size: 1513.32 MB\n",
            "Duration: 15.21 minutes (912.8 seconds)\n",
            "Resolution: 1920x1080\n",
            "Frame Rate: 30.00 FPS\n",
            "Total Frames: 27385\n",
            "Estimated Bitrate: 13.26 Mbps\n",
            "\n",
            "QUALITY METRICS:\n",
            "Average Brightness: 139.8 (0-255)\n",
            "Average Contrast: 65.4\n",
            "\n",
            "RECOMMENDATIONS:\n",
            "âœ… Frame rate is suitable for training (25-30 FPS range).\n",
            "âœ… Resolution is adequate for face tracking.\n",
            "âœ… Brightness levels appear good.\n",
            "âœ… Contrast levels appear adequate.\n",
            "============================================================\n",
            "Analysis saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/analysis/0_video_analysis_ml_sanjay_assortmentSounds55_15min_dataset.json\n",
            "finsihed step0  for video ml_sanjay_assortmentSounds55_15min_dataset.mp4\n",
            "will now process step 0 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_calmSinging_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Starting video analysis...\n",
            "============================================================\n",
            "VIDEO ANALYSIS REPORT\n",
            "============================================================\n",
            "File: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_calmSinging_15min_dataset.mp4\n",
            "File Size: 1506.66 MB\n",
            "Duration: 15.10 minutes (906.2 seconds)\n",
            "Resolution: 1920x1080\n",
            "Frame Rate: 30.00 FPS\n",
            "Total Frames: 27186\n",
            "Estimated Bitrate: 13.30 Mbps\n",
            "\n",
            "QUALITY METRICS:\n",
            "Average Brightness: 147.9 (0-255)\n",
            "Average Contrast: 64.0\n",
            "\n",
            "RECOMMENDATIONS:\n",
            "âœ… Frame rate is suitable for training (25-30 FPS range).\n",
            "âœ… Resolution is adequate for face tracking.\n",
            "âœ… Brightness levels appear good.\n",
            "âœ… Contrast levels appear adequate.\n",
            "============================================================\n",
            "Analysis saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/analysis/0_video_analysis_ml_sanjay_calmSinging_15min_dataset.json\n",
            "finsihed step0  for video ml_sanjay_calmSinging_15min_dataset.mp4\n",
            "will now process step 0 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_commentary_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Starting video analysis...\n",
            "============================================================\n",
            "VIDEO ANALYSIS REPORT\n",
            "============================================================\n",
            "File: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_commentary_15min_dataset.mp4\n",
            "File Size: 1493.81 MB\n",
            "Duration: 15.39 minutes (923.4 seconds)\n",
            "Resolution: 1920x1080\n",
            "Frame Rate: 30.00 FPS\n",
            "Total Frames: 27703\n",
            "Estimated Bitrate: 12.94 Mbps\n",
            "\n",
            "QUALITY METRICS:\n",
            "Average Brightness: 160.3 (0-255)\n",
            "Average Contrast: 63.1\n",
            "\n",
            "RECOMMENDATIONS:\n",
            "âœ… Frame rate is suitable for training (25-30 FPS range).\n",
            "âœ… Resolution is adequate for face tracking.\n",
            "âœ… Brightness levels appear good.\n",
            "âœ… Contrast levels appear adequate.\n",
            "============================================================\n",
            "Analysis saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/analysis/0_video_analysis_ml_sanjay_commentary_15min_dataset.json\n",
            "finsihed step0  for video ml_sanjay_commentary_15min_dataset.mp4\n",
            "will now process step 0 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_explanationRnn_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Starting video analysis...\n",
            "============================================================\n",
            "VIDEO ANALYSIS REPORT\n",
            "============================================================\n",
            "File: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_explanationRnn_15min_dataset.mp4\n",
            "File Size: 1553.52 MB\n",
            "Duration: 15.78 minutes (946.5 seconds)\n",
            "Resolution: 1920x1080\n",
            "Frame Rate: 30.00 FPS\n",
            "Total Frames: 28395\n",
            "Estimated Bitrate: 13.13 Mbps\n",
            "\n",
            "QUALITY METRICS:\n",
            "Average Brightness: 162.9 (0-255)\n",
            "Average Contrast: 66.3\n",
            "\n",
            "RECOMMENDATIONS:\n",
            "âœ… Frame rate is suitable for training (25-30 FPS range).\n",
            "âœ… Resolution is adequate for face tracking.\n",
            "âœ… Brightness levels appear good.\n",
            "âœ… Contrast levels appear adequate.\n",
            "============================================================\n",
            "Analysis saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/analysis/0_video_analysis_ml_sanjay_explanationRnn_15min_dataset.json\n",
            "finsihed step0  for video ml_sanjay_explanationRnn_15min_dataset.mp4\n",
            "will now process step 0 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_frutratedAngry_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Starting video analysis...\n",
            "============================================================\n",
            "VIDEO ANALYSIS REPORT\n",
            "============================================================\n",
            "File: hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_frutratedAngry_15min_dataset.mp4\n",
            "File Size: 1610.32 MB\n",
            "Duration: 15.05 minutes (903.3 seconds)\n",
            "Resolution: 1920x1080\n",
            "Frame Rate: 30.00 FPS\n",
            "Total Frames: 27098\n",
            "Estimated Bitrate: 14.26 Mbps\n",
            "\n",
            "QUALITY METRICS:\n",
            "Average Brightness: 141.2 (0-255)\n",
            "Average Contrast: 64.0\n",
            "\n",
            "RECOMMENDATIONS:\n",
            "âœ… Frame rate is suitable for training (25-30 FPS range).\n",
            "âœ… Resolution is adequate for face tracking.\n",
            "âœ… Brightness levels appear good.\n",
            "âœ… Contrast levels appear adequate.\n",
            "============================================================\n",
            "Analysis saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/analysis/0_video_analysis_ml_sanjay_frutratedAngry_15min_dataset.json\n",
            "finsihed step0  for video ml_sanjay_frutratedAngry_15min_dataset.mp4\n"
          ]
        }
      ],
      "source": [
        "# step 0 video analysis functions\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Video Analysis Script - Analyze video properties and quality\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def analyze_video(video_path):\n",
        "    \"\"\"\n",
        "    Analyze video file properties and quality\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Get video properties\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Get file size\n",
        "    file_size = os.path.getsize(video_path) / (1024 * 1024)  # MB\n",
        "\n",
        "    # Sample frames to check quality\n",
        "    sample_frames = []\n",
        "    frame_indices = np.linspace(0, frame_count - 1, min(10, frame_count), dtype=int)\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            sample_frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Analyze frame quality\n",
        "    avg_brightness = 0\n",
        "    avg_contrast = 0\n",
        "    if sample_frames:\n",
        "        brightnesses = []\n",
        "        contrasts = []\n",
        "        for frame in sample_frames:\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            brightnesses.append(np.mean(gray))\n",
        "            contrasts.append(np.std(gray))\n",
        "\n",
        "        avg_brightness = np.mean(brightnesses)\n",
        "        avg_contrast = np.mean(contrasts)\n",
        "\n",
        "    # Compile results\n",
        "    analysis = {\n",
        "        'file_path': video_path,\n",
        "        'file_size_mb': file_size,\n",
        "        'frame_count': frame_count,\n",
        "        'fps': fps,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'duration_seconds': duration,\n",
        "        'duration_minutes': duration / 60,\n",
        "        'resolution': f\"{width}x{height}\",\n",
        "        'avg_brightness': avg_brightness,\n",
        "        'avg_contrast': avg_contrast,\n",
        "        'bitrate_mbps': (file_size * 8) / duration if duration > 0 else 0,\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def print_analysis_report(analysis):\n",
        "    \"\"\"\n",
        "    Print a formatted analysis report\n",
        "    \"\"\"\n",
        "    if not analysis:\n",
        "        return\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"VIDEO ANALYSIS REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"File: {analysis['file_path']}\")\n",
        "    print(f\"File Size: {analysis['file_size_mb']:.2f} MB\")\n",
        "    print(f\"Duration: {analysis['duration_minutes']:.2f} minutes ({analysis['duration_seconds']:.1f} seconds)\")\n",
        "    print(f\"Resolution: {analysis['resolution']}\")\n",
        "    print(f\"Frame Rate: {analysis['fps']:.2f} FPS\")\n",
        "    print(f\"Total Frames: {analysis['frame_count']}\")\n",
        "    print(f\"Estimated Bitrate: {analysis['bitrate_mbps']:.2f} Mbps\")\n",
        "    print()\n",
        "    print(\"QUALITY METRICS:\")\n",
        "    print(f\"Average Brightness: {analysis['avg_brightness']:.1f} (0-255)\")\n",
        "    print(f\"Average Contrast: {analysis['avg_contrast']:.1f}\")\n",
        "    print()\n",
        "\n",
        "    # Quality recommendations\n",
        "    print(\"RECOMMENDATIONS:\")\n",
        "\n",
        "    # Check FPS consistency\n",
        "    if analysis['fps'] < 25:\n",
        "        print(\"âš ï¸  Low frame rate detected. Consider using 25-30 FPS for better model training.\")\n",
        "    elif analysis['fps'] > 35:\n",
        "        print(\"â„¹ï¸  High frame rate detected. You may downsample to 25-30 FPS to reduce computational load.\")\n",
        "    else:\n",
        "        print(\"âœ… Frame rate is suitable for training (25-30 FPS range).\")\n",
        "\n",
        "    # Check resolution\n",
        "    if analysis['width'] < 640 or analysis['height'] < 480:\n",
        "        print(\"âš ï¸  Low resolution detected. Higher resolution may improve face tracking accuracy.\")\n",
        "    else:\n",
        "        print(\"âœ… Resolution is adequate for face tracking.\")\n",
        "\n",
        "    # Check brightness\n",
        "    if analysis['avg_brightness'] < 80:\n",
        "        print(\"âš ï¸  Video appears dark. Consider brightness adjustment for better face detection.\")\n",
        "    elif analysis['avg_brightness'] > 200:\n",
        "        print(\"âš ï¸  Video appears overexposed. Consider brightness adjustment.\")\n",
        "    else:\n",
        "        print(\"âœ… Brightness levels appear good.\")\n",
        "\n",
        "    # Check contrast\n",
        "    if analysis['avg_contrast'] < 20:\n",
        "        print(\"âš ï¸  Low contrast detected. May affect feature extraction quality.\")\n",
        "    else:\n",
        "        print(\"âœ… Contrast levels appear adequate.\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def step0_main( video_path):\n",
        "    \"\"\"\n",
        "    Main function to analyze the video\n",
        "    \"\"\"\n",
        "\n",
        "    # root path\n",
        "    root_path = Path.cwd()  # use current working directory in Colab\n",
        "    print(f\"Root path: {root_path}\")\n",
        "\n",
        "    print(\"Starting video analysis...\")\n",
        "    analysis = analyze_video(video_path)\n",
        "\n",
        "    if analysis:\n",
        "        print_analysis_report(analysis)\n",
        "\n",
        "        # Save analysis to file\n",
        "        # lets also make the name unique by adding the video name to the file name\n",
        "        import json\n",
        "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        output_file = f\"{root_path}/data/analysis/0_video_analysis_{video_name}.json\"\n",
        "\n",
        "        if not os.path.exists(output_file):\n",
        "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(analysis, f, indent=2)\n",
        "        print(f\"Analysis saved to: {output_file}\")\n",
        "    else:\n",
        "        print(\"Failed to analyze video.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# for all videos in the dataset you need to analyze and do step 0 and store in \n",
        "# so we need to first eget the path of the video/s from the dataset path\n",
        "\n",
        "print(dataset_results)\n",
        "\n",
        "# so from teh below we have to get paths of teh Vidoes or MP4 files etc\n",
        "# {'repo_id': 'sanjuhs/test-video-dataset', 'dataset_name': 'test_video', 'dataset_dir': 'hf_datasets/test_video', 'total_files_requested': 3, 'downloaded_files': [{'filename': '.gitattributes', 'local_path': 'hf_datasets/test_video/.gitattributes', 'size_mb': 0.0023469924926757812}, {'filename': 'README.md', 'local_path': 'hf_datasets/test_video/README.md', 'size_mb': 2.288818359375e-05}, {'filename': 'test.mp4', 'local_path': 'hf_datasets/test_video/test.mp4', 'size_mb': 10.61563491821289}], 'failed_files': [], 'success_count': 3, 'failure_count': 0, 'total_size_mb': 10.61800479888916}\n",
        "\n",
        "for f in dataset_results[\"downloaded_files\"]:\n",
        "    if f[\"filename\"].endswith(\".mp4\"):\n",
        "        video_path = f[\"local_path\"]\n",
        "        print(\"will now process step 0 for video\" , video_path)\n",
        "        step0_main(video_path)\n",
        "        print(\"finsihed step0  for video\" , f[\"filename\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1757394356.553881 51401898 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1\n",
            "W0000 00:00:1757394356.571178 51401898 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
            "W0000 00:00:1757394356.592810 52473064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "W0000 00:00:1757394356.606550 52473064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now doing step 1 extract blendshapes and pose for all videos in the dataset\n",
            "will now process step 1 for video hf_datasets/ml_sanjay_5_15min_datasets/ml_sanjay_assortmentSounds55_15min_dataset.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Initializing Face Blendshape Extractor...\n",
            "Starting feature extraction...\n",
            "FPS limit: 30, original video FPS: 30.0\n",
            "Frame interval: 1 (processing every 1 frame(s))\n",
            "Effective extraction FPS: 30.0\n",
            "Processing 27385 frames out of 27385 total frames...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features:   7%|â–‹         | 2034/27385 [11:00<46:22,  9.11it/s]  "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 425\u001b[39m\n\u001b[32m    423\u001b[39m video_path = f[\u001b[33m\"\u001b[39m\u001b[33mlocal_path\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    424\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mwill now process step 1 for video\u001b[39m\u001b[33m\"\u001b[39m , video_path)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[43mstep1_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfinsihed step1  for video\u001b[39m\u001b[33m\"\u001b[39m , f[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 388\u001b[39m, in \u001b[36mstep1_main\u001b[39m\u001b[34m(video_path)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting feature extraction...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# For initial testing, limit to first 1000 frames (~33 seconds at 30fps)\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# Remove max_frames=1000 to process the entire video\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m extraction_data = \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_from_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mroot_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data/extracted_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_frames=1000  # Remove this line to process full video\u001b[39;49;00m\n\u001b[32m    392\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mFaceBlendshapeExtractor.extract_from_video\u001b[39m\u001b[34m(self, video_path, output_dir, max_frames, fps_limit)\u001b[39m\n\u001b[32m    140\u001b[39m pbar = tqdm(total=\u001b[38;5;28mlen\u001b[39m(frames_to_process), desc=\u001b[33m\"\u001b[39m\u001b[33mExtracting features\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, frame_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(frames_to_process):\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# Seek to specific frame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     ret, frame = cap.read()\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features:   7%|â–‹         | 2034/27385 [11:16<46:22,  9.11it/s]"
          ]
        }
      ],
      "source": [
        "# step 1 extract blendshapes\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "MediaPipe Blendshapes and Head Pose Extraction Script\n",
        "Extracts 52 blendshapes + 7 head pose values (x,y,z,qw,qx,qy,qz) = 59 values per frame\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "class FaceBlendshapeExtractor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize MediaPipe Face Landmarker with blendshapes\"\"\"\n",
        "        self.mp_face_mesh = mp.solutions.face_mesh\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
        "        \n",
        "        # MediaPipe blendshape names (52 categories)\n",
        "        self.blendshape_names = [\n",
        "            '_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft', \n",
        "            'browOuterUpRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft', \n",
        "            'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft', 'eyeLookInRight', \n",
        "            'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft', \n",
        "            'eyeSquintRight', 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen', \n",
        "            'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight', 'mouthFrownLeft', \n",
        "            'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight', \n",
        "            'mouthPressLeft', 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower', \n",
        "            'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft', 'mouthSmileRight', \n",
        "            'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight', \n",
        "            'noseSneerLeft', 'noseSneerRight'\n",
        "        ]\n",
        "        \n",
        "        # Download the face landmarker model if it doesn't exist\n",
        "        model_path = self._download_face_landmarker_model()\n",
        "        \n",
        "        # Create Face Landmarker\n",
        "        base_options = python.BaseOptions(model_asset_path=model_path)\n",
        "        options = vision.FaceLandmarkerOptions(\n",
        "            base_options=base_options,\n",
        "            output_face_blendshapes=True,\n",
        "            output_facial_transformation_matrixes=True,\n",
        "            num_faces=1\n",
        "        )\n",
        "        self.detector = vision.FaceLandmarker.create_from_options(options)\n",
        "        \n",
        "    def _download_face_landmarker_model(self):\n",
        "        \"\"\"Download the face landmarker model if it doesn't exist\"\"\"\n",
        "        model_dir = Path(\"models\")\n",
        "        model_dir.mkdir(exist_ok=True)\n",
        "        model_path = model_dir / \"face_landmarker_v2_with_blendshapes.task\"\n",
        "        \n",
        "        if not model_path.exists():\n",
        "            print(\"Downloading Face Landmarker model...\")\n",
        "            import urllib.request\n",
        "            url = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n",
        "            try:\n",
        "                urllib.request.urlretrieve(url, model_path)\n",
        "                print(f\"Model downloaded to: {model_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading model: {e}\")\n",
        "                print(\"Please download the model manually from:\")\n",
        "                print(url)\n",
        "                sys.exit(1)\n",
        "        \n",
        "        return str(model_path)\n",
        "    \n",
        "    def extract_from_video(self, video_path, output_dir=\"extracted_features\", max_frames=None, fps_limit=30):\n",
        "        \"\"\"\n",
        "        Extract blendshapes and head pose from video. Default fps limit is 30. \n",
        "        If this function is called with no mention of fps_limit, then it will consider the fps_limit as 30.\n",
        "        If fps_limit is explicitly mentioned as None, then it will process the entire video at the fps of the video.\n",
        "        \n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            output_dir: Directory to save extracted features\n",
        "            max_frames: Maximum number of frames to process (for testing)\n",
        "            fps_limit: Target FPS for extraction (default 30)\n",
        "        \n",
        "        Returns:\n",
        "            dict: Extracted features data\n",
        "        \"\"\"\n",
        "        \n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
        "        \n",
        "        # Create output directory\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Open video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        \n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Could not open video: {video_path}\")\n",
        "        \n",
        "        # Get video properties\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        \n",
        "        # Determine effective FPS limit\n",
        "        if fps_limit is None:\n",
        "            fps_limit = fps\n",
        "            print(f\"FPS limit not provided, will process at original {fps} FPS\")\n",
        "        else:\n",
        "            print(f\"FPS limit: {fps_limit}, original video FPS: {fps}\")\n",
        "        \n",
        "        # Calculate frame interval based on FPS limit\n",
        "        if fps_limit >= fps:\n",
        "            # Process every frame if fps_limit is higher than or equal to video fps\n",
        "            frame_interval = 1\n",
        "            effective_fps = fps\n",
        "        else:\n",
        "            # Skip frames to achieve target fps_limit\n",
        "            frame_interval = int(fps / fps_limit)\n",
        "            effective_fps = fps_limit\n",
        "        \n",
        "        print(f\"Frame interval: {frame_interval} (processing every {frame_interval} frame(s))\")\n",
        "        print(f\"Effective extraction FPS: {effective_fps}\")\n",
        "        \n",
        "        # Calculate frames to process\n",
        "        frames_to_process = list(range(0, total_frames, frame_interval))\n",
        "        if max_frames:\n",
        "            frames_to_process = frames_to_process[:max_frames]\n",
        "        \n",
        "        print(f\"Processing {len(frames_to_process)} frames out of {total_frames} total frames...\")\n",
        "        \n",
        "        # Storage for extracted features\n",
        "        frame_data = []\n",
        "        failed_frames = []\n",
        "        \n",
        "        # Process frames\n",
        "        pbar = tqdm(total=len(frames_to_process), desc=\"Extracting features\")\n",
        "        \n",
        "        for i, frame_idx in enumerate(frames_to_process):\n",
        "            # Seek to specific frame\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            \n",
        "            if not ret:\n",
        "                print(f\"Failed to read frame {frame_idx}\")\n",
        "                failed_frames.append(frame_idx)\n",
        "                # Add placeholder for failed frame\n",
        "                blendshapes = {name: 0.0 for name in self.blendshape_names}\n",
        "                placeholder = {\n",
        "                    'frame_index': frame_idx,\n",
        "                    'timestamp': frame_idx / fps,\n",
        "                    'blendshapes': blendshapes,\n",
        "                    'headPosition': {'x': 0.0, 'y': 0.0, 'z': 0.0},\n",
        "                    'headRotation': {'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0},\n",
        "                    'has_face': False\n",
        "                }\n",
        "                frame_data.append(placeholder)\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "            \n",
        "            # Convert BGR to RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Create MediaPipe Image\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "            \n",
        "            # Extract features\n",
        "            features = self._extract_frame_features(mp_image, frame_idx)\n",
        "            \n",
        "            if features:\n",
        "                # Update timestamp to use original frame index for accurate timing\n",
        "                features['timestamp'] = frame_idx / fps\n",
        "                frame_data.append(features)\n",
        "            else:\n",
        "                failed_frames.append(frame_idx)\n",
        "                # Add placeholder data for failed frames\n",
        "                blendshapes = {name: 0.0 for name in self.blendshape_names}\n",
        "                placeholder = {\n",
        "                    'frame_index': frame_idx,\n",
        "                    'timestamp': frame_idx / fps,\n",
        "                    'blendshapes': blendshapes,\n",
        "                    'headPosition': {'x': 0.0, 'y': 0.0, 'z': 0.0},\n",
        "                    'headRotation': {'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0},\n",
        "                    'has_face': False\n",
        "                }\n",
        "                frame_data.append(placeholder)\n",
        "            \n",
        "            pbar.update(1)\n",
        "        \n",
        "        pbar.close()\n",
        "        cap.release()\n",
        "        \n",
        "        # Generate session ID based on timestamp\n",
        "        import time\n",
        "        session_start_time = int(time.time() * 1000)  # Current time in milliseconds\n",
        "        session_id = f\"session_{session_start_time}_extract\"\n",
        "        \n",
        "        # Convert timestamps to milliseconds and add session ID\n",
        "        for frame in frame_data:\n",
        "            frame['timestamp'] = int(frame['timestamp'] * 1000)  # Convert to milliseconds\n",
        "            frame['sessionId'] = session_id\n",
        "        \n",
        "        # Create final output structure similar to human head data\n",
        "        output_data = {\n",
        "            'sessionInfo': {\n",
        "                'sessionId': session_id,\n",
        "                'startTime': session_start_time,\n",
        "                'targetFPS': fps_limit,\n",
        "                'originalFPS': fps,\n",
        "                'frameInterval': frame_interval,\n",
        "                'videoPath': video_path\n",
        "            },\n",
        "            'frameCount': len(frame_data),\n",
        "            'failedFrames': len(failed_frames),\n",
        "            'failureRate': len(failed_frames) / len(frame_data) if frame_data else 1.0,\n",
        "            'frames': frame_data\n",
        "        }\n",
        "        \n",
        "        # Save single clean JSON file\n",
        "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        output_file = Path(output_dir) / f\"blendshapes_and_pose_{video_name}.json\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "        \n",
        "        print(f\"\\\\nExtraction complete!\")\n",
        "        print(f\"Original video: {total_frames} frames at {fps} FPS\")\n",
        "        print(f\"Processed frames: {len(frame_data)} at {effective_fps} FPS (every {frame_interval} frame(s))\")\n",
        "        print(f\"Failed frames: {len(failed_frames)} ({output_data['failureRate']:.2%})\")\n",
        "        print(f\"Features saved to: {output_file}\")\n",
        "        print(f\"Session ID: {session_id}\")\n",
        "        \n",
        "        return output_data\n",
        "    \n",
        "    def _extract_frame_features(self, mp_image, frame_idx):\n",
        "        \"\"\"\n",
        "        Extract features from a single frame\n",
        "        \n",
        "        Args:\n",
        "            mp_image: MediaPipe Image object\n",
        "            frame_idx: Frame index\n",
        "        \n",
        "        Returns:\n",
        "            dict: Frame features or None if extraction failed\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Detect face landmarks and blendshapes\n",
        "            detection_result = self.detector.detect(mp_image)\n",
        "            \n",
        "            if not detection_result.face_landmarks:\n",
        "                return None  # No face detected\n",
        "            \n",
        "            # Get first face (we only process one face)\n",
        "            face_landmarks = detection_result.face_landmarks[0]\n",
        "            \n",
        "            # Extract blendshapes as named dictionary\n",
        "            blendshapes = {}\n",
        "            \n",
        "            if detection_result.face_blendshapes:\n",
        "                face_blendshapes = detection_result.face_blendshapes[0]\n",
        "                for i, bs in enumerate(face_blendshapes):\n",
        "                    if i < len(self.blendshape_names):\n",
        "                        blendshapes[self.blendshape_names[i]] = bs.score\n",
        "            else:\n",
        "                # Fallback if no blendshapes detected\n",
        "                for name in self.blendshape_names:\n",
        "                    blendshapes[name] = 0.0\n",
        "            \n",
        "            # Extract head pose from transformation matrix\n",
        "            head_position = {'x': 0.0, 'y': 0.0, 'z': 0.0}\n",
        "            head_rotation = {'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0}\n",
        "            \n",
        "            if detection_result.facial_transformation_matrixes:\n",
        "                transform_matrix = detection_result.facial_transformation_matrixes[0]\n",
        "                pose_array = self._matrix_to_pose(transform_matrix)\n",
        "                head_position = {'x': pose_array[0], 'y': pose_array[1], 'z': pose_array[2]}\n",
        "                head_rotation = {'w': pose_array[3], 'x': pose_array[4], 'y': pose_array[5], 'z': pose_array[6]}\n",
        "            \n",
        "            # Calculate timestamp\n",
        "            timestamp = frame_idx / 30.0  # Will be updated with actual FPS later\n",
        "            \n",
        "            frame_features = {\n",
        "                'frame_index': frame_idx,\n",
        "                'timestamp': timestamp,\n",
        "                'blendshapes': blendshapes,\n",
        "                'headPosition': head_position,\n",
        "                'headRotation': head_rotation,\n",
        "                'has_face': True\n",
        "            }\n",
        "            \n",
        "            return frame_features\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {frame_idx}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def _matrix_to_pose(self, transform_matrix):\n",
        "        \"\"\"\n",
        "        Convert 4x4 transformation matrix to translation + quaternion\n",
        "        \n",
        "        Args:\n",
        "            transform_matrix: 4x4 transformation matrix\n",
        "        \n",
        "        Returns:\n",
        "            list: [x, y, z, qw, qx, qy, qz]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert to numpy array\n",
        "            matrix = np.array(transform_matrix.data).reshape(4, 4)\n",
        "            \n",
        "            # Extract translation (x, y, z)\n",
        "            translation = matrix[:3, 3]\n",
        "            \n",
        "            # Extract rotation matrix\n",
        "            rotation_matrix = matrix[:3, :3]\n",
        "            \n",
        "            # Convert rotation matrix to quaternion\n",
        "            quaternion = self._rotation_matrix_to_quaternion(rotation_matrix)\n",
        "            \n",
        "            # Normalize quaternion\n",
        "            quaternion = quaternion / np.linalg.norm(quaternion)\n",
        "            \n",
        "            # Clamp translation to reasonable range (Â±0.2 m as suggested)\n",
        "            translation = np.clip(translation, -0.2, 0.2)\n",
        "            \n",
        "            # Return as [x, y, z, qw, qx, qy, qz]\n",
        "            pose = [translation[0], translation[1], translation[2], \n",
        "                   quaternion[0], quaternion[1], quaternion[2], quaternion[3]]\n",
        "            \n",
        "            return pose\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error converting matrix to pose: {e}\")\n",
        "            return [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  # Default identity pose\n",
        "    \n",
        "    def _rotation_matrix_to_quaternion(self, R):\n",
        "        \"\"\"\n",
        "        Convert 3x3 rotation matrix to quaternion [w, x, y, z]\n",
        "        \"\"\"\n",
        "        trace = np.trace(R)\n",
        "        \n",
        "        if trace > 0:\n",
        "            S = np.sqrt(trace + 1.0) * 2  # S = 4 * qw\n",
        "            qw = 0.25 * S\n",
        "            qx = (R[2, 1] - R[1, 2]) / S\n",
        "            qy = (R[0, 2] - R[2, 0]) / S\n",
        "            qz = (R[1, 0] - R[0, 1]) / S\n",
        "        elif R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n",
        "            S = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2  # S = 4 * qx\n",
        "            qw = (R[2, 1] - R[1, 2]) / S\n",
        "            qx = 0.25 * S\n",
        "            qy = (R[0, 1] + R[1, 0]) / S\n",
        "            qz = (R[0, 2] + R[2, 0]) / S\n",
        "        elif R[1, 1] > R[2, 2]:\n",
        "            S = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2  # S = 4 * qy\n",
        "            qw = (R[0, 2] - R[2, 0]) / S\n",
        "            qx = (R[0, 1] + R[1, 0]) / S\n",
        "            qy = 0.25 * S\n",
        "            qz = (R[1, 2] + R[2, 1]) / S\n",
        "        else:\n",
        "            S = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2  # S = 4 * qz\n",
        "            qw = (R[1, 0] - R[0, 1]) / S\n",
        "            qx = (R[0, 2] + R[2, 0]) / S\n",
        "            qy = (R[1, 2] + R[2, 1]) / S\n",
        "            qz = 0.25 * S\n",
        "        \n",
        "        return np.array([qw, qx, qy, qz])\n",
        "\n",
        "def step1_main( video_path):\n",
        "    \"\"\"\n",
        "    Main function to extract features from video\n",
        "    \"\"\"\n",
        "\n",
        "    # root path\n",
        "    root_path = Path.cwd()  # use current working directory in Colab\n",
        "    print(f\"Root path: {root_path}\")\n",
        "\n",
        "    # video_path = f\"{root_path}/data/test.mp4\"\n",
        "    \n",
        "    print(\"Initializing Face Blendshape Extractor...\")\n",
        "    extractor = FaceBlendshapeExtractor()\n",
        "    \n",
        "    print(\"Starting feature extraction...\")\n",
        "    # For initial testing, limit to first 1000 frames (~33 seconds at 30fps)\n",
        "    # Remove max_frames=1000 to process the entire video\n",
        "    extraction_data = extractor.extract_from_video(\n",
        "        video_path, \n",
        "        output_dir=f\"{root_path}/data/extracted_features\",\n",
        "        # max_frames=1000  # Remove this line to process full video\n",
        "    )\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"FEATURE EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Session ID: {extraction_data['sessionInfo']['sessionId']}\")\n",
        "    print(f\"Video: {extraction_data['sessionInfo']['videoPath']}\")\n",
        "    print(f\"Total frames processed: {extraction_data['frameCount']}\")\n",
        "    print(f\"Target FPS: {extraction_data['sessionInfo']['targetFPS']}\")\n",
        "    print(f\"Original FPS: {extraction_data['sessionInfo']['originalFPS']:.2f}\")\n",
        "    print(f\"Duration: {extraction_data['frameCount']/extraction_data['sessionInfo']['targetFPS']:.2f} seconds\")\n",
        "    print(f\"Failed frames: {extraction_data['failedFrames']}\")\n",
        "    print(f\"Success rate: {(1-extraction_data['failureRate'])*100:.1f}%\")\n",
        "    \n",
        "    # Sample feature verification\n",
        "    if extraction_data['frames']:\n",
        "        sample_frame = extraction_data['frames'][0]\n",
        "        print(f\"\\\\nSample frame features:\")\n",
        "        print(f\"  Blendshapes count: {len(sample_frame['blendshapes'])}\")\n",
        "        print(f\"  Head position: {sample_frame['headPosition']}\")\n",
        "        print(f\"  Head rotation: {sample_frame['headRotation']}\")\n",
        "        print(f\"  Has face: {sample_frame['has_face']}\")\n",
        "        print(f\"  Session ID: {sample_frame['sessionId']}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"Now doing step 1 extract blendshapes and pose for all videos in the dataset\")\n",
        "for f in dataset_results[\"downloaded_files\"]:\n",
        "    if f[\"filename\"].endswith(\".mp4\"):\n",
        "        video_path = f[\"local_path\"]\n",
        "        print(\"will now process step 1 for video\" , video_path)\n",
        "        step1_main(video_path)\n",
        "        print(\"finsihed step1  for video\" , f[\"filename\"])\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will now process step 0 for video hf_datasets/test_video/test.mp4\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Initializing Audio Feature Extractor...\n",
            "Audio Feature Extractor initialized:\n",
            "  Sample rate: 16000 Hz\n",
            "  Mel features: 80\n",
            "  Mel frame rate: 100.0 Hz\n",
            "  Hop length: 160 samples (10.0ms)\n",
            "  Window length: 400 samples (25.0ms)\n",
            "\\nStarting audio feature extraction...\n",
            "Extracting audio from: hf_datasets/test_video/test.mp4\n",
            "MoviePy - Writing audio in /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/extracted_audio_test.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Audio extracted to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/extracted_audio_test.wav\n",
            "Loading audio from: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/extracted_audio_test.wav\n",
            "Audio loaded: 534080 samples, 33.38 seconds\n",
            "Computing mel spectrogram...\n",
            "Computing additional audio features...\n",
            "\\nAudio feature extraction complete!\n",
            "Mel features shape: (3339, 80)\n",
            "Features saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/audio_features_test.json\n",
            "\\n============================================================\n",
            "AUDIO FEATURE EXTRACTION SUMMARY\n",
            "============================================================\n",
            "Audio file: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/extracted_audio_test.wav\n",
            "Duration: 33.38 seconds\n",
            "Sample rate: 16000 Hz\n",
            "Mel features: 80\n",
            "Number of frames: 3339\n",
            "Mel frame rate: 100.0 Hz\n",
            "Feature dimensions: 3339 x 80\n",
            "Voice activity: 70.0% of frames\n",
            "============================================================\n",
            "finsihed step2  for video test.mp4\n"
          ]
        }
      ],
      "source": [
        "# step 2 extract audio features\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Audio Feature Extraction Script\n",
        "Extracts audio from video and computes mel spectrograms for the TCN model\n",
        "\"\"\"\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from moviepy import VideoFileClip\n",
        "\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=80, hop_length=160, win_length=400, n_fft=512):\n",
        "        \"\"\"\n",
        "        Initialize audio feature extractor\n",
        "        \n",
        "        Args:\n",
        "            sample_rate: Target sample rate (16kHz for efficiency)\n",
        "            n_mels: Number of mel filter banks (80 is standard)\n",
        "            hop_length: Hop length in samples (10ms at 16kHz)\n",
        "            win_length: Window length in samples (25ms at 16kHz)\n",
        "            n_fft: FFT size\n",
        "        \"\"\"\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.n_fft = n_fft\n",
        "        \n",
        "        # Calculate frame rate for mel spectrograms\n",
        "        # At 16kHz with hop_length=160, we get 100 mel frames per second\n",
        "        self.mel_frame_rate = sample_rate / hop_length\n",
        "        \n",
        "        print(f\"Audio Feature Extractor initialized:\")\n",
        "        print(f\"  Sample rate: {sample_rate} Hz\")\n",
        "        print(f\"  Mel features: {n_mels}\")\n",
        "        print(f\"  Mel frame rate: {self.mel_frame_rate} Hz\")\n",
        "        print(f\"  Hop length: {hop_length} samples ({hop_length/sample_rate*1000:.1f}ms)\")\n",
        "        print(f\"  Window length: {win_length} samples ({win_length/sample_rate*1000:.1f}ms)\")\n",
        "    \n",
        "    def extract_audio_from_video(self, video_path, output_dir=\"extracted_features\"):\n",
        "        \"\"\"\n",
        "        Extract audio from video file\n",
        "        \n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            output_dir: Directory to save audio\n",
        "        \n",
        "        Returns:\n",
        "            str: Path to extracted audio file\n",
        "        \"\"\"\n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
        "        \n",
        "        # Create output directory\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        print(f\"Extracting audio from: {video_path}\")\n",
        "        \n",
        "        # Load video with moviepy\n",
        "        video = VideoFileClip(video_path)\n",
        "        audio = video.audio\n",
        "        \n",
        "        # Save audio as WAV file\n",
        "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "        audio_path = Path(output_dir) / f\"extracted_audio_{video_name}.wav\"\n",
        "        audio.write_audiofile(str(audio_path), \n",
        "                             fps=self.sample_rate)\n",
        "        \n",
        "        # Clean up\n",
        "        audio.close()\n",
        "        video.close()\n",
        "        \n",
        "        print(f\"Audio extracted to: {audio_path}\")\n",
        "        return str(audio_path)\n",
        "    \n",
        "    def extract_mel_features(self, audio_path, output_dir=\"extracted_features\", max_duration=None):\n",
        "        \"\"\"\n",
        "        Extract mel spectrogram features from audio\n",
        "        \n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "            output_dir: Directory to save features\n",
        "            max_duration: Maximum duration to process (seconds, for testing)\n",
        "        \n",
        "        Returns:\n",
        "            dict: Extracted audio features\n",
        "        \"\"\"\n",
        "        if not os.path.exists(audio_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "        \n",
        "        print(f\"Loading audio from: {audio_path}\")\n",
        "        \n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "        \n",
        "        # Limit duration for testing\n",
        "        if max_duration:\n",
        "            max_samples = int(max_duration * self.sample_rate)\n",
        "            audio = audio[:max_samples]\n",
        "            print(f\"Limited audio to {max_duration} seconds ({len(audio)} samples)\")\n",
        "        \n",
        "        print(f\"Audio loaded: {len(audio)} samples, {len(audio)/sr:.2f} seconds\")\n",
        "        \n",
        "        # Extract mel spectrogram\n",
        "        print(\"Computing mel spectrogram...\")\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=self.sample_rate,\n",
        "            n_mels=self.n_mels,\n",
        "            hop_length=self.hop_length,\n",
        "            win_length=self.win_length,\n",
        "            n_fft=self.n_fft,\n",
        "            fmin=0,\n",
        "            fmax=self.sample_rate // 2\n",
        "        )\n",
        "        \n",
        "        # Convert to log mel spectrogram (dB)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        \n",
        "        # Transpose to (time, features) format\n",
        "        mel_features = log_mel_spec.T  # Shape: (time_frames, n_mels)\n",
        "        \n",
        "        # Compute additional features\n",
        "        print(\"Computing additional audio features...\")\n",
        "        \n",
        "        # Voice Activity Detection (VAD) using RMS energy\n",
        "        rms_energy = librosa.feature.rms(\n",
        "            y=audio,\n",
        "            hop_length=self.hop_length,\n",
        "            frame_length=self.win_length\n",
        "        )[0]\n",
        "        \n",
        "        # Simple VAD threshold (adjust based on your data)\n",
        "        vad_threshold = np.percentile(rms_energy, 30)  # Bottom 30% is likely silence\n",
        "        voice_activity = (rms_energy > vad_threshold).astype(float)\n",
        "        \n",
        "        # Zero Crossing Rate (useful for voiced/unvoiced detection)\n",
        "        zcr = librosa.feature.zero_crossing_rate(\n",
        "            audio,\n",
        "            hop_length=self.hop_length,\n",
        "            frame_length=self.win_length\n",
        "        )[0]\n",
        "        \n",
        "        # Ensure all features have the same length\n",
        "        min_length = min(len(mel_features), len(voice_activity), len(zcr))\n",
        "        mel_features = mel_features[:min_length]\n",
        "        voice_activity = voice_activity[:min_length]\n",
        "        zcr = zcr[:min_length]\n",
        "        \n",
        "        # Create time stamps for each frame\n",
        "        timestamps = librosa.frames_to_time(\n",
        "            range(min_length),\n",
        "            sr=self.sample_rate,\n",
        "            hop_length=self.hop_length\n",
        "        )\n",
        "        \n",
        "        # Compile features\n",
        "        audio_features = {\n",
        "            'audio_path': audio_path,\n",
        "            'sample_rate': self.sample_rate,\n",
        "            'duration_seconds': len(audio) / self.sample_rate,\n",
        "            'n_mels': self.n_mels,\n",
        "            'hop_length': self.hop_length,\n",
        "            'mel_frame_rate': self.mel_frame_rate,\n",
        "            'n_frames': min_length,\n",
        "            'timestamps': timestamps.tolist(),\n",
        "            'mel_features': mel_features.tolist(),  # Shape: (time, n_mels)\n",
        "            'voice_activity': voice_activity.tolist(),\n",
        "            'zero_crossing_rate': zcr.tolist(),\n",
        "            'rms_energy': rms_energy[:min_length].tolist()\n",
        "        }\n",
        "        \n",
        "        # Save features\n",
        "        # lets also make the name unique by adding the video name to the file name\n",
        "        video_name = os.path.splitext(os.path.basename(audio_path))[0].replace(\"extracted_audio_\", \"\")\n",
        "\n",
        "        output_file = Path(output_dir) / f\"audio_features_{video_name}.json\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(audio_features, f, indent=2)\n",
        "        \n",
        "        print(f\"\\\\nAudio feature extraction complete!\")\n",
        "        print(f\"Mel features shape: {mel_features.shape}\")\n",
        "        print(f\"Features saved to: {output_file}\")\n",
        "        \n",
        "        return audio_features\n",
        "    \n",
        "    def extract_from_video(self, video_path, output_dir=\"extracted_features\", max_duration=None):\n",
        "        \"\"\"\n",
        "        Complete pipeline: extract audio from video and compute features\n",
        "        \n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            output_dir: Directory to save features\n",
        "            max_duration: Maximum duration to process (seconds, for testing)\n",
        "        \n",
        "        Returns:\n",
        "            dict: Extracted audio features\n",
        "        \"\"\"\n",
        "        # Step 1: Extract audio from video\n",
        "        audio_path = self.extract_audio_from_video(video_path, output_dir)\n",
        "        \n",
        "        # Step 2: Extract mel features from audio\n",
        "        features = self.extract_mel_features(audio_path, output_dir, max_duration)\n",
        "        \n",
        "        return features\n",
        "\n",
        "def step2_main(video_path):\n",
        "    \"\"\"\n",
        "    Main function to extract audio features from video\n",
        "    \"\"\"\n",
        "\n",
        "    # root path\n",
        "    root_path = Path.cwd()  # use current working directory in Colab\n",
        "    print(f\"Root path: {root_path}\")\n",
        "\n",
        "    # video_path = f\"{root_path}/data/test.mp4\"\n",
        "    \n",
        "    print(\"Initializing Audio Feature Extractor...\")\n",
        "    extractor = AudioFeatureExtractor(\n",
        "        sample_rate=16000,  # 16kHz for efficiency\n",
        "        n_mels=80,          # 80 mel features\n",
        "        hop_length=160,     # 10ms hop (160 samples at 16kHz)\n",
        "        win_length=400,     # 25ms window (400 samples at 16kHz)\n",
        "        n_fft=512          # 512-point FFT\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\nStarting audio feature extraction...\")\n",
        "    \n",
        "    # For testing, limit to same duration as blendshapes (~33 seconds)\n",
        "    # Remove max_duration=33.33 to process the entire video\n",
        "    features = extractor.extract_from_video(\n",
        "        video_path,\n",
        "        output_dir=f\"{root_path}/data/extracted_features\",\n",
        "        # max_duration=33.33  # Match the 1000 frames we extracted earlier\n",
        "    )\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"AUDIO FEATURE EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Audio file: {features['audio_path']}\")\n",
        "    print(f\"Duration: {features['duration_seconds']:.2f} seconds\")\n",
        "    print(f\"Sample rate: {features['sample_rate']} Hz\")\n",
        "    print(f\"Mel features: {features['n_mels']}\")\n",
        "    print(f\"Number of frames: {features['n_frames']}\")\n",
        "    print(f\"Mel frame rate: {features['mel_frame_rate']:.1f} Hz\")\n",
        "    print(f\"Feature dimensions: {len(features['mel_features'])} x {len(features['mel_features'][0])}\")\n",
        "    \n",
        "    # Voice activity statistics\n",
        "    vad_ratio = np.mean(features['voice_activity'])\n",
        "    print(f\"Voice activity: {vad_ratio:.1%} of frames\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"Now doing step 2 extract audio features for all videos in the dataset\")\n",
        "for f in dataset_results[\"downloaded_files\"]:\n",
        "    if f[\"filename\"].endswith(\".mp4\"):\n",
        "        video_path = f[\"local_path\"]\n",
        "        print(\"will now process step 2 for video\" , video_path)\n",
        "        step2_main(video_path)\n",
        "        print(\"finsihed step2  for video\" , f[\"filename\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating synchronized training dataset with PROPER normalization...\n",
            "FIXED: This version FIXES the z-score over-normalization problem!\n",
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Dataset Creator initialized:\n",
            "  Sequence length: 240ms\n",
            "  Overlap: 120ms\n",
            "  Step size: 120ms\n",
            "\\nLoading extracted features...\n",
            "Loading features from 1 videos: ['test']\n",
            "\n",
            "Loading features for video: test\n",
            "  Audio: 3339 frames\n",
            "  Visual: 1002 frames\n",
            "\n",
            "Combined dataset:\n",
            "  Total audio frames: 3339\n",
            "  Total visual frames: 1002\n",
            "\\nSynchronizing audio and visual features...\n",
            "\\nSynchronization details:\n",
            "  Audio: 3339 frames, 0.000s to 33.380s\n",
            "  Visual: 1002 frames, 0.000s to 33.366s\n",
            "  Common range: 0.000s to 33.366s (33.366s)\n",
            "\\nSynchronized dataset:\n",
            "  Duration: 33.36 seconds\n",
            "  Audio features: (3337, 80)\n",
            "  Target features: (3337, 59)\n",
            "  Face detection rate: 99.9%\n",
            "\\nCreating training sequences...\n",
            "\\nCreating sequences:\n",
            "  Sequence length: 24 frames (240ms)\n",
            "  Step size: 12 frames (120ms)\n",
            "  Generated 277 sequences\n",
            "  Audio sequences shape: (277, 24, 80)\n",
            "  Target sequences shape: (277, 24, 59)\n",
            "\\nApplying PROPER normalization...\n",
            "\\n=== APPLYING PROPER NORMALIZATION (NO Z-SCORE!) ===\n",
            "  Audio features: (6648, 80)\n",
            "  Target features: (6648, 59)\n",
            "\\nORIGINAL RANGES:\n",
            "  Audio: [-80.000, 0.000]\n",
            "  Targets: [-0.294, 1.000]\n",
            "\\n--- AUDIO NORMALIZATION ---\n",
            "Method: Clipping to natural mel spectrogram dB range\n",
            "Audio after normalization: [-80.000, 0.000]\n",
            "Audio mean: -58.991, std: 15.124\n",
            "\\n--- TARGET NORMALIZATION ---\n",
            "Blendshapes: (6648, 52)\n",
            "Pose: (6648, 7)\n",
            "Method: Clipping blendshapes to natural [0, 1] range\n",
            "Blendshapes after normalization: [0.000, 0.976]\n",
            "Blendshapes mean: 0.088, std: 0.167\n",
            "Pose after normalization: [-0.294, 1.000]\n",
            "\\nFinal target range: [-0.294, 1.000]\n",
            "Final target mean: 0.089, std: 0.206\n",
            "\\n=== NORMALIZATION VALIDATION ===\n",
            "OK: Audio variation preserved (std: 15.124)\n",
            "OK: Target variation preserved (std: 0.206)\n",
            "OK: Audio normalization looks good (not z-score)\n",
            "OK: Target normalization looks good (not z-score)\n",
            "\\nOK: Normalization parameters saved to /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/extracted_features/normalization_params.json\n",
            "\\nSaving dataset...\n",
            "\\nDataset saved to /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/training_dataset\n",
            "  Files: audio_sequences.npy, target_sequences.npy, vad_sequences.npy, dataset_metadata.json\n",
            "  Normalization method: proper_scaling_preserves_natural_ranges\n",
            "\\n============================================================\n",
            "DATASET CREATION SUMMARY - FIXED NORMALIZATION\n",
            "============================================================\n",
            "Output directory: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/training_dataset\n",
            "Number of sequences: 277\n",
            "Sequence length: 240ms\n",
            "Audio features per sequence: 24 x 80\n",
            "Target features per sequence: 24 x 59\n",
            "Normalization method: proper_scaling_preserves_natural_ranges\n",
            "\\nSUCCESS: Ready for TCN training with PROPER data!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 3 to create the dataset with all the extracted features\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "FIXED Dataset Creation Script\n",
        "Synchronizes audio features with blendshape/pose targets for training\n",
        "WITH PROPER NORMALIZATION (no more z-score destruction)\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "import joblib\n",
        "\n",
        "class DatasetCreator:\n",
        "    def __init__(self, sequence_length_ms=240, overlap_ms=120):\n",
        "        \"\"\"\n",
        "        Initialize dataset creator\n",
        "        \n",
        "        Args:\n",
        "            sequence_length_ms: Length of input sequences in milliseconds (240-320ms recommended)\n",
        "            overlap_ms: Overlap between sequences in milliseconds\n",
        "        \"\"\"\n",
        "        self.sequence_length_ms = sequence_length_ms\n",
        "        self.overlap_ms = overlap_ms\n",
        "        \n",
        "        print(f\"Dataset Creator initialized:\")\n",
        "        print(f\"  Sequence length: {sequence_length_ms}ms\")\n",
        "        print(f\"  Overlap: {overlap_ms}ms\")\n",
        "        print(f\"  Step size: {sequence_length_ms - overlap_ms}ms\")\n",
        "    \n",
        "    def load_features(self, features_dir=\"extracted_features\"):\n",
        "        \"\"\"\n",
        "        Load both audio and visual features\n",
        "        \n",
        "        Args:\n",
        "            features_dir: Directory containing extracted features\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (audio_data, visual_data)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        features_path = Path(features_dir)\n",
        "        \n",
        "        # Try both .json and .npy formats for compatibility\n",
        "        audio_file_json = features_path / \"audio_features.json\"\n",
        "        audio_file_npy = features_path / \"audio_features.npy\"\n",
        "        \n",
        "        if audio_file_npy.exists():\n",
        "            # Load numpy format (more common now)\n",
        "            audio_features = np.load(audio_file_npy)\n",
        "            print(f\"Loaded audio features from .npy: {audio_features.shape}\")\n",
        "            \n",
        "            # Create compatible format\n",
        "            audio_data = {\n",
        "                'mel_features': audio_features.tolist(),\n",
        "                'timestamps': (np.arange(len(audio_features)) / 100.0).tolist(),  # 100 FPS assumption\n",
        "                'voice_activity': np.ones(len(audio_features)).tolist()  # Placeholder VAD\n",
        "            }\n",
        "        elif audio_file_json.exists():\n",
        "            # Load JSON format\n",
        "            with open(audio_file_json, 'r') as f:\n",
        "                audio_data = json.load(f)\n",
        "            print(f\"Loaded audio features from .json: {len(audio_data['mel_features'])} frames\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Audio features not found: {audio_file_npy} or {audio_file_json}\")\n",
        "        \n",
        "        # Load visual features\n",
        "        visual_file_json = features_path / \"blendshapes_and_pose.json\"\n",
        "        visual_file_npy = features_path / \"blendshape_features.npy\"\n",
        "        \n",
        "        if visual_file_npy.exists():\n",
        "            # Load numpy format\n",
        "            visual_features = np.load(visual_file_npy)\n",
        "            print(f\"Loaded visual features from .npy: {visual_features.shape}\")\n",
        "            \n",
        "            # Create compatible format\n",
        "            visual_data = {\n",
        "                'frames': []\n",
        "            }\n",
        "            for i, frame_features in enumerate(visual_features):\n",
        "                visual_data['frames'].append({\n",
        "                    'timestamp': i / 30.0,  # 30 FPS assumption\n",
        "                    'blendshapes': frame_features[:52].tolist(),  # First 52 are blendshapes\n",
        "                    'head_pose': frame_features[52:59].tolist() if len(frame_features) >= 59 else [0]*7,\n",
        "                    'has_face': True\n",
        "                })\n",
        "        elif visual_file_json.exists():\n",
        "            # Load JSON format\n",
        "            with open(visual_file_json, 'r') as f:\n",
        "                visual_data = json.load(f)\n",
        "            print(f\"Loaded visual features from .json: {len(visual_data['frames'])} frames\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Visual features not found: {visual_file_npy} or {visual_file_json}\")\n",
        "        \n",
        "        return audio_data, visual_data\n",
        "\n",
        "\n",
        "    def load_features_multi_video(self, features_dir=\"extracted_features\", dataset_results=None):\n",
        "        \"\"\"\n",
        "        Load audio and visual features from multiple videos\n",
        "        \n",
        "        Args:\n",
        "            features_dir: Directory containing extracted features\n",
        "            dataset_results: Dataset results containing video file info\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (combined_audio_data, combined_visual_data)\n",
        "        \"\"\"\n",
        "        all_audio_data = []\n",
        "        all_visual_data = []\n",
        "        \n",
        "        # Get list of video names from dataset_results\n",
        "        video_names = []\n",
        "        for f in dataset_results[\"downloaded_files\"]:\n",
        "            if f[\"filename\"].endswith(\".mp4\"):\n",
        "                video_name = os.path.splitext(os.path.basename(f[\"local_path\"]))[0]\n",
        "                video_names.append(video_name)\n",
        "        \n",
        "        print(f\"Loading features from {len(video_names)} videos: {video_names}\")\n",
        "        \n",
        "        for video_name in video_names:\n",
        "            print(f\"\\nLoading features for video: {video_name}\")\n",
        "            \n",
        "            features_path = Path(features_dir)\n",
        "            \n",
        "            # Load audio features for this video\n",
        "            audio_file = features_path / f\"audio_features_{video_name}.json\"\n",
        "            if audio_file.exists():\n",
        "                with open(audio_file, 'r') as f:\n",
        "                    audio_data = json.load(f)\n",
        "                print(f\"  Audio: {len(audio_data['mel_features'])} frames\")\n",
        "                all_audio_data.append(audio_data)\n",
        "            else:\n",
        "                print(f\"  WARNING: Audio features not found for {video_name}\")\n",
        "            \n",
        "            # Load visual features for this video  \n",
        "            visual_file = features_path / f\"blendshapes_and_pose_{video_name}.json\"\n",
        "            if visual_file.exists():\n",
        "                with open(visual_file, 'r') as f:\n",
        "                    visual_data = json.load(f)\n",
        "                print(f\"  Visual: {len(visual_data['frames'])} frames\")\n",
        "                all_visual_data.append(visual_data)\n",
        "            else:\n",
        "                print(f\"  WARNING: Visual features not found for {video_name}\")\n",
        "        \n",
        "        # Combine all audio data\n",
        "        combined_audio = {\n",
        "            'mel_features': [],\n",
        "            'timestamps': [],\n",
        "            'voice_activity': []\n",
        "        }\n",
        "        \n",
        "        current_time_offset = 0.0\n",
        "        for audio_data in all_audio_data:\n",
        "            # Adjust timestamps to be continuous across videos\n",
        "            adjusted_timestamps = [t + current_time_offset for t in audio_data['timestamps']]\n",
        "            \n",
        "            combined_audio['mel_features'].extend(audio_data['mel_features'])\n",
        "            combined_audio['timestamps'].extend(adjusted_timestamps)\n",
        "            combined_audio['voice_activity'].extend(audio_data['voice_activity'])\n",
        "            \n",
        "            # Update offset for next video\n",
        "            current_time_offset = adjusted_timestamps[-1] + 0.1  # Small gap between videos\n",
        "        \n",
        "        # Combine all visual data\n",
        "        combined_visual = {'frames': []}\n",
        "        \n",
        "        current_time_offset = 0.0\n",
        "        for visual_data in all_visual_data:\n",
        "            for frame in visual_data['frames']:\n",
        "                # Adjust timestamp\n",
        "                adjusted_frame = frame.copy()\n",
        "                adjusted_frame['timestamp'] = frame['timestamp'] + current_time_offset\n",
        "                combined_visual['frames'].append(adjusted_frame)\n",
        "            \n",
        "            # Update offset for next video\n",
        "            if visual_data['frames']:\n",
        "                current_time_offset = visual_data['frames'][-1]['timestamp'] + current_time_offset + 0.1\n",
        "        \n",
        "        print(f\"\\nCombined dataset:\")\n",
        "        print(f\"  Total audio frames: {len(combined_audio['mel_features'])}\")\n",
        "        print(f\"  Total visual frames: {len(combined_visual['frames'])}\")\n",
        "        \n",
        "        return combined_audio, combined_visual\n",
        "    \n",
        "    def synchronize_features(self, audio_data, visual_data):\n",
        "        \"\"\"\n",
        "        Synchronize audio and visual features using timestamps\n",
        "        \n",
        "        Args:\n",
        "            audio_data: Audio features dict\n",
        "            visual_data: Visual features dict\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (synchronized_audio, synchronized_targets, metadata)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Extract data arrays\n",
        "        audio_features = np.array(audio_data['mel_features'])  # Shape: (time, 80)\n",
        "        audio_timestamps = np.array(audio_data['timestamps'])\n",
        "        audio_vad = np.array(audio_data['voice_activity'])\n",
        "        \n",
        "        # Extract visual features and timestamps\n",
        "        visual_frames = visual_data['frames']\n",
        "        # Convert timestamps from milliseconds to seconds if needed\n",
        "        visual_timestamps = np.array([frame['timestamp'] for frame in visual_frames])\n",
        "        if visual_timestamps.max() > 1000:  # Likely in milliseconds\n",
        "            visual_timestamps = visual_timestamps / 1000.0\n",
        "        \n",
        "        # Combine blendshapes and pose into targets (59 values per frame)\n",
        "        targets = []\n",
        "        has_face_flags = []\n",
        "        \n",
        "        # Define the expected blendshape order (52 categories)\n",
        "        blendshape_names = [\n",
        "            '_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft', \n",
        "            'browOuterUpRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft', \n",
        "            'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft', 'eyeLookInRight', \n",
        "            'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft', \n",
        "            'eyeSquintRight', 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen', \n",
        "            'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight', 'mouthFrownLeft', \n",
        "            'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight', \n",
        "            'mouthPressLeft', 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower', \n",
        "            'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft', 'mouthSmileRight', \n",
        "            'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight', \n",
        "            'noseSneerLeft', 'noseSneerRight'\n",
        "        ]\n",
        "        \n",
        "        for frame in visual_frames:\n",
        "            # Extract blendshapes in the correct order\n",
        "            if isinstance(frame['blendshapes'], dict):\n",
        "                # New format: dictionary with named blendshapes\n",
        "                blendshapes_list = []\n",
        "                for name in blendshape_names:\n",
        "                    blendshapes_list.append(frame['blendshapes'].get(name, 0.0))\n",
        "            else:\n",
        "                # Old format: already a list\n",
        "                blendshapes_list = frame['blendshapes']\n",
        "            \n",
        "            # Extract head pose (position + rotation = 7 values: x,y,z,qw,qx,qy,qz)\n",
        "            if 'headPosition' in frame and 'headRotation' in frame:\n",
        "                # New format: separate position and rotation objects\n",
        "                head_pose = [\n",
        "                    frame['headPosition']['x'],\n",
        "                    frame['headPosition']['y'], \n",
        "                    frame['headPosition']['z'],\n",
        "                    frame['headRotation']['w'],\n",
        "                    frame['headRotation']['x'],\n",
        "                    frame['headRotation']['y'],\n",
        "                    frame['headRotation']['z']\n",
        "                ]\n",
        "            elif 'head_pose' in frame:\n",
        "                # Old format: already a list\n",
        "                head_pose = frame['head_pose']\n",
        "            else:\n",
        "                # Fallback: default values\n",
        "                head_pose = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
        "            \n",
        "            # Combine 52 blendshapes + 7 head pose values = 59 total\n",
        "            target = blendshapes_list + head_pose\n",
        "            targets.append(target)\n",
        "            has_face_flags.append(frame['has_face'])\n",
        "        \n",
        "        targets = np.array(targets)  # Shape: (time, 59)\n",
        "        has_face_flags = np.array(has_face_flags)\n",
        "        \n",
        "        print(f\"\\\\nSynchronization details:\")\n",
        "        print(f\"  Audio: {len(audio_features)} frames, {audio_timestamps[0]:.3f}s to {audio_timestamps[-1]:.3f}s\")\n",
        "        print(f\"  Visual: {len(targets)} frames, {visual_timestamps[0]:.3f}s to {visual_timestamps[-1]:.3f}s\")\n",
        "        \n",
        "        # Find common time range\n",
        "        start_time = max(audio_timestamps[0], visual_timestamps[0])\n",
        "        end_time = min(audio_timestamps[-1], visual_timestamps[-1])\n",
        "        \n",
        "        print(f\"  Common range: {start_time:.3f}s to {end_time:.3f}s ({end_time-start_time:.3f}s)\")\n",
        "        \n",
        "        # Interpolate visual features to audio timestamps\n",
        "        # This upsamples visual features from 30fps to 100fps (audio frame rate)\n",
        "        \n",
        "        # Filter audio to common time range\n",
        "        audio_mask = (audio_timestamps >= start_time) & (audio_timestamps <= end_time)\n",
        "        sync_audio_timestamps = audio_timestamps[audio_mask]\n",
        "        sync_audio_features = audio_features[audio_mask]\n",
        "        sync_audio_vad = audio_vad[audio_mask]\n",
        "        \n",
        "        # Interpolate visual targets to audio timestamps\n",
        "        sync_targets = np.zeros((len(sync_audio_timestamps), 59))\n",
        "        sync_has_face = np.zeros(len(sync_audio_timestamps), dtype=bool)\n",
        "        \n",
        "        for i, target_dim in enumerate(range(59)):\n",
        "            target_values = targets[:, target_dim]\n",
        "            sync_targets[:, i] = np.interp(sync_audio_timestamps, visual_timestamps, target_values)\n",
        "        \n",
        "        # Interpolate face detection flags\n",
        "        face_values = has_face_flags.astype(float)\n",
        "        interpolated_face = np.interp(sync_audio_timestamps, visual_timestamps, face_values)\n",
        "        sync_has_face = interpolated_face > 0.5  # Threshold for face presence\n",
        "        \n",
        "        print(f\"\\\\nSynchronized dataset:\")\n",
        "        print(f\"  Duration: {sync_audio_timestamps[-1] - sync_audio_timestamps[0]:.2f} seconds\")\n",
        "        print(f\"  Audio features: {sync_audio_features.shape}\")\n",
        "        print(f\"  Target features: {sync_targets.shape}\")\n",
        "        print(f\"  Face detection rate: {np.mean(sync_has_face):.1%}\")\n",
        "        \n",
        "        # Create metadata\n",
        "        metadata = {\n",
        "            'duration_seconds': float(sync_audio_timestamps[-1] - sync_audio_timestamps[0]),\n",
        "            'sample_rate_hz': float(len(sync_audio_timestamps) / (sync_audio_timestamps[-1] - sync_audio_timestamps[0])),\n",
        "            'num_frames': len(sync_audio_timestamps),\n",
        "            'audio_features_dim': sync_audio_features.shape[1],\n",
        "            'target_features_dim': sync_targets.shape[1],\n",
        "            'face_detection_rate': float(np.mean(sync_has_face)),\n",
        "            'voice_activity_rate': float(np.mean(sync_audio_vad))\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'audio_features': sync_audio_features,\n",
        "            'targets': sync_targets,\n",
        "            'timestamps': sync_audio_timestamps,\n",
        "            'voice_activity': sync_audio_vad,\n",
        "            'has_face': sync_has_face,\n",
        "            'metadata': metadata\n",
        "        }\n",
        "    \n",
        "    def create_sequences(self, synchronized_data):\n",
        "        \"\"\"\n",
        "        Create training sequences from synchronized data\n",
        "        \n",
        "        Args:\n",
        "            synchronized_data: Output from synchronize_features()\n",
        "        \n",
        "        Returns:\n",
        "            dict: Training sequences\n",
        "        \"\"\"\n",
        "        audio_features = synchronized_data['audio_features']\n",
        "        targets = synchronized_data['targets']\n",
        "        voice_activity = synchronized_data['voice_activity']\n",
        "        has_face = synchronized_data['has_face']\n",
        "        timestamps = synchronized_data['timestamps']\n",
        "        \n",
        "        # Calculate sequence parameters\n",
        "        sample_rate = synchronized_data['metadata']['sample_rate_hz']\n",
        "        seq_length_frames = int(self.sequence_length_ms * sample_rate / 1000)\n",
        "        step_size_frames = int((self.sequence_length_ms - self.overlap_ms) * sample_rate / 1000)\n",
        "        \n",
        "        print(f\"\\\\nCreating sequences:\")\n",
        "        print(f\"  Sequence length: {seq_length_frames} frames ({self.sequence_length_ms}ms)\")\n",
        "        print(f\"  Step size: {step_size_frames} frames ({self.sequence_length_ms - self.overlap_ms}ms)\")\n",
        "        \n",
        "        # Generate sequences\n",
        "        sequences_audio = []\n",
        "        sequences_targets = []\n",
        "        sequences_vad = []\n",
        "        sequences_face = []\n",
        "        sequences_timestamps = []\n",
        "        \n",
        "        for start_idx in range(0, len(audio_features) - seq_length_frames + 1, step_size_frames):\n",
        "            end_idx = start_idx + seq_length_frames\n",
        "            \n",
        "            # Extract sequence\n",
        "            seq_audio = audio_features[start_idx:end_idx]  # Shape: (seq_len, 80)\n",
        "            seq_targets = targets[start_idx:end_idx]       # Shape: (seq_len, 59)\n",
        "            seq_vad = voice_activity[start_idx:end_idx]\n",
        "            seq_face = has_face[start_idx:end_idx]\n",
        "            seq_time = timestamps[start_idx:end_idx]\n",
        "            \n",
        "            # Quality checks\n",
        "            face_ratio = np.mean(seq_face)\n",
        "            vad_ratio = np.mean(seq_vad)\n",
        "            \n",
        "            # Only include sequences with reasonable face detection\n",
        "            if face_ratio >= 0.5:  # At least 50% of frames have face detected\n",
        "                sequences_audio.append(seq_audio)\n",
        "                sequences_targets.append(seq_targets)\n",
        "                sequences_vad.append(seq_vad)\n",
        "                sequences_face.append(seq_face)\n",
        "                sequences_timestamps.append(seq_time)\n",
        "        \n",
        "        sequences_audio = np.array(sequences_audio)      # Shape: (num_seq, seq_len, 80)\n",
        "        sequences_targets = np.array(sequences_targets)  # Shape: (num_seq, seq_len, 59)\n",
        "        sequences_vad = np.array(sequences_vad)          # Shape: (num_seq, seq_len)\n",
        "        sequences_face = np.array(sequences_face)        # Shape: (num_seq, seq_len)\n",
        "        \n",
        "        print(f\"  Generated {len(sequences_audio)} sequences\")\n",
        "        print(f\"  Audio sequences shape: {sequences_audio.shape}\")\n",
        "        print(f\"  Target sequences shape: {sequences_targets.shape}\")\n",
        "        \n",
        "        return {\n",
        "            'audio_sequences': sequences_audio,\n",
        "            'target_sequences': sequences_targets,\n",
        "            'vad_sequences': sequences_vad,\n",
        "            'face_sequences': sequences_face,\n",
        "            'sequence_timestamps': sequences_timestamps,\n",
        "            'metadata': {\n",
        "                'num_sequences': len(sequences_audio),\n",
        "                'sequence_length_frames': seq_length_frames,\n",
        "                'sequence_length_ms': self.sequence_length_ms,\n",
        "                'step_size_frames': step_size_frames,\n",
        "                'overlap_ms': self.overlap_ms,\n",
        "                'audio_feature_dim': sequences_audio.shape[2],\n",
        "                'target_feature_dim': sequences_targets.shape[2]\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def normalize_features(self, sequences_data, output_dir=\"extracted_features\"):\n",
        "        \"\"\"\n",
        "        FIXED NORMALIZATION - Preserves natural scales and relationships\n",
        "        NO MORE Z-SCORE DESTRUCTION!\n",
        "        \n",
        "        Args:\n",
        "            sequences_data: Output from create_sequences()\n",
        "            output_dir: Directory to save normalization parameters\n",
        "        \n",
        "        Returns:\n",
        "            dict: Normalized sequences with scalers\n",
        "        \"\"\"\n",
        "        audio_sequences = sequences_data['audio_sequences']\n",
        "        target_sequences = sequences_data['target_sequences']\n",
        "        \n",
        "        # Reshape for normalization (flatten time dimension)\n",
        "        audio_flat = audio_sequences.reshape(-1, audio_sequences.shape[-1])\n",
        "        targets_flat = target_sequences.reshape(-1, target_sequences.shape[-1])\n",
        "        \n",
        "        print(f\"\\\\n=== APPLYING PROPER NORMALIZATION (NO Z-SCORE!) ===\")\n",
        "        print(f\"  Audio features: {audio_flat.shape}\")\n",
        "        print(f\"  Target features: {targets_flat.shape}\")\n",
        "        \n",
        "        # Check original ranges\n",
        "        print(f\"\\\\nORIGINAL RANGES:\")\n",
        "        print(f\"  Audio: [{audio_flat.min():.3f}, {audio_flat.max():.3f}]\")\n",
        "        print(f\"  Targets: [{targets_flat.min():.3f}, {targets_flat.max():.3f}]\")\n",
        "        \n",
        "        # ============ AUDIO NORMALIZATION ============\n",
        "        print(f\"\\\\n--- AUDIO NORMALIZATION ---\")\n",
        "        print(f\"Method: Clipping to natural mel spectrogram dB range\")\n",
        "        \n",
        "        # Keep audio in natural mel spectrogram range (-80 to 10 dB)\n",
        "        audio_normalized_flat = np.clip(audio_flat, -80.0, 10.0)\n",
        "        \n",
        "        # Optional: Light scaling to improve training stability\n",
        "        # Uncomment if you want to scale to [-1, 1] while preserving relationships:\n",
        "        # audio_normalized_flat = (audio_normalized_flat + 80.0) / 45.0 - 1.0\n",
        "        \n",
        "        print(f\"Audio after normalization: [{audio_normalized_flat.min():.3f}, {audio_normalized_flat.max():.3f}]\")\n",
        "        print(f\"Audio mean: {audio_normalized_flat.mean():.3f}, std: {audio_normalized_flat.std():.3f}\")\n",
        "        \n",
        "        # ============ TARGET NORMALIZATION ============\n",
        "        print(f\"\\\\n--- TARGET NORMALIZATION ---\")\n",
        "        \n",
        "        # Split blendshapes (0-52) and pose (52-59) for different treatment\n",
        "        if targets_flat.shape[1] >= 52:\n",
        "            blendshapes_flat = targets_flat[:, :52]  # First 52 are blendshapes\n",
        "            pose_flat = targets_flat[:, 52:] if targets_flat.shape[1] > 52 else None\n",
        "            \n",
        "            print(f\"Blendshapes: {blendshapes_flat.shape}\")\n",
        "            if pose_flat is not None:\n",
        "                print(f\"Pose: {pose_flat.shape}\")\n",
        "            \n",
        "            # Method 1: Clip blendshapes to natural [0, 1] range\n",
        "            print(f\"Method: Clipping blendshapes to natural [0, 1] range\")\n",
        "            blendshapes_normalized = np.clip(blendshapes_flat, 0.0, 1.0)\n",
        "            \n",
        "            print(f\"Blendshapes after normalization: [{blendshapes_normalized.min():.3f}, {blendshapes_normalized.max():.3f}]\")\n",
        "            print(f\"Blendshapes mean: {blendshapes_normalized.mean():.3f}, std: {blendshapes_normalized.std():.3f}\")\n",
        "            \n",
        "            # Normalize pose if present\n",
        "            if pose_flat is not None:\n",
        "                # For pose, use light clipping since range can vary\n",
        "                pose_normalized = np.clip(pose_flat, -1.0, 1.0)\n",
        "                print(f\"Pose after normalization: [{pose_normalized.min():.3f}, {pose_normalized.max():.3f}]\")\n",
        "                \n",
        "                # Combine back\n",
        "                targets_normalized_flat = np.concatenate([blendshapes_normalized, pose_normalized], axis=1)\n",
        "            else:\n",
        "                targets_normalized_flat = blendshapes_normalized\n",
        "                \n",
        "        else:\n",
        "            # Fallback for unexpected feature count\n",
        "            print(f\"Unexpected target feature count ({targets_flat.shape[1]}), using clipping to [0, 1]\")\n",
        "            targets_normalized_flat = np.clip(targets_flat, 0.0, 1.0)\n",
        "        \n",
        "        print(f\"\\\\nFinal target range: [{targets_normalized_flat.min():.3f}, {targets_normalized_flat.max():.3f}]\")\n",
        "        print(f\"Final target mean: {targets_normalized_flat.mean():.3f}, std: {targets_normalized_flat.std():.3f}\")\n",
        "        \n",
        "        # Reshape back to sequences\n",
        "        audio_normalized = audio_normalized_flat.reshape(audio_sequences.shape)\n",
        "        targets_normalized = targets_normalized_flat.reshape(target_sequences.shape)\n",
        "        \n",
        "        # ============ VALIDATION ============\n",
        "        print(f\"\\\\n=== NORMALIZATION VALIDATION ===\")\n",
        "        audio_std = audio_normalized.std()\n",
        "        target_std = targets_normalized.std()\n",
        "        \n",
        "        # Check that we didn't destroy variation\n",
        "        if audio_std < 0.1:\n",
        "            print(f\"WARNING: Audio std ({audio_std:.3f}) is very low - may indicate over-normalization\")\n",
        "        else:\n",
        "            print(f\"OK: Audio variation preserved (std: {audio_std:.3f})\")\n",
        "            \n",
        "        if target_std < 0.05:\n",
        "            print(f\"WARNING: Target std ({target_std:.3f}) is very low - may indicate over-normalization\")\n",
        "        else:\n",
        "            print(f\"OK: Target variation preserved (std: {target_std:.3f})\")\n",
        "        \n",
        "        # Check that we're NOT in z-score territory\n",
        "        if abs(audio_normalized.mean()) < 0.01 and abs(audio_normalized.std() - 1.0) < 0.01:\n",
        "            print(f\"ERROR: Audio shows z-score pattern! This should not happen.\")\n",
        "        else:\n",
        "            print(f\"OK: Audio normalization looks good (not z-score)\")\n",
        "            \n",
        "        if abs(targets_normalized.mean()) < 0.01 and abs(targets_normalized.std() - 1.0) < 0.01:\n",
        "            print(f\"ERROR: Targets show z-score pattern! This should not happen.\")\n",
        "        else:\n",
        "            print(f\"OK: Target normalization looks good (not z-score)\")\n",
        "        \n",
        "        # Create dummy scalers for compatibility (though we're not using StandardScaler anymore)\n",
        "        output_path = Path(output_dir)\n",
        "        \n",
        "        # Save the normalization parameters for inference\n",
        "        normalization_params = {\n",
        "            'audio_method': 'clipping',\n",
        "            'audio_min': -80.0,\n",
        "            'audio_max': 10.0,\n",
        "            'target_method': 'clipping',\n",
        "            'blendshape_min': 0.0,\n",
        "            'blendshape_max': 1.0,\n",
        "            'pose_min': -1.0,\n",
        "            'pose_max': 1.0\n",
        "        }\n",
        "        \n",
        "        with open(output_path / \"normalization_params.json\", 'w') as f:\n",
        "            json.dump(normalization_params, f, indent=2)\n",
        "        \n",
        "        print(f\"\\\\nOK: Normalization parameters saved to {output_path / 'normalization_params.json'}\")\n",
        "        \n",
        "        # Update sequences data\n",
        "        normalized_data = sequences_data.copy()\n",
        "        normalized_data['audio_sequences'] = audio_normalized\n",
        "        normalized_data['target_sequences'] = targets_normalized\n",
        "        normalized_data['normalization_method'] = 'proper_scaling_preserves_natural_ranges'\n",
        "        \n",
        "        # Add normalization stats (but these are NOT z-score stats!)\n",
        "        normalized_data['normalization_stats'] = {\n",
        "            'method': 'clipping_to_natural_ranges',\n",
        "            'audio_range': [float(audio_normalized.min()), float(audio_normalized.max())],\n",
        "            'audio_mean': float(audio_normalized.mean()),\n",
        "            'audio_std': float(audio_normalized.std()),\n",
        "            'target_range': [float(targets_normalized.min()), float(targets_normalized.max())],\n",
        "            'target_mean': float(targets_normalized.mean()),\n",
        "            'target_std': float(targets_normalized.std())\n",
        "        }\n",
        "        \n",
        "        return normalized_data\n",
        "    \n",
        "    def save_dataset(self, dataset, output_dir=\"extracted_features\"):\n",
        "        \"\"\"\n",
        "        Save the final dataset\n",
        "        \n",
        "        Args:\n",
        "            dataset: Final dataset from normalize_features()\n",
        "            output_dir: Directory to save dataset\n",
        "        \"\"\"\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Save sequences as numpy arrays (more efficient for training)\n",
        "        np.save(output_path / \"audio_sequences.npy\", dataset['audio_sequences'])\n",
        "        np.save(output_path / \"target_sequences.npy\", dataset['target_sequences'])\n",
        "        np.save(output_path / \"vad_sequences.npy\", dataset['vad_sequences'])\n",
        "        # Note: Saving as 'vad_sequences.npy' for compatibility with diagnostic script\n",
        "        \n",
        "        # Save metadata as JSON\n",
        "        metadata = {\n",
        "            'dataset_info': dataset['metadata'],\n",
        "            'normalization_stats': dataset['normalization_stats'],\n",
        "            'normalization_method': dataset.get('normalization_method', 'proper_scaling')\n",
        "        }\n",
        "        \n",
        "        with open(output_path / \"dataset_metadata.json\", 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"\\\\nDataset saved to {output_path}\")\n",
        "        print(f\"  Files: audio_sequences.npy, target_sequences.npy, vad_sequences.npy, dataset_metadata.json\")\n",
        "        print(f\"  Normalization method: {dataset.get('normalization_method', 'proper_scaling')}\")\n",
        "        \n",
        "        return output_path\n",
        "\n",
        "def step3_main( dataset_results ):\n",
        "    \"\"\"\n",
        "    Main function to create the training dataset with PROPER normalization\n",
        "    \"\"\"\n",
        "    print(\"Creating synchronized training dataset with PROPER normalization...\")\n",
        "    print(\"FIXED: This version FIXES the z-score over-normalization problem!\")\n",
        "\n",
        "    # root path\n",
        "    root_path = Path.cwd()  # use current working directory in Colab\n",
        "    print(f\"Root path: {root_path}\")\n",
        "    \n",
        "    # Initialize dataset creator\n",
        "    # 240ms sequences with 120ms overlap = 120ms step size\n",
        "    creator = DatasetCreator(sequence_length_ms=240, overlap_ms=120)\n",
        "    \n",
        "    # Load features\n",
        "    print(\"\\\\nLoading extracted features...\")\n",
        "    audio_data, visual_data = creator.load_features_multi_video(f\"{root_path}/data/extracted_features\", dataset_results)\n",
        "    # audio_data, visual_data = creator.load_features(f\"{root_path}/data/extracted_features\",)\n",
        "    \n",
        "    # Synchronize features\n",
        "    print(\"\\\\nSynchronizing audio and visual features...\")\n",
        "    synchronized_data = creator.synchronize_features(audio_data, visual_data)\n",
        "    \n",
        "    # Create training sequences\n",
        "    print(\"\\\\nCreating training sequences...\")\n",
        "    sequences_data = creator.create_sequences(synchronized_data)\n",
        "    \n",
        "    # Apply PROPER normalization (no more z-score!)\n",
        "    print(\"\\\\nApplying PROPER normalization...\")\n",
        "    final_dataset = creator.normalize_features(sequences_data, f\"{root_path}/data/extracted_features\")\n",
        "    \n",
        "    # Save dataset\n",
        "    print(\"\\\\nSaving dataset...\")\n",
        "    output_path = creator.save_dataset(final_dataset, f\"{root_path}/data/training_dataset\")\n",
        "    \n",
        "    # Print final summary\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"DATASET CREATION SUMMARY - FIXED NORMALIZATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Output directory: {output_path}\")\n",
        "    print(f\"Number of sequences: {final_dataset['metadata']['num_sequences']}\")\n",
        "    print(f\"Sequence length: {final_dataset['metadata']['sequence_length_ms']}ms\")\n",
        "    print(f\"Audio features per sequence: {final_dataset['metadata']['sequence_length_frames']} x {final_dataset['metadata']['audio_feature_dim']}\")\n",
        "    print(f\"Target features per sequence: {final_dataset['metadata']['sequence_length_frames']} x {final_dataset['metadata']['target_feature_dim']}\")\n",
        "    print(f\"Normalization method: {final_dataset.get('normalization_method', 'proper_scaling')}\")\n",
        "    print(f\"\\\\nSUCCESS: Ready for TCN training with PROPER data!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "step3_main( dataset_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root path: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning\n",
            "Now uploading the training dataset to the hub\n",
            "The training Dataset is /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/1_data_cleaning/data/training_dataset\n",
            "imported huggingface_hub\n",
            "made the api object <huggingface_hub.hf_api.HfApi object at 0x30997c410>\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m api = HfApi(token=HF_TOKEN)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmade the api object\u001b[39m\u001b[33m\"\u001b[39m , api)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mroot_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data/training_dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msanjuhs/audio_to_blendshapes_test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33muploaded the dataset to the hub!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1669\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1668\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4982\u001b[39m, in \u001b[36mHfApi.upload_folder\u001b[39m\u001b[34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[39m\n\u001b[32m   4978\u001b[39m commit_operations = delete_operations + add_operations\n\u001b[32m   4980\u001b[39m commit_message = commit_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUpload folder using huggingface_hub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4982\u001b[39m commit_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4985\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4992\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4994\u001b[39m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[32m   4995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info.pr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1669\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1668\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4239\u001b[39m, in \u001b[36mHfApi.create_commit\u001b[39m\u001b[34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[39m\n\u001b[32m   4236\u001b[39m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[32m   4237\u001b[39m _warn_on_overwriting_operations(operations)\n\u001b[32m-> \u001b[39m\u001b[32m4239\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4241\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[32m   4245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[32m   4248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4250\u001b[39m files_to_copy = _fetch_files_to_copy(\n\u001b[32m   4251\u001b[39m     copies=copies,\n\u001b[32m   4252\u001b[39m     repo_type=repo_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4256\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.endpoint,\n\u001b[32m   4257\u001b[39m )\n\u001b[32m   4258\u001b[39m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4465\u001b[39m, in \u001b[36mHfApi.preupload_lfs_files\u001b[39m\u001b[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[39m\n\u001b[32m   4463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(additions_no_upload_mode) > \u001b[32m0\u001b[39m:\n\u001b[32m   4464\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4465\u001b[39m         \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4466\u001b[39m \u001b[43m            \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions_no_upload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4467\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4468\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4469\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4470\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4471\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4472\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4473\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4474\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4475\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4476\u001b[39m         e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:686\u001b[39m, in \u001b[36m_fetch_upload_modes\u001b[39m\u001b[34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gitignore_content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    684\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33mgitIgnore\u001b[39m\u001b[33m\"\u001b[39m] = gitignore_content\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m resp = \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mendpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43ms/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/preupload/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrevision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreate_pr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m hf_raise_for_status(resp)\n\u001b[32m    693\u001b[39m preupload_info = _validate_preupload_info(resp.json())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:637\u001b[39m, in \u001b[36mSession.post\u001b[39m\u001b[34m(self, url, data, json, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    627\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# now i need to use hugging face token to upload the training dataset to the hub\n",
        "\n",
        "root_path = Path.cwd()  # use current working directory in Colab\n",
        "print(f\"Root path: {root_path}\")\n",
        "\n",
        "print(\"Now uploading the training dataset to the hub\")\n",
        "print(\"The training Dataset is\" , f\"{root_path}/data/training_dataset\")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "print(\"imported huggingface_hub\")\n",
        "# load the dotenv file\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "# print(\"HF_TOKEN\" , HF_TOKEN)\n",
        "\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "print(\"made the api object\" , api)\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=f\"{root_path}/data/training_dataset\",\n",
        "    repo_id=\"sanjuhs/audio_to_blendshapes_test\",\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "print(\"uploaded the dataset to the hub!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m api = HfApi(token=HF_TOKEN)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Test connection by getting user info\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     user_info = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Token valid for user: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_info[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Check if repo exists\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1777\u001b[39m, in \u001b[36mHfApi.whoami\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1775\u001b[39m \u001b[38;5;66;03m# Get the effective token using the helper function get_token\u001b[39;00m\n\u001b[32m   1776\u001b[39m effective_token = token \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token \u001b[38;5;129;01mor\u001b[39;00m get_token() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m r = \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/api/whoami-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1782\u001b[39m     hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Test token validity\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "try:\n",
        "    # Test connection by getting user info\n",
        "    user_info = api.whoami()\n",
        "    print(f\"âœ… Token valid for user: {user_info['name']}\")\n",
        "    \n",
        "    # Check if repo exists\n",
        "    try:\n",
        "        repo_info = api.repo_info(\"sanjuhs/audio_to_blendshapes_test\", repo_type=\"dataset\")\n",
        "        print(f\"âœ… Repository exists and accessible\")\n",
        "    except:\n",
        "        print(\"â„¹ï¸  Repository doesn't exist yet, will be created during upload\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Token/connection issue: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing basic connectivity...\n",
            "âœ… huggingface.co reachable in 80.12s - Status: 200\n",
            "âœ… HF API reachable in 80.41s - Status: 401\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "print(\"Testing basic connectivity...\")\n",
        "\n",
        "try:\n",
        "    start = time.time()\n",
        "    response = requests.get(\"https://huggingface.co\", timeout=10)\n",
        "    end = time.time()\n",
        "    print(f\"âœ… huggingface.co reachable in {end-start:.2f}s - Status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Cannot reach huggingface.co: {e}\")\n",
        "\n",
        "try:\n",
        "    start = time.time()\n",
        "    response = requests.get(\"https://huggingface.co/api/whoami\", \n",
        "                          headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}, \n",
        "                          timeout=10)\n",
        "    end = time.time()\n",
        "    print(f\"âœ… HF API reachable in {end-start:.2f}s - Status: {response.status_code}\")\n",
        "    if response.status_code == 200:\n",
        "        print(f\"User: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Cannot reach HF API: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Huggingface reachable in 40.17s - Status: 200\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# Test a simple, fast site first\n",
        "try:\n",
        "    start = time.time()\n",
        "    url = \"https://google.com\"\n",
        "    url2=\"https://huggingface.co\"\n",
        "    # response = requests.get(url, timeout=5)\n",
        "    response2 = requests.get(url2, timeout=5)\n",
        "    end = time.time()\n",
        "    # print(f\"âœ… Google reachable in {end-start:.2f}s - Status: {response.status_code}\")\n",
        "    print(f\"âœ… Huggingface reachable in {end-start:.2f}s - Status: {response2.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ No internet connection: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
