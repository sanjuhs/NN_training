{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6592e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets get training , first we will build the model then load the data and then test it\n"
     ]
    }
   ],
   "source": [
    "print(\"lets get training , first we will build the model then load the data and then test it\")\n",
    "import socket\n",
    "_orig_getaddrinfo = socket.getaddrinfo\n",
    "def _ipv4_only(host, port, family=0, type=0, proto=0, flags=0):\n",
    "    return _orig_getaddrinfo(host, port, socket.AF_INET, type, proto, flags)\n",
    "socket.getaddrinfo = _ipv4_only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbd46e",
   "metadata": {},
   "source": [
    "# Lets get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc9d2b",
   "metadata": {},
   "source": [
    "### Step 0 load the NN model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3720276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0 load the Neural network / model architecture\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "TCN (Temporal Convolutional Network) for Audio-to-Blendshapes\n",
    "Real-time causal model for audio to facial animation\n",
    "UPDATED: 10+ seconds receptive field for personality and context modeling\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable Convolution for efficiency\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=padding,\n",
    "            groups=in_channels,  # Key: groups=in_channels makes it depthwise\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Pointwise convolution\n",
    "        self.pointwise = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.depthwise(x))\n",
    "        x = self.bn2(self.pointwise(x))\n",
    "        return x\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single TCN block with dilated causal convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        # Calculate padding for causal convolution (no future information)\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        # Depthwise separable convolution\n",
    "        self.conv = DepthwiseSeparableConv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=self.padding\n",
    "        )\n",
    "        \n",
    "        # Activation and normalization\n",
    "        self.activation = nn.GELU()  # GELU as recommended\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with causal convolution\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, time)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, channels, time)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        \n",
    "        # Apply convolution with causal padding\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        # Remove future information (causal)\n",
    "        if self.padding > 0:\n",
    "            out = out[:, :, :-self.padding]\n",
    "        \n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Residual connection\n",
    "        if self.residual_conv is not None:\n",
    "            residual = self.residual_conv(residual)\n",
    "        \n",
    "        # Ensure same sequence length for residual connection\n",
    "        if residual.size(2) != out.size(2):\n",
    "            min_len = min(residual.size(2), out.size(2))\n",
    "            residual = residual[:, :, :min_len]\n",
    "            out = out[:, :, :min_len]\n",
    "        \n",
    "        return out + residual\n",
    "\n",
    "class AudioToBlendshapesTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    TCN model for real-time audio to blendshapes + head pose\n",
    "    UPDATED: 10+ seconds receptive field for personality modeling\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=80,           # Mel features\n",
    "                 output_dim=59,          # 52 blendshapes + 7 head pose\n",
    "                 hidden_channels=256,    # Increased for longer memory\n",
    "                 num_layers=10,          # Increased for 10+ second memory\n",
    "                 kernel_size=3,          # Convolution kernel size\n",
    "                 dropout=0.1,            # Dropout rate\n",
    "                 max_dilation=128,       # Increased for long-term patterns\n",
    "                 custom_dilations=None): # Optional explicit dilations list\n",
    "        \"\"\"\n",
    "        Initialize TCN model with 10+ second receptive field\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension (80 mel features)\n",
    "            output_dim: Output dimension (59: 52 blendshapes + 7 pose)\n",
    "            hidden_channels: Hidden channels in TCN blocks (increased to 256)\n",
    "            num_layers: Number of TCN layers (10 for long memory)\n",
    "            kernel_size: Convolution kernel size\n",
    "            dropout: Dropout rate\n",
    "            max_dilation: Maximum dilation factor (128 for 10+ seconds)\n",
    "            custom_dilations: Optional explicit dilations list (overrides num_layers/max_dilation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        # Calculate dilations\n",
    "        if custom_dilations is not None and len(custom_dilations) > 0:\n",
    "            self.dilations = list(custom_dilations)\n",
    "            self.num_layers = len(self.dilations)\n",
    "        else:\n",
    "            self.num_layers = num_layers\n",
    "            self.dilations = [min(2**i, max_dilation) for i in range(num_layers)]\n",
    "        \n",
    "        # Calculate receptive field\n",
    "        self.receptive_field = self._calculate_receptive_field(kernel_size, self.dilations)\n",
    "        \n",
    "        print(f\"TCN Model Architecture (10+ Second Memory):\")\n",
    "        print(f\"  Input dim: {input_dim}\")\n",
    "        print(f\"  Output dim: {output_dim}\")\n",
    "        print(f\"  Hidden channels: {hidden_channels}\")\n",
    "        print(f\"  Layers: {num_layers}\")\n",
    "        print(f\"  Dilations: {self.dilations}\")\n",
    "        print(f\"  Receptive field: {self.receptive_field} frames ({self.receptive_field*10:.0f}ms = {self.receptive_field*10/1000:.1f}s)\")\n",
    "        print(f\"  üéØ Can see {self.receptive_field*10/1000:.1f} seconds into the past!\")\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_conv = nn.Conv1d(input_dim, hidden_channels, 1)\n",
    "        \n",
    "        # TCN layers\n",
    "        self.tcn_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            in_ch = hidden_channels\n",
    "            out_ch = hidden_channels\n",
    "            dilation = self.dilations[i]\n",
    "            \n",
    "            self.tcn_layers.append(\n",
    "                TCNBlock(\n",
    "                    in_channels=in_ch,\n",
    "                    out_channels=out_ch,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Output layers with proper bounding for blendshapes [0,1] + pose\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Conv1d(hidden_channels, hidden_channels // 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(hidden_channels // 2, output_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Separate output activation: sigmoid for blendshapes [0,1], tanh for pose [-1,1]\n",
    "        self.blendshape_activation = nn.Sigmoid()  # For indices 0-51 (blendshapes)\n",
    "        self.pose_activation = nn.Tanh()           # For indices 52-58 (pose, scaled to reasonable range)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Scale final layer weights for better initial range\n",
    "        with torch.no_grad():\n",
    "            for module in self.output_layers:\n",
    "                if isinstance(module, nn.Conv1d) and hasattr(module, 'weight'):\n",
    "                    module.weight.data *= 0.3  # Slightly larger to avoid vanishing activations\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data *= 0.0\n",
    "        \n",
    "        # Calculate model size\n",
    "        self.num_parameters = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        size_mb = self.num_parameters * 4 / (1024 * 1024)  # 4 bytes per float32\n",
    "        print(f\"  Parameters: {self.num_parameters:,} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Memory usage estimate\n",
    "        print(f\"  Estimated GPU memory for batch_size=8: ~{size_mb * 8:.0f} MB\")\n",
    "    \n",
    "    def _calculate_receptive_field(self, kernel_size, dilations):\n",
    "        \"\"\"Calculate the receptive field of the network\"\"\"\n",
    "        receptive_field = 1\n",
    "        for dilation in dilations:\n",
    "            receptive_field += (kernel_size - 1) * dilation\n",
    "        return receptive_field\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, time, features) or (batch, features, time)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, time, output_dim)\n",
    "        \"\"\"\n",
    "        # Ensure input is (batch, features, time)\n",
    "        if x.dim() == 3 and x.size(-1) == self.input_dim:\n",
    "            x = x.transpose(1, 2)  # (batch, time, features) -> (batch, features, time)\n",
    "        \n",
    "        # Add shape assertion for debugging\n",
    "        assert x.shape[1] == self.input_dim, f\"Expected input dim {self.input_dim}, got {x.shape[1]}\"\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_conv(x)  # (batch, hidden_channels, time)\n",
    "        \n",
    "        # Pass through TCN layers\n",
    "        for tcn_layer in self.tcn_layers:\n",
    "            x = tcn_layer(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_layers(x)  # (batch, output_dim, time)\n",
    "        \n",
    "        # Apply appropriate activations for different output components\n",
    "        # Blendshapes (0-51): sigmoid to [0,1]\n",
    "        blendshapes = self.blendshape_activation(x[:, :52, :])\n",
    "        # Pose (52-58): tanh to [-1,1] for reasonable pose range\n",
    "        pose = self.pose_activation(x[:, 52:, :]) * 0.2  # Scale to [-0.2, 0.2] for pose\n",
    "        \n",
    "        # Combine outputs\n",
    "        x = torch.cat([blendshapes, pose], dim=1)\n",
    "        \n",
    "        # Return as (batch, time, output_dim) for convenience\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model information\"\"\"\n",
    "        return {\n",
    "            'architecture': 'TCN',\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'hidden_channels': self.hidden_channels,\n",
    "            'num_layers': self.num_layers,\n",
    "            'dilations': self.dilations,\n",
    "            'receptive_field_frames': self.receptive_field,\n",
    "            'receptive_field_ms': self.receptive_field * 10,  # Assuming 100Hz\n",
    "            'receptive_field_seconds': self.receptive_field * 10 / 1000,\n",
    "            'num_parameters': self.num_parameters,\n",
    "            'model_size_mb': self.num_parameters * 4 / (1024 * 1024)\n",
    "        }\n",
    "\n",
    "def create_model(config=None):\n",
    "    \"\"\"\n",
    "    Create TCN model with 10+ second memory configuration\n",
    "    \n",
    "    Args:\n",
    "        config: Optional model configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        TCN model instance with 10+ second receptive field\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        # UPDATED: Configuration for 10+ second memory (personality modeling)\n",
    "        config = {\n",
    "            'input_dim': 80,        # 80 mel features\n",
    "            'output_dim': 59,       # 52 blendshapes + 7 head pose\n",
    "            'hidden_channels': 256, # Increased for more capacity\n",
    "            'num_layers': 10,       # Increased for 10+ second memory\n",
    "            'kernel_size': 3,       # Standard kernel size\n",
    "            'dropout': 0.1,         # Light dropout\n",
    "            'max_dilation': 128     # Increased for long-term patterns\n",
    "        }\n",
    "        print(\"üöÄ Using 10+ second memory configuration!\")\n",
    "        print(f\"   Expected receptive field: ~10.2 seconds\")\n",
    "    \n",
    "    model = AudioToBlendshapesTCN(**config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942117c",
   "metadata": {},
   "source": [
    "### Step 1 Download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca937505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies (if needed)...\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "üì¶ Listing files for: sanjuhs/audio_to_blendshapes_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjayprasads/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Found 6 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading -> test:   0%|          | 0/6 [00:00<?, ?it/s]/Users/sanjayprasads/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Downloading -> test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:06<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: test | success: 6, skipped: 0, failed: 0\n",
      "üìÅ Saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/2_architecture_training/data/test\n",
      "\n",
      "üì¶ Listing files for: sanjuhs/audio_to_blendshapes_main\n",
      "üóÇÔ∏è  Found 6 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading -> train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:08<00:00, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: train | success: 6, skipped: 0, failed: 0\n",
      "üìÅ Saved to: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/2_architecture_training/data/train\n",
      "\n",
      "üìä Summary:\n",
      "Test:  success=6, skipped=0, failed=0\n",
      "Train: success=6, skipped=0, failed=0\n",
      "\n",
      "All files are under: /Users/sanjayprasads/Desktop/Coding/Python/NN_training/V2A-over-training-old-nn/2_architecture_training/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1 download the hugging face dataset \n",
    "# -----------------------------\n",
    "# Simple Hugging Face dataset downloader (download all files)\n",
    "# -----------------------------\n",
    "\n",
    "# Config: your dataset repo IDs\n",
    "HUGGINGFACE_TEST_DATASET_REPO  = \"sanjuhs/audio_to_blendshapes_test\"\n",
    "HUGGINGFACE_TRAIN_DATASET_REPO = \"sanjuhs/audio_to_blendshapes_main\"\n",
    "\n",
    "# (Optional) If your repos are private, set HF_TOKEN in env first:\n",
    "# import os; os.environ[\"HF_TOKEN\"] = \"hf_....\"\n",
    "\n",
    "print(\"Installing dependencies (if needed)...\")\n",
    "%pip install --quiet huggingface_hub tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Where to put everything (relative to current working directory)\n",
    "WORK_DIR = Path(\"data\")  # change to \"hf_datasets\" if you prefer\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def download_entire_dataset(repo_id: str, subdir_name: str, skip_existing: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Download *all* files from a HF dataset repo into data/<subdir_name>.\n",
    "    Returns a summary dict.\n",
    "    \"\"\"\n",
    "    target_dir = WORK_DIR / subdir_name\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nüì¶ Listing files for: {repo_id}\")\n",
    "    try:\n",
    "        files: List[str] = list_repo_files(repo_id, repo_type=\"dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to list files: {e}\")\n",
    "        return {\"repo_id\": repo_id, \"ok\": False, \"error\": str(e)}\n",
    "\n",
    "    print(f\"üóÇÔ∏è  Found {len(files)} files\")\n",
    "    downloaded, failed = [], []\n",
    "\n",
    "    for filename in tqdm(files, desc=f\"Downloading -> {subdir_name}\"):\n",
    "        # If re-running, optionally skip files that exist in target_dir\n",
    "        local_path = target_dir / filename\n",
    "        if skip_existing and local_path.exists():\n",
    "            downloaded.append({\"filename\": filename, \"local_path\": str(local_path), \"skipped\": True})\n",
    "            continue\n",
    "\n",
    "        # Ensure parent folders exist (HF repos can contain nested paths)\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # hf_hub_download will place the file under local_dir, preserving structure\n",
    "            dl_path = hf_hub_download(\n",
    "                repo_id=repo_id,\n",
    "                filename=filename,\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=target_dir,\n",
    "                local_dir_use_symlinks=False,   # write real files\n",
    "                token=os.getenv(\"HF_TOKEN\", None)  # supports private repos if token is set\n",
    "            )\n",
    "            downloaded.append({\n",
    "                \"filename\": filename,\n",
    "                \"local_path\": str(dl_path),\n",
    "                \"skipped\": False\n",
    "            })\n",
    "        except Exception as e:\n",
    "            failed.append({\"filename\": filename, \"error\": str(e)})\n",
    "\n",
    "    summary = {\n",
    "        \"repo_id\": repo_id,\n",
    "        \"dataset_dir\": str(target_dir.resolve()),\n",
    "        \"total_files\": len(files),\n",
    "        \"success_count\": len([d for d in downloaded if not d.get(\"skipped\")]),\n",
    "        \"skipped_count\": len([d for d in downloaded if d.get(\"skipped\")]),\n",
    "        \"failure_count\": len(failed),\n",
    "        \"downloaded\": downloaded,\n",
    "        \"failed\": failed,\n",
    "    }\n",
    "\n",
    "    # Save a summary JSON next to the files\n",
    "    with open(target_dir / \"download_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\n",
    "        f\"‚úÖ Done: {subdir_name} | \"\n",
    "        f\"success: {summary['success_count']}, skipped: {summary['skipped_count']}, failed: {summary['failure_count']}\"\n",
    "    )\n",
    "    print(f\"üìÅ Saved to: {summary['dataset_dir']}\")\n",
    "    return summary\n",
    "\n",
    "# ---- Run downloads ----\n",
    "test_summary  = download_entire_dataset(HUGGINGFACE_TEST_DATASET_REPO,  \"test\")\n",
    "train_summary = download_entire_dataset(HUGGINGFACE_TRAIN_DATASET_REPO, \"train\")\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"Test:  success={test_summary.get('success_count', 0)}, skipped={test_summary.get('skipped_count', 0)}, failed={test_summary.get('failure_count', 0)}\")\n",
    "print(f\"Train: success={train_summary.get('success_count', 0)}, skipped={train_summary.get('skipped_count', 0)}, failed={train_summary.get('failure_count', 0)}\")\n",
    "print(f\"\\nAll files are under: {WORK_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d6ed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Visualizer loaded successfully!\n",
      "üìä Available methods:\n",
      "  - add_epoch_data(epoch_data): Add metrics for an epoch\n",
      "  - add_dead_neuron_data(epoch, counts): Add dead neuron counts\n",
      "  - save_history(): Save history to JSON\n",
      "  - load_history(): Load history from JSON\n",
      "  - plot_training_overview(): Complete training overview plot\n",
      "  - plot_loss_breakdown(): Detailed loss analysis\n",
      "  - plot_feature_analysis(): Feature-specific performance\n",
      "  - plot_dead_neurons(): Plot dead neuron counts\n",
      "  - create_training_summary_report(): Text summary report\n",
      "  - plot_real_time_training(epoch, train_metrics, val_metrics, dead_neuron_counts): Real-time plot\n",
      "  - analyze_dead_neurons(model, loader, threshold): Analyze dead neurons\n"
     ]
    }
   ],
   "source": [
    "# step 1.5 load visualization\n",
    "# Visualization and Plotting Functions for Training Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive training visualization and analysis tool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, save_dir=\"plots\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.history = []\n",
    "        self.dead_neuron_history = [] # To store dead neuron counts\n",
    "\n",
    "    def add_epoch_data(self, epoch_data):\n",
    "        \"\"\"Add epoch data to history\"\"\"\n",
    "        self.history.append(epoch_data)\n",
    "\n",
    "    def add_dead_neuron_data(self, epoch, dead_neuron_counts):\n",
    "        \"\"\"Add dead neuron counts for an epoch\"\"\"\n",
    "        self.dead_neuron_history.append({'epoch': epoch, 'counts': dead_neuron_counts})\n",
    "\n",
    "    def save_history(self, filename=\"training_history.json\"):\n",
    "        \"\"\"Save training history to file\"\"\"\n",
    "        # Need to handle non-serializable types if present in epoch_data\n",
    "        # For now, assuming basic types are stored. If dead_neuron_counts contains tensors,\n",
    "        # they should be converted to lists/numpy arrays before calling this.\n",
    "        serializable_history = []\n",
    "        for h in self.history:\n",
    "            serializable_history.append({\n",
    "                'epoch': h['epoch'],\n",
    "                'train': {k: (v if isinstance(v, (int, float, str, bool)) else str(v)) for k, v in h['train'].items()},\n",
    "                'val': {k: (v if isinstance(v, (int, float, str, bool)) else str(v)) for k, v in h['val'].items()}\n",
    "            })\n",
    "\n",
    "        serializable_dead_neurons = []\n",
    "        for d in self.dead_neuron_history:\n",
    "             serializable_dead_neurons.append({\n",
    "                 'epoch': d['epoch'],\n",
    "                 'counts': {name: count for name, count in d['counts'].items()} # Assuming counts are simple numbers\n",
    "             })\n",
    "\n",
    "\n",
    "        full_history = {\n",
    "            'training_metrics': serializable_history,\n",
    "            'dead_neuron_analysis': serializable_dead_neurons\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(self.save_dir / filename, 'w') as f:\n",
    "            json.dump(full_history, f, indent=2)\n",
    "\n",
    "    def load_history(self, filename=\"training_history.json\"):\n",
    "        \"\"\"Load training history from file\"\"\"\n",
    "        try:\n",
    "            with open(self.save_dir / filename, 'r') as f:\n",
    "                full_history = json.load(f)\n",
    "                self.history = full_history.get('training_metrics', [])\n",
    "                self.dead_neuron_history = full_history.get('dead_neuron_analysis', [])\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"History file {filename} not found\")\n",
    "\n",
    "\n",
    "    def plot_training_overview(self, save=True):\n",
    "        \"\"\"Create comprehensive training overview plot\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "\n",
    "        # Convert history to DataFrame for easier plotting\n",
    "        df_train = pd.DataFrame([h['train'] for h in self.history])\n",
    "        df_val = pd.DataFrame([h['val'] for h in self.history])\n",
    "        epochs = [h['epoch'] for h in self.history]\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Training Progress Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Loss curves\n",
    "        axes[0,0].plot(epochs, df_train['total_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0,0].plot(epochs, df_val['total_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        axes[0,0].set_title('Total Loss')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Loss components\n",
    "        axes[0,1].plot(epochs, df_train['base_loss'], label='Base Loss')\n",
    "        axes[0,1].plot(epochs, df_train['temporal_loss'], label='Temporal Loss')\n",
    "        axes[0,1].plot(epochs, df_train['pose_loss'], label='Pose Loss')\n",
    "        axes[0,1].set_title('Training Loss Components')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Loss')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "        # MAE metrics\n",
    "        axes[0,2].plot(epochs, df_val['overall_mae'], 'g-', label='Overall MAE', linewidth=2)\n",
    "        axes[0,2].plot(epochs, df_val['mouth_mae'], 'orange', label='Mouth MAE', linewidth=2)\n",
    "        axes[0,2].plot(epochs, df_val['pose_mae'], 'purple', label='Pose MAE', linewidth=2)\n",
    "        axes[0,2].set_title('Mean Absolute Error')\n",
    "        axes[0,2].set_xlabel('Epoch')\n",
    "        axes[0,2].set_ylabel('MAE')\n",
    "        axes[0,2].legend()\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "        # Specific feature MAE\n",
    "        axes[1,0].plot(epochs, df_val['jaw_open_mae'], label='Jaw Open MAE')\n",
    "        axes[1,0].plot(epochs, df_val['lip_close_mae'], label='Lip Close MAE')\n",
    "        axes[1,0].plot(epochs, df_val['smile_mae'], label='Smile MAE')\n",
    "        axes[1,0].set_title('Specific Feature MAE')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('MAE')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Correlation metrics\n",
    "        axes[1,1].plot(epochs, df_val['jaw_corr'], 'b-', label='Jaw Correlation', linewidth=2)\n",
    "        axes[1,1].plot(epochs, df_val['lip_corr'], 'r-', label='Lip Correlation', linewidth=2)\n",
    "        axes[1,1].plot(epochs, df_val['smile_corr'], 'g-', label='Smile Correlation', linewidth=2)\n",
    "        axes[1,1].set_title('Feature Correlations')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Correlation')\n",
    "        axes[1,1].set_ylim(-1, 1)\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        axes[1,1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Training vs Validation Loss Comparison\n",
    "        axes[1,2].plot(epochs, df_train['total_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[1,2].plot(epochs, df_val['total_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[1,2].fill_between(epochs, df_train['total_loss'], df_val['total_loss'],\n",
    "                              alpha=0.3, color='gray', label='Overfitting Gap')\n",
    "        axes[1,2].set_title('Training vs Validation Loss')\n",
    "        axes[1,2].set_xlabel('Epoch')\n",
    "        axes[1,2].set_ylabel('Loss')\n",
    "        axes[1,2].legend()\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(self.save_dir / f'training_overview_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png',\n",
    "                       dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_breakdown(self, save=True):\n",
    "        \"\"\"Plot detailed loss component breakdown\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "\n",
    "        df_train = pd.DataFrame([h['train'] for h in self.history])\n",
    "        epochs = [h['epoch'] for h in self.history]\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Training Loss Component Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Stacked area plot of loss components\n",
    "        axes[0,0].fill_between(epochs, 0, df_train['base_loss'], alpha=0.7, label='Base Loss')\n",
    "        axes[0,0].fill_between(epochs, df_train['base_loss'],\n",
    "                              df_train['base_loss'] + df_train['temporal_loss'],\n",
    "                              alpha=0.7, label='Temporal Loss')\n",
    "        axes[0,0].fill_between(epochs, df_train['base_loss'] + df_train['temporal_loss'],\n",
    "                              df_train['base_loss'] + df_train['temporal_loss'] + df_train['pose_loss'],\n",
    "                              alpha=0.7, label='Pose Loss')\n",
    "        axes[0,0].set_title('Loss Components (Stacked)')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Individual loss components\n",
    "        axes[0,1].plot(epochs, df_train['base_loss'], 'b-', label='Base Loss', linewidth=2)\n",
    "        axes[0,1].plot(epochs, df_train['temporal_loss'], 'r-', label='Temporal Loss', linewidth=2)\n",
    "        axes[0,1].plot(epochs, df_train['pose_loss'], 'g-', label='Pose Loss', linewidth=2)\n",
    "        axes[0,1].set_title('Individual Loss Components')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Loss')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Loss ratios\n",
    "        total_loss = df_train['base_loss'] + df_train['temporal_loss'] + df_train['pose_loss']\n",
    "        base_ratio = df_train['base_loss'] / total_loss\n",
    "        temp_ratio = df_train['temporal_loss'] / total_loss\n",
    "        pose_ratio = df_train['pose_loss'] / total_loss\n",
    "\n",
    "        axes[1,0].plot(epochs, base_ratio, 'b-', label='Base Loss %', linewidth=2)\n",
    "        axes[1,0].plot(epochs, temp_ratio, 'r-', label='Temporal Loss %', linewidth=2)\n",
    "        axes[1,0].plot(epochs, pose_ratio, 'g-', label='Pose Loss %', linewidth=2)\n",
    "        axes[1,0].set_title('Loss Component Ratios')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('Ratio')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Loss improvement rate\n",
    "        loss_improvement = []\n",
    "        for i in range(1, len(df_train['total_loss'])):\n",
    "            improvement = (df_train['total_loss'].iloc[i-1] - df_train['total_loss'].iloc[i]) / df_train['total_loss'].iloc[i-1] * 100\n",
    "            loss_improvement.append(improvement)\n",
    "\n",
    "        axes[1,1].plot(epochs[1:], loss_improvement, 'purple', linewidth=2)\n",
    "        axes[1,1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        axes[1,1].set_title('Loss Improvement Rate (%)')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Improvement %')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(self.save_dir / f'loss_breakdown_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png',\n",
    "                       dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_feature_analysis(self, save=True):\n",
    "        \"\"\"Plot detailed feature-specific analysis\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "\n",
    "        df_val = pd.DataFrame([h['val'] for h in self.history])\n",
    "        epochs = [h['epoch'] for h in self.history]\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Feature-Specific Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # MAE comparison\n",
    "        axes[0,0].plot(epochs, df_val['jaw_open_mae'], 'b-', label='Jaw Open', linewidth=2, marker='o')\n",
    "        axes[0,0].plot(epochs, df_val['lip_close_mae'], 'r-', label='Lip Close', linewidth=2, marker='s')\n",
    "        axes[0,0].plot(epochs, df_val['smile_mae'], 'g-', label='Smile', linewidth=2, marker='^')\n",
    "        axes[0,0].set_title('Feature-Specific MAE')\n",
    "        axes[0,0].set_xlabel('Epoch')\n",
    "        axes[0,0].set_ylabel('MAE')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Correlation trends\n",
    "        axes[0,1].plot(epochs, df_val['jaw_corr'], 'b-', label='Jaw Correlation', linewidth=2, marker='o')\n",
    "        axes[0,1].plot(epochs, df_val['lip_corr'], 'r-', label='Lip Correlation', linewidth=2, marker='s')\n",
    "        axes[0,1].plot(epochs, df_val['smile_corr'], 'g-', label='Smile Correlation', linewidth=2, marker='^')\n",
    "        axes[0,1].set_title('Feature Correlations Over Time')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Correlation')\n",
    "        axes[0,1].set_ylim(-1, 1)\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        axes[0,1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Performance improvement heatmap\n",
    "        features = ['jaw_open_mae', 'lip_close_mae', 'smile_mae', 'jaw_corr', 'lip_corr', 'smile_corr']\n",
    "        improvement_matrix = []\n",
    "\n",
    "        for feature in features:\n",
    "            feature_data = df_val[feature]\n",
    "            if 'corr' in feature:\n",
    "                # For correlations, improvement means increase\n",
    "                improvement = [(feature_data.iloc[i] - feature_data.iloc[0]) for i in range(len(feature_data))]\n",
    "            else:\n",
    "                # For MAE, improvement means decrease\n",
    "                improvement = [(feature_data.iloc[0] - feature_data.iloc[i]) for i in range(len(feature_data))]\n",
    "            improvement_matrix.append(improvement)\n",
    "\n",
    "        im = axes[1,0].imshow(improvement_matrix, cmap='RdYlGn', aspect='auto')\n",
    "        axes[1,0].set_title('Feature Improvement Heatmap')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('Features')\n",
    "        axes[1,0].set_yticks(range(len(features)))\n",
    "        axes[1,0].set_yticklabels(features)\n",
    "        plt.colorbar(im, ax=axes[1,0])\n",
    "\n",
    "        # Overall vs specific performance\n",
    "        axes[1,1].plot(epochs, df_val['overall_mae'], 'k-', label='Overall MAE', linewidth=3)\n",
    "        axes[1,1].plot(epochs, df_val['mouth_mae'], 'orange', label='Mouth MAE', linewidth=2)\n",
    "        axes[1,1].plot(epochs, df_val['pose_mae'], 'purple', label='Pose MAE', linewidth=2)\n",
    "        axes[1,1].set_title('Overall vs Region-Specific Performance')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('MAE')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(self.save_dir / f'feature_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png',\n",
    "                       dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_dead_neurons(self, save=True):\n",
    "        \"\"\"Plot the number of dead neurons over epochs\"\"\"\n",
    "        if not self.dead_neuron_history:\n",
    "            print(\"No dead neuron history available\")\n",
    "            return\n",
    "\n",
    "        epochs = [d['epoch'] for d in self.dead_neuron_history]\n",
    "        # Assuming counts is a dict like {'layer_name': count}\n",
    "        layer_names = self.dead_neuron_history[0]['counts'].keys()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        fig.suptitle('Dead Neuron Count Over Epochs', fontsize=16, fontweight='bold')\n",
    "\n",
    "        for name in layer_names:\n",
    "            counts = [d['counts'].get(name, 0) for d in self.dead_neuron_history]\n",
    "            ax.plot(epochs, counts, label=name, marker='o')\n",
    "\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Number of Dead Neurons')\n",
    "        ax.set_title('Dead Neurons per Layer')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(self.save_dir / f'dead_neurons_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.png',\n",
    "                       dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def create_training_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive training summary report\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "\n",
    "        df_train = pd.DataFrame([h['train'] for h in self.history])\n",
    "        df_val = pd.DataFrame([h['val'] for h in self.history])\n",
    "        df_dead = pd.DataFrame(self.dead_neuron_history)\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(\"üèãÔ∏è  TRAINING SUMMARY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìä Total Epochs: {len(self.history)}\")\n",
    "        print(f\"üéØ Best Validation Loss: {df_val['total_loss'].min():.6f} (Epoch {df_val['total_loss'].idxmin() + 1})\")\n",
    "        print(f\"üìâ Final Training Loss: {df_train['total_loss'].iloc[-1]:.6f}\")\n",
    "        print(f\"üìà Final Validation Loss: {df_val['total_loss'].iloc[-1]:.6f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üé≠ FEATURE PERFORMANCE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        final_metrics = df_val.iloc[-1]\n",
    "        best_metrics = df_val.loc[df_val['total_loss'].idxmin()]\n",
    "\n",
    "        print(f\"Overall MAE:     Final: {final_metrics['overall_mae']:.4f} | Best: {best_metrics['overall_mae']:.4f}\")\n",
    "        print(f\"Mouth MAE:       Final: {final_metrics['mouth_mae']:.4f} | Best: {best_metrics['mouth_mae']:.4f}\")\n",
    "        print(f\"Jaw Open MAE:    Final: {final_metrics['jaw_open_mae']:.4f} | Best: {best_metrics['jaw_open_mae']:.4f}\")\n",
    "        print(f\"Lip Close MAE:   Final: {final_metrics['lip_close_mae']:.4f} | Best: {best_metrics['lip_close_mae']:.4f}\")\n",
    "        print(f\"Smile MAE:       Final: {final_metrics['smile_mae']:.4f} | Best: {best_metrics['smile_mae']:.4f}\")\n",
    "        print(f\"Pose MAE:        Final: {final_metrics['pose_mae']:.4f} | Best: {best_metrics['pose_mae']:.4f}\")\n",
    "\n",
    "        print(f\"\\nJaw Correlation:   Final: {final_metrics['jaw_corr']:.3f} | Best: {best_metrics['jaw_corr']:.3f}\")\n",
    "        print(f\"Lip Correlation:   Final: {final_metrics['lip_corr']:.3f} | Best: {best_metrics['lip_corr']:.3f}\")\n",
    "        print(f\"Smile Correlation: Final: {final_metrics['smile_corr']:.3f} | Best: {best_metrics['smile_corr']:.3f}\")\n",
    "\n",
    "        # Training stability analysis\n",
    "        loss_std = df_val['total_loss'].std()\n",
    "        loss_trend = np.polyfit(range(len(df_val)), df_val['total_loss'], 1)[0]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìà TRAINING STABILITY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Loss Standard Deviation: {loss_std:.6f}\")\n",
    "        print(f\"Loss Trend (slope): {loss_trend:.8f} {'üìâ Improving' if loss_trend < 0 else 'üìà Degrading'}\")\n",
    "\n",
    "        # Overfitting analysis\n",
    "        train_val_gap = df_train['total_loss'].iloc[-1] - df_val['total_loss'].iloc[-1]\n",
    "        print(f\"Train-Val Gap: {train_val_gap:.6f} {'‚ö†Ô∏è  Possible Overfitting' if train_val_gap < -0.01 else '‚úÖ Good Generalization'}\")\n",
    "\n",
    "        # Dead neuron analysis summary\n",
    "        if not df_dead.empty:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"üíÄ DEAD NEURON ANALYSIS\")\n",
    "            print(\"=\"*50)\n",
    "            final_dead_counts = df_dead.iloc[-1]['counts']\n",
    "            print(\"Final Dead Neuron Counts per Layer:\")\n",
    "            for name, count in final_dead_counts.items():\n",
    "                 print(f\"  {name}: {count}\")\n",
    "\n",
    "\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    def plot_real_time_training(self, epoch, train_metrics, val_metrics, dead_neuron_counts=None):\n",
    "        \"\"\"Plot real-time training progress (call during training)\"\"\"\n",
    "        plt.clf() # Clear previous figure\n",
    "\n",
    "        if len(self.history) < 1: # Need at least one epoch to plot\n",
    "            return\n",
    "\n",
    "        epochs = [h['epoch'] for h in self.history]\n",
    "        train_losses = [h['train']['total_loss'] for h in self.history]\n",
    "        val_losses = [h['val']['total_loss'] for h in self.history]\n",
    "        val_maes = [h['val']['overall_mae'] for h in self.history]\n",
    "        jaw_corrs = [h['val']['jaw_corr'] for h in self.history]\n",
    "        lip_corrs = [h['val']['lip_corr'] for h in self.history]\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # More subplots for more data\n",
    "\n",
    "        # Loss Progress\n",
    "        axes[0].plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "        axes[0].plot(epochs, val_losses, 'r-', label='Val Loss')\n",
    "        axes[0].set_title(f'Loss Progress (Epoch {epoch})')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # MAE and Correlation Progress\n",
    "        ax2_twin = axes[1].twinx() # Use twin axis for MAE and Correlation\n",
    "\n",
    "        axes[1].plot(epochs, val_maes, 'g-', label='Val MAE')\n",
    "        ax2_twin.plot(epochs, jaw_corrs, 'm--', label='Jaw Corr')\n",
    "        ax2_twin.plot(epochs, lip_corrs, 'c--', label='Lip Corr')\n",
    "\n",
    "        axes[1].set_title(f'Metrics Progress (Epoch {epoch})')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        ax2_twin.set_ylabel('Correlation')\n",
    "        axes[1].legend(loc='upper left')\n",
    "        ax2_twin.legend(loc='upper right')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        ax2_twin.grid(False) # Avoid overlapping grids\n",
    "\n",
    "        # Dead Neuron Count (if available)\n",
    "        if dead_neuron_counts is not None and self.dead_neuron_history:\n",
    "            df_dead_current = pd.DataFrame(self.dead_neuron_history)\n",
    "            epochs_dead = df_dead_current['epoch']\n",
    "            layer_names = df_dead_current['counts'].iloc[0].keys()\n",
    "\n",
    "            for name in layer_names:\n",
    "                 counts = [d['counts'].get(name, 0) for d in self.dead_neuron_history]\n",
    "                 axes[2].plot(epochs_dead, counts, label=name)\n",
    "\n",
    "            axes[2].set_title(f'Dead Neurons (Epoch {epoch})')\n",
    "            axes[2].set_xlabel('Epoch')\n",
    "            axes[2].set_ylabel('Count')\n",
    "            axes[2].legend()\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[2].set_title('Dead Neuron Analysis (Not Available)')\n",
    "            axes[2].text(0.5, 0.5, 'Run dead neuron analysis',\n",
    "                         horizontalalignment='center', verticalalignment='center',\n",
    "                         transform=axes[2].transAxes, color='gray')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.draw() # Draw the plot\n",
    "        plt.pause(0.01) # Pause to allow drawing\n",
    "\n",
    "    def analyze_dead_neurons(self, model, loader, threshold=1e-6):\n",
    "        \"\"\"\n",
    "        Analyzes dead neurons in GELU activations based on activation variance.\n",
    "        A neuron is considered 'dead' if its activation variance is below a threshold\n",
    "        across a batch of data.\n",
    "        \"\"\"\n",
    "        print(\"\\nüíÄ Analyzing dead neurons...\")\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device # Get model device\n",
    "\n",
    "        # Dictionary to store activation sums and counts for each GELU layer\n",
    "        activations = {}\n",
    "\n",
    "        # Register hooks to capture activations after GELU layers\n",
    "        hooks = []\n",
    "        def get_activation_hook(name):\n",
    "            def hook_fn(module, input, output):\n",
    "                # Store sum of activations squared and count for variance calculation\n",
    "                if name not in activations:\n",
    "                    activations[name] = {'sum_sq': 0, 'count': 0}\n",
    "                # output shape is (batch, channels, time) for Conv1D\n",
    "                # Sum over batch and time dimensions\n",
    "                activations[name]['sum_sq'] += torch.sum(output**2, dim=(0, 2)).cpu().numpy()\n",
    "                activations[name]['count'] += output.size(0) * output.size(2) # Total elements per channel\n",
    "\n",
    "            return hook_fn\n",
    "\n",
    "        # Find GELU layers and register hooks\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.GELU, nn.SiLU)):\n",
    "                 # Check if it's part of a TCNBlock or output layers\n",
    "                 if 'tcn_layers' in name or 'output_layers' in name:\n",
    "                    hooks.append(module.register_forward_hook(get_activation_hook(name)))\n",
    "                    print(f\"  Registered hook for: {name}\")\n",
    "\n",
    "\n",
    "        # Process data to capture activations\n",
    "        num_batches_to_analyze = min(len(loader), 10) # Analyze a few batches\n",
    "        with torch.no_grad():\n",
    "            for i, (audio, _, _) in enumerate(loader):\n",
    "                if i >= num_batches_to_analyze:\n",
    "                    break\n",
    "                audio = audio.to(device)\n",
    "                # Need to match the input shape expected by AudioToBlendshapesTCN forward\n",
    "                # It expects (batch, time, features) or (batch, features, time)\n",
    "                # The loader gives (batch, time, features), which AudioToBlendshapesTCN handles.\n",
    "                _ = model(audio)\n",
    "\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # Calculate variance and count dead neurons\n",
    "        dead_neuron_counts = {}\n",
    "        print(\"\\nCalculating dead neurons...\")\n",
    "        for name, data in activations.items():\n",
    "            # Variance = Sum(x^2) / N - (Sum(x)/N)^2\n",
    "            # We only have Sum(x^2) and N here, so we approximate variance by mean of squares.\n",
    "            # A more robust method would require storing sums of activations as well.\n",
    "            # For a simple check, mean of squares can indicate activity.\n",
    "            # A value close to 0 means the neuron's output was mostly zero.\n",
    "            mean_sq = data['sum_sq'] / data['count']\n",
    "            # Count neurons where mean of squares is below threshold\n",
    "            dead_count = np.sum(mean_sq < threshold)\n",
    "            total_neurons = len(mean_sq)\n",
    "            dead_neuron_counts[name] = int(dead_count)\n",
    "            print(f\"  {name}: {dead_count}/{total_neurons} dead ({dead_count/total_neurons:.2%})\")\n",
    "\n",
    "        return dead_neuron_counts\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training Visualizer loaded successfully!\")\n",
    "print(\"üìä Available methods:\")\n",
    "print(\"  - add_epoch_data(epoch_data): Add metrics for an epoch\")\n",
    "print(\"  - add_dead_neuron_data(epoch, counts): Add dead neuron counts\")\n",
    "print(\"  - save_history(): Save history to JSON\")\n",
    "print(\"  - load_history(): Load history from JSON\")\n",
    "print(\"  - plot_training_overview(): Complete training overview plot\")\n",
    "print(\"  - plot_loss_breakdown(): Detailed loss analysis\")\n",
    "print(\"  - plot_feature_analysis(): Feature-specific performance\")\n",
    "print(\"  - plot_dead_neurons(): Plot dead neuron counts\")\n",
    "print(\"  - create_training_summary_report(): Text summary report\")\n",
    "print(\"  - plot_real_time_training(epoch, train_metrics, val_metrics, dead_neuron_counts): Real-time plot\")\n",
    "print(\"  - analyze_dead_neurons(model, loader, threshold): Analyze dead neurons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb43cc4",
   "metadata": {},
   "source": [
    "### Step 2 training the model with 10s model\n",
    "\n",
    "\n",
    "for the visualizations : Please Check the 'plots' folder for saved visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8ca3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN Model Architecture (10+ Second Memory):\n",
      "  Input dim: 80\n",
      "  Output dim: 59\n",
      "  Hidden channels: 256\n",
      "  Layers: 3\n",
      "  Dilations: [1, 2, 4]\n",
      "  Receptive field: 15 frames (150ms = 0.1s)\n",
      "  üéØ Can see 0.1 seconds into the past!\n",
      "  Parameters: 263,227 (1.0 MB)\n",
      "  Estimated GPU memory for batch_size=8: ~8 MB\n",
      "Dilations: [1, 2, 4]\n",
      "Receptive field (frames): 15\n"
     ]
    }
   ],
   "source": [
    "# Utility: report model dilations and receptive field (self-contained)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from __main__ import AudioToBlendshapesTCN\n",
    "\n",
    "DATA_DIR = Path(\"data/train\")\n",
    "audio = np.load(DATA_DIR / \"audio_sequences.npy\")\n",
    "seq_len = int(audio.shape[1])\n",
    "in_features = int(audio.shape[-1])\n",
    "\n",
    "# Build dilations capped by available sequence length\n",
    "kernel_size = 3\n",
    "proposed = []\n",
    "rf = 1\n",
    "for i in range(20):\n",
    "    d = 2**i\n",
    "    next_rf = rf + (kernel_size - 1) * d\n",
    "    if next_rf > seq_len:\n",
    "        break\n",
    "    proposed.append(d)\n",
    "    rf = next_rf\n",
    "if len(proposed) == 0:\n",
    "    proposed = [1]\n",
    "\n",
    "model_inspect = AudioToBlendshapesTCN(\n",
    "    input_dim=in_features,\n",
    "    output_dim=59,\n",
    "    hidden_channels=256,\n",
    "    num_layers=len(proposed),\n",
    "    kernel_size=3,\n",
    "    dropout=0.1,\n",
    "    max_dilation=128,\n",
    "    custom_dilations=proposed,\n",
    ")\n",
    "info = model_inspect.get_model_info()\n",
    "print(\"Dilations:\", info.get('dilations'))\n",
    "print(\"Receptive field (frames):\", info.get('receptive_field_frames'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd79840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Visualization enabled - plots will be generated during training\n",
      "Dataset: data/train\n",
      "  audio   : (40743, 23, 80)  (B, T, F_audio)\n",
      "  targets : (40743, 23, 59) (B, T, 59)\n",
      "  vad     : (40743, 23)     (B, T)\n",
      "  audio z-norm -> mean‚âà-64.5040, std‚âà10.5197\n",
      "  WARNING: pose large values (max abs 1.000)\n",
      "Using AudioToBlendshapesTCN configuration. custom_dilations=None\n",
      "TCN Model Architecture (10+ Second Memory):\n",
      "  Input dim: 80\n",
      "  Output dim: 59\n",
      "  Hidden channels: 256\n",
      "  Layers: 10\n",
      "  Dilations: [1, 2, 4, 8, 16, 32, 64, 128, 128, 128]\n",
      "  Receptive field: 1023 frames (10230ms = 10.2s)\n",
      "  üéØ Can see 10.2 seconds into the past!\n",
      "  Parameters: 734,523 (2.8 MB)\n",
      "  Estimated GPU memory for batch_size=8: ~22 MB\n",
      "üìä Visualization enabled - plots will be generated during training\n",
      "\n",
      "=== Training config ===\n",
      "Device         : cpu\n",
      "Samples        : total=905, train=724, val=181\n",
      "Batch size     : 8\n",
      "Total steps    : 4550\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 91/91 [02:07<00:00,  1.40s/it]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:10<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.0409 (base=0.0313, temp=0.1459, sil=0.0000, pose=0.1166)\n",
      " Val : loss=0.0200 (MAE=0.0859, jaw_r=-0.099, lip_r=0.262, mouth_MAE=0.0868)\n",
      "\n",
      "üíÄ Analyzing dead neurons...\n",
      "  Registered hook for: tcn_layers.0.activation\n",
      "  Registered hook for: tcn_layers.1.activation\n",
      "  Registered hook for: tcn_layers.2.activation\n",
      "  Registered hook for: tcn_layers.3.activation\n",
      "  Registered hook for: tcn_layers.4.activation\n",
      "  Registered hook for: tcn_layers.5.activation\n",
      "  Registered hook for: tcn_layers.6.activation\n",
      "  Registered hook for: tcn_layers.7.activation\n",
      "  Registered hook for: tcn_layers.8.activation\n",
      "  Registered hook for: tcn_layers.9.activation\n",
      "  Registered hook for: output_layers.1\n",
      "\n",
      "Calculating dead neurons...\n",
      "  tcn_layers.0.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.1.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.2.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.3.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.4.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.5.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.6.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.7.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.8.activation: 0/256 dead (0.00%)\n",
      "  tcn_layers.9.activation: 0/256 dead (0.00%)\n",
      "  output_layers.1: 0/128 dead (0.00%)\n",
      "üíæ Model saved to: models/best_tcn_model.pth\n",
      "‚úÖ New best model\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 91/91 [02:03<00:00,  1.36s/it]\n",
      "Validation:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:10<00:01,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 579\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    581\u001b[39m     \u001b[38;5;66;03m# If running in Jupyter, call main automatically\u001b[39;00m\n\u001b[32m    582\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 575\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    572\u001b[39m trainer.criterion.enable_pose_loss(\u001b[32m0.02\u001b[39m)\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Longer sequences ‚Üí reduce batch size for memory\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 424\u001b[39m, in \u001b[36mTCNTrainer.train\u001b[39m\u001b[34m(self, dataset, num_epochs, batch_size, validation_split)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    423\u001b[39m tr = \u001b[38;5;28mself\u001b[39m.train_epoch(train_loader)\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m va = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (base=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[33m'\u001b[39m\u001b[33mbase_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, temp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[33m'\u001b[39m\u001b[33mtemporal_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, sil=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[33m'\u001b[39m\u001b[33msilence_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, pose=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[33m'\u001b[39m\u001b[33mpose_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    427\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Val : loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[33m'\u001b[39m\u001b[33moverall_mae\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, jaw_r=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[33m'\u001b[39m\u001b[33mjaw_corr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lip_r=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[33m'\u001b[39m\u001b[33mlip_corr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, mouth_MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[33m'\u001b[39m\u001b[33mmouth_mae\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mTCNTrainer.validate\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m audio, targets, vad \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc=\u001b[33m\"\u001b[39m\u001b[33mValidation\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    333\u001b[39m     audio, targets, vad = audio.to(\u001b[38;5;28mself\u001b[39m.device), targets.to(\u001b[38;5;28mself\u001b[39m.device), vad.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     loss_dict = \u001b[38;5;28mself\u001b[39m.criterion(preds, targets, vad)\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m agg:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 257\u001b[39m, in \u001b[36mAudioToBlendshapesTCN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Pass through TCN layers\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tcn_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tcn_layers:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     x = \u001b[43mtcn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Output projection\u001b[39;00m\n\u001b[32m    260\u001b[39m x = \u001b[38;5;28mself\u001b[39m.output_layers(x)  \u001b[38;5;66;03m# (batch, output_dim, time)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mTCNBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     88\u001b[39m residual = x\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Apply convolution with causal padding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Remove future information (causal)\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mDepthwiseSeparableConv1d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdepthwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     46\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn2(\u001b[38;5;28mself\u001b[39m.pointwise(x))\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:371\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Coding/Python/NN_training/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:366\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    356\u001b[39m         F.pad(\n\u001b[32m    357\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    365\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# step 2.2 training the model with viz\n",
    "#!/usr/bin/env python3\n",
    "# hey here make sure to explictly change teh code so that we can for sure use the 10 second model in this case and train\n",
    "\"\"\"\n",
    "Train Audio-to-Blendshapes (TCN) on extracted features.\n",
    "\n",
    "Defaults to training on the TEST dataset features at:\n",
    "    data/test/extracted_features\n",
    "\n",
    "To train on the MAIN dataset instead, change:\n",
    "    DATA_DIR = Path(\"data/test/extracted_features\")\n",
    "to:\n",
    "    DATA_DIR = Path(\"data/train/extracted_features\")\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Import the correct model class (AudioToBlendshapesTCN)\n",
    "from __main__ import AudioToBlendshapesTCN # Import the 10-layer model from cell LPF3Pxcfn7rQ\n",
    "from __main__ import TrainingVisualizer # Import TrainingVisualizer from cell clJb4OJ8ycSI\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple TCN model (dilated temporal convs) - REMOVED/REPLACED\n",
    "# -----------------------------\n",
    "# The TCNModel class defined here was replaced by AudioToBlendshapesTCN imported above.\n",
    "# Keeping the original for reference if needed, but it's not used in the main function below.\n",
    "\n",
    "# class TemporalBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         padding = (kernel_size - 1) * dilation // 2  # keep length\n",
    "#         self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
    "#         self.norm1 = nn.BatchNorm1d(out_ch)\n",
    "#         self.act1 = nn.GELU()\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
    "#         self.norm2 = nn.BatchNorm1d(out_ch)\n",
    "#         self.act2 = nn.GELU()\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "#         self.res = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.norm1(y)\n",
    "#         y = self.act1(y)\n",
    "#         y = self.dropout1(y)\n",
    "\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.norm2(y)\n",
    "#         y = self.act2(y)\n",
    "#         y = self.dropout2(y)\n",
    "\n",
    "#         return y + self.res(x)\n",
    "\n",
    "\n",
    "# class TCNModel(nn.Module):\n",
    "#     def __init__(self, in_features: int, out_features: int = 59,\n",
    "#                  hidden: int = 128, levels: int = 4, kernel_size: int = 3, dropout: float = 0.1):\n",
    "#         super().__init__()\n",
    "#         chans = [in_features] + [hidden] * levels\n",
    "#         blocks = []\n",
    "#         for i in range(levels):\n",
    "#             dilation = 2 ** i\n",
    "#             blocks.append(\n",
    "#                 TemporalBlock(chans[i], chans[i + 1], kernel_size=kernel_size, dilation=dilation, dropout=dropout)\n",
    "#             )\n",
    "#         self.tcn = nn.Sequential(*blocks)\n",
    "#         self.head = nn.Conv1d(hidden, out_features, kernel_size=1)\n",
    "\n",
    "#         # store config for saving\n",
    "#         self._config = dict(in_features=in_features, out_features=out_features,\n",
    "#                             hidden=hidden, levels=levels, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (B, T, F_in)  ->  y: (B, T, 59)\n",
    "#         \"\"\"\n",
    "#         x = x.permute(0, 2, 1)         # (B, F_in, T)\n",
    "#         y = self.tcn(x)                # (B, hidden, T)\n",
    "#         y = self.head(y)               # (B, 59, T)\n",
    "#         y = y.permute(0, 2, 1)         # (B, T, 59)\n",
    "#         return y\n",
    "\n",
    "#     def get_model_info(self):\n",
    "#         return self._config\n",
    "\n",
    "# Function to initialize weights (added)\n",
    "\n",
    "\n",
    "# The create_model function below is the one being used in the main function.\n",
    "# It should now create the AudioToBlendshapesTCN model.\n",
    "def create_model(in_features: int, out_features: int = 59, seq_len: int | None = None) -> nn.Module:\n",
    "    \"\"\"Factory expected by your trainer. Adapts dilations to sequence length to avoid receptive-field overshoot.\"\"\"\n",
    "    # Choose dilations based on available temporal context if provided\n",
    "    custom_dilations = None\n",
    "    if seq_len is not None and seq_len > 0:\n",
    "        # Build exponential dilations but cap so that receptive field <= seq_len\n",
    "        proposed = []\n",
    "        running_rf = 1\n",
    "        kernel_size = 3\n",
    "        for i in range(20):  # up to 20 layers if short dilations\n",
    "            d = 2**i\n",
    "            # next receptive field if we add this layer\n",
    "            next_rf = running_rf + (kernel_size - 1) * d\n",
    "            if next_rf > seq_len:\n",
    "                break\n",
    "            proposed.append(d)\n",
    "            running_rf = next_rf\n",
    "        # If sequence very short, at least include dilation=1\n",
    "        if len(proposed) == 0:\n",
    "            proposed = [1]\n",
    "        custom_dilations = proposed\n",
    "        num_layers = len(custom_dilations)\n",
    "    else:\n",
    "        num_layers = 10\n",
    "\n",
    "    config = {\n",
    "        'input_dim': in_features,\n",
    "        'output_dim': out_features,\n",
    "        'hidden_channels': 256,\n",
    "        'num_layers': num_layers,\n",
    "        'kernel_size': 3,\n",
    "        'dropout': 0.1,\n",
    "        'max_dilation': 128,\n",
    "        'custom_dilations': custom_dilations\n",
    "    }\n",
    "    print(f\"Using AudioToBlendshapesTCN configuration. custom_dilations={custom_dilations}\")\n",
    "    model = AudioToBlendshapesTCN(**config)\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Loss (your original, unchanged except prints trimmed)\n",
    "# -----------------------------\n",
    "class AudioBlendshapeLoss(nn.Module):\n",
    "    def __init__(self, base_weight=1.0, temporal_weight=0.0, silence_weight=0.0, pose_clamp_weight=0.0):\n",
    "        super().__init__()\n",
    "        self.base_weight = base_weight\n",
    "        self.temporal_weight = temporal_weight\n",
    "        self.silence_weight = silence_weight\n",
    "        self.pose_clamp_weight = pose_clamp_weight\n",
    "        self.mouth_indices = [10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
    "        self.pose_indices = list(range(52, 59))\n",
    "\n",
    "    def enable_temporal_loss(self, weight=0.1):\n",
    "        self.temporal_weight = weight\n",
    "\n",
    "    def enable_silence_loss(self, weight=2.0):\n",
    "        self.silence_weight = weight\n",
    "\n",
    "    def enable_pose_loss(self, weight=0.05):\n",
    "        self.pose_clamp_weight = weight\n",
    "\n",
    "    def forward(self, predictions, targets, voice_activity=None):\n",
    "        base_loss = nn.functional.smooth_l1_loss(predictions, targets, reduction='mean')\n",
    "\n",
    "        # temporal\n",
    "        pred_diff1 = predictions[:, 1:] - predictions[:, :-1]\n",
    "        target_diff1 = targets[:, 1:] - targets[:, :-1]\n",
    "        temporal_loss1 = torch.mean(torch.abs(pred_diff1 - target_diff1))\n",
    "        pred_diff2 = pred_diff1[:, 1:] - pred_diff1[:, :-1]\n",
    "        target_diff2 = target_diff1[:, 1:] - target_diff1[:, :-1]\n",
    "        temporal_loss2 = torch.mean(torch.abs(pred_diff2 - target_diff2))\n",
    "        temporal_loss = temporal_loss1 + temporal_loss2\n",
    "\n",
    "        # silence\n",
    "        silence_loss = 0.0\n",
    "        if voice_activity is not None and self.silence_weight > 0:\n",
    "            silence_mask = (voice_activity.unsqueeze(-1) == 0)\n",
    "            if silence_mask.any():\n",
    "                mouth_pred = predictions[:, :, self.mouth_indices]\n",
    "                mouth_target = targets[:, :, self.mouth_indices]\n",
    "                mouth_error = torch.abs(mouth_pred - mouth_target)\n",
    "                silence_error = mouth_error[silence_mask.expand_as(mouth_error)]\n",
    "                silence_loss = silence_error.mean() if silence_error.numel() > 0 else 0.0\n",
    "\n",
    "        # pose clamp\n",
    "        pose_loss = 0.0\n",
    "        if self.pose_clamp_weight > 0:\n",
    "            pose_pred = predictions[:, :, self.pose_indices]\n",
    "            pose_target = targets[:, :, self.pose_indices]\n",
    "            trans_pred = pose_pred[:, :, :3]\n",
    "            trans_clamped = torch.clamp(trans_pred, -0.2, 0.2)\n",
    "            clamp_penalty = torch.mean((trans_pred - trans_clamped) ** 2)\n",
    "            pose_loss = nn.functional.mse_loss(pose_pred, pose_target, reduction='mean') + clamp_penalty\n",
    "\n",
    "        total_loss = (\n",
    "            self.base_weight * base_loss +\n",
    "            self.temporal_weight * temporal_loss +\n",
    "            self.silence_weight * (silence_loss if not isinstance(silence_loss, float) else torch.tensor(silence_loss, device=predictions.device)) +\n",
    "            self.pose_clamp_weight * (pose_loss if not isinstance(pose_loss, float) else torch.tensor(pose_loss, device=predictions.device))\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'base_loss': base_loss.item(),\n",
    "            'temporal_loss': temporal_loss.item(),\n",
    "            'silence_loss': silence_loss if isinstance(silence_loss, float) else silence_loss.item(),\n",
    "            'pose_loss': pose_loss if isinstance(pose_loss, float) else pose_loss.item()\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer (cleaned & made robust)\n",
    "# -----------------------------\n",
    "class TCNTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu', enable_visualization=True):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = AudioBlendshapeLoss()\n",
    "        self.jaw_open_idx = 25\n",
    "        self.lip_close_idx = 12\n",
    "        self.smile_idx = 20\n",
    "\n",
    "        # Initialize visualizer\n",
    "        self.enable_visualization = enable_visualization\n",
    "        if self.enable_visualization:\n",
    "            # Check if TrainingVisualizer is defined in the global scope\n",
    "            if 'TrainingVisualizer' in globals():\n",
    "                self.visualizer = TrainingVisualizer()\n",
    "                print(\"üìä Visualization enabled - plots will be generated during training\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  TrainingVisualizer not found. Visualization disabled.\")\n",
    "                self.enable_visualization = False\n",
    "\n",
    "\n",
    "    def setup_optimizer(self, learning_rate=1e-4, weight_decay=1e-4): # Lowered learning_rate\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.base_lr = learning_rate\n",
    "        # Increased max_lr to potentially break plateau\n",
    "        self.max_lr = 5e-3 # Increased from 3e-3\n",
    "\n",
    "    def setup_scheduler(self, total_steps):\n",
    "        if total_steps <= 0:\n",
    "            self.scheduler = None\n",
    "            return\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer, max_lr=self.max_lr, total_steps=total_steps, pct_start=0.3, anneal_strategy='cos'\n",
    "        )\n",
    "\n",
    "    def load_dataset(self, data_dir: Path, normalize=True):\n",
    "        audio = np.load(data_dir / \"audio_sequences.npy\")\n",
    "        targets = np.load(data_dir / \"target_sequences.npy\")\n",
    "        vad = np.load(data_dir / \"vad_sequences.npy\")\n",
    "        with open(data_dir / \"dataset_metadata.json\", \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        print(f\"Dataset: {data_dir}\")\n",
    "        print(f\"  audio   : {audio.shape}  (B, T, F_audio)\")\n",
    "        print(f\"  targets : {targets.shape} (B, T, 59)\")\n",
    "        print(f\"  vad     : {vad.shape}     (B, T)\")\n",
    "\n",
    "        if normalize:\n",
    "            m = audio.mean(axis=(0, 1), keepdims=True)\n",
    "            s = audio.std(axis=(0, 1), keepdims=True) + 1e-6\n",
    "            audio = (audio - m) / s\n",
    "            print(f\"  audio z-norm -> mean‚âà{float(m.mean()):.4f}, std‚âà{float(s.mean()):.4f}\")\n",
    "\n",
    "            blend = targets[:, :, :52]\n",
    "            pose = targets[:, :, 52:]\n",
    "            if blend.min() < -0.1 or blend.max() > 1.1:\n",
    "                print(\"  WARNING: blendshape range outside [0,1]\")\n",
    "            if np.abs(pose).max() > 0.5:\n",
    "                print(f\"  WARNING: pose large values (max abs {np.abs(pose).max():.3f})\")\n",
    "\n",
    "        return {\n",
    "            'audio': torch.from_numpy(audio).float(),\n",
    "            'targets': torch.from_numpy(targets).float(),\n",
    "            'vad': torch.from_numpy(vad).float(),\n",
    "            'metadata': metadata\n",
    "        }\n",
    "\n",
    "    def create_data_loader(self, dataset, batch_size=16, shuffle=True):\n",
    "        ds = torch.utils.data.TensorDataset(dataset['audio'], dataset['targets'], dataset['vad'])\n",
    "        # Ensure contiguous tensors to avoid dataloader pinning issues and improve hooks stability\n",
    "        ds.tensors = tuple(t.contiguous() for t in ds.tensors)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            ds, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=0, pin_memory=(self.device == 'cuda')\n",
    "        )\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        agg = dict(total_loss=0.0, base_loss=0.0, temporal_loss=0.0, silence_loss=0.0, pose_loss=0.0)\n",
    "        n = 0\n",
    "\n",
    "        for audio, targets, vad in tqdm(loader, desc=\"Training\"):\n",
    "            audio, targets, vad = audio.to(self.device), targets.to(self.device), vad.to(self.device)\n",
    "\n",
    "            preds = self.model(audio)\n",
    "            loss_dict = self.criterion(preds, targets, vad)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_dict['total_loss'].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) # Added gradient clipping\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            for k in agg:\n",
    "                agg[k] += float(loss_dict[k])\n",
    "            n += 1\n",
    "\n",
    "        for k in agg:\n",
    "            agg[k] /= max(n, 1)\n",
    "        return agg\n",
    "\n",
    "    def validate(self, loader):\n",
    "        self.model.eval()\n",
    "        agg = dict(total_loss=0.0, base_loss=0.0, temporal_loss=0.0, silence_loss=0.0, pose_loss=0.0)\n",
    "        n = 0\n",
    "        preds_all, targs_all = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for audio, targets, vad in tqdm(loader, desc=\"Validation\"):\n",
    "                audio, targets, vad = audio.to(self.device), targets.to(self.device), vad.to(self.device)\n",
    "                preds = self.model(audio)\n",
    "                loss_dict = self.criterion(preds, targets, vad)\n",
    "\n",
    "                for k in agg:\n",
    "                    agg[k] += float(loss_dict[k])\n",
    "                n += 1\n",
    "\n",
    "                preds_all.append(preds.cpu())\n",
    "                targs_all.append(targets.cpu())\n",
    "\n",
    "        for k in agg:\n",
    "            agg[k] /= max(n, 1)\n",
    "\n",
    "        preds_all = torch.cat(preds_all, dim=0)\n",
    "        targs_all = torch.cat(targs_all, dim=0)\n",
    "        agg.update(self.compute_detailed_metrics(preds_all, targs_all))\n",
    "        return agg\n",
    "\n",
    "    def compute_detailed_metrics(self, predictions, targets):\n",
    "        pred = predictions.view(-1, predictions.size(-1)).numpy()\n",
    "        targ = targets.view(-1, targets.size(-1)).numpy()\n",
    "\n",
    "        mae_ch = np.mean(np.abs(pred - targ), axis=0)\n",
    "        overall_mae = float(np.mean(mae_ch))\n",
    "        mouth_mae = float(np.mean(mae_ch[10:30]))\n",
    "        jaw_open_mae = float(mae_ch[self.jaw_open_idx])\n",
    "        lip_close_mae = float(mae_ch[self.lip_close_idx])\n",
    "        smile_mae = float(mae_ch[self.smile_idx])\n",
    "        pose_mae = float(np.mean(mae_ch[52:59]))\n",
    "\n",
    "        def safe_r(a, b):\n",
    "            r = pearsonr(a, b)[0]\n",
    "            return float(0.0 if np.isnan(r) else r)\n",
    "\n",
    "        jaw_corr = safe_r(pred[:, self.jaw_open_idx], targ[:, self.jaw_open_idx])\n",
    "        lip_corr = safe_r(pred[:, self.lip_close_idx], targ[:, self.lip_close_idx])\n",
    "        smile_corr = safe_r(pred[:, self.smile_idx], targ[:, self.smile_idx])\n",
    "\n",
    "        return dict(\n",
    "            overall_mae=overall_mae,\n",
    "            mouth_mae=mouth_mae,\n",
    "            jaw_open_mae=jaw_open_mae,\n",
    "            lip_close_mae=lip_close_mae,\n",
    "            smile_mae=smile_mae,\n",
    "            pose_mae=pose_mae,\n",
    "            jaw_corr=jaw_corr,\n",
    "            lip_corr=lip_corr,\n",
    "            smile_corr=smile_corr\n",
    "        )\n",
    "\n",
    "    def save_model(self, path, metrics=None):\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'model_config': self.model.get_model_info(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'validation_metrics': metrics\n",
    "        }, path)\n",
    "        print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "    def train(self, dataset, num_epochs=15, batch_size=16, validation_split=0.2):\n",
    "        total_samples = len(dataset['audio'])\n",
    "        val_size = int(total_samples * validation_split)\n",
    "        train_size = total_samples - val_size\n",
    "\n",
    "        idx = torch.randperm(total_samples)\n",
    "        train_idx, val_idx = idx[:train_size], idx[train_size:]\n",
    "\n",
    "        train_ds = {k: v[train_idx] for k, v in dataset.items() if k in ('audio', 'targets', 'vad')}\n",
    "        val_ds   = {k: v[val_idx]   for k, v in dataset.items() if k in ('audio', 'targets', 'vad')}\n",
    "\n",
    "        train_loader = self.create_data_loader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = self.create_data_loader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.setup_optimizer()\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        self.setup_scheduler(total_steps)\n",
    "\n",
    "        print(\"\\n=== Training config ===\")\n",
    "        print(f\"Device         : {self.device}\")\n",
    "        print(f\"Samples        : total={total_samples}, train={train_size}, val={val_size}\")\n",
    "        print(f\"Batch size     : {batch_size}\")\n",
    "        print(f\"Total steps    : {total_steps}\")\n",
    "\n",
    "        best_val = math.inf\n",
    "        history = []\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "            tr = self.train_epoch(train_loader)\n",
    "            va = self.validate(val_loader)\n",
    "\n",
    "            print(f\"Train: loss={tr['total_loss']:.4f} (base={tr['base_loss']:.4f}, temp={tr['temporal_loss']:.4f}, sil={tr['silence_loss']:.4f}, pose={tr['pose_loss']:.4f})\")\n",
    "            print(f\" Val : loss={va['total_loss']:.4f} (MAE={va['overall_mae']:.4f}, jaw_r={va['jaw_corr']:.3f}, lip_r={va['lip_corr']:.3f}, mouth_MAE={va['mouth_mae']:.4f})\")\n",
    "\n",
    "            epoch_data = dict(epoch=epoch, train=tr, val=va)\n",
    "            history.append(epoch_data)\n",
    "\n",
    "            # Add to visualizer and perform dead neuron analysis\n",
    "            if self.enable_visualization:\n",
    "                self.visualizer.add_epoch_data(epoch_data)\n",
    "\n",
    "                # Perform dead neuron analysis after each epoch\n",
    "                try:\n",
    "                    # Pass the train_loader to the dead neuron analysis\n",
    "                    dead_counts = self.visualizer.analyze_dead_neurons(self.model, train_loader)\n",
    "                    self.visualizer.add_dead_neuron_data(epoch, dead_counts)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Dead neuron analysis error: {e}\")\n",
    "\n",
    "\n",
    "                # Show real-time plot every 5 epochs or at the end\n",
    "                if epoch % 5 == 0 or epoch == num_epochs:\n",
    "                    try:\n",
    "                        # Pass dead_counts to the real-time plot\n",
    "                        self.visualizer.plot_real_time_training(epoch, tr, va, dead_counts)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è  Visualization error: {e}\")\n",
    "\n",
    "            if va['total_loss'] < best_val:\n",
    "                best_val = va['total_loss']\n",
    "                self.save_model(\"models/best_tcn_model.pth\", va)\n",
    "                print(\"‚úÖ New best model\")\n",
    "\n",
    "        print(f\"\\nDone. Best val loss: {best_val:.4f}\")\n",
    "\n",
    "        # Generate comprehensive visualizations at the end\n",
    "        if self.enable_visualization and len(history) > 1:\n",
    "            print(\"\\nüé® Generating training visualizations...\")\n",
    "            try:\n",
    "                # Save training history\n",
    "                self.visualizer.save_history(\"training_history.json\")\n",
    "\n",
    "                # Generate comprehensive plots\n",
    "                print(\"üìä Creating training overview...\")\n",
    "                self.visualizer.plot_training_overview(save=True)\n",
    "\n",
    "                print(\"üìà Creating loss breakdown analysis...\")\n",
    "                self.visualizer.plot_loss_breakdown(save=True)\n",
    "\n",
    "                print(\"üé≠ Creating feature analysis...\")\n",
    "                self.visualizer.plot_feature_analysis(save=True)\n",
    "\n",
    "                print(\"üíÄ Creating dead neuron plot...\")\n",
    "                self.visualizer.plot_dead_neurons(save=True)\n",
    "\n",
    "\n",
    "                print(\"üìã Generating training summary report...\")\n",
    "                self.visualizer.create_training_summary_report()\n",
    "\n",
    "                print(f\"‚úÖ All plots saved to: {self.visualizer.save_dir}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error generating final visualizations: {e}\")\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: pack sequences to reach desired receptive field\n",
    "# -----------------------------\n",
    "import math as _math\n",
    "\n",
    "def pack_sequences(dataset, target_len_frames: int, stride_groups: int | None = None):\n",
    "    \"\"\"\n",
    "    Concatenate multiple short sequences along time to reach at least target_len_frames.\n",
    "    Assumes shapes:\n",
    "      audio   [N, T0, F]\n",
    "      targets [N, T0, 59]\n",
    "      vad     [N, T0]\n",
    "    Returns a new dataset dict with shapes approximately:\n",
    "      [N_new, T_new(>=target_len_frames), ...]\n",
    "    \"\"\"\n",
    "    audio = dataset['audio']\n",
    "    targets = dataset['targets']\n",
    "    vad = dataset['vad']\n",
    "\n",
    "    N, T0, F = audio.shape\n",
    "    groups = int(_math.ceil(target_len_frames / T0))\n",
    "    if groups <= 1:\n",
    "        return dataset  # already long enough\n",
    "\n",
    "    if stride_groups is None or stride_groups < 1:\n",
    "        stride_groups = groups  # non-overlapping by default\n",
    "\n",
    "    new_audio, new_targets, new_vad = [], [], []\n",
    "\n",
    "    for start in range(0, N - groups + 1, stride_groups):\n",
    "        end = start + groups\n",
    "        a = audio[start:end]      # [groups, T0, F]\n",
    "        t = targets[start:end]    # [groups, T0, 59]\n",
    "        v = vad[start:end]        # [groups, T0]\n",
    "        # Concatenate along time\n",
    "        a_cat = a.reshape(-1, F)                # [groups*T0, F]\n",
    "        t_cat = t.reshape(-1, t.shape[-1])      # [groups*T0, 59]\n",
    "        v_cat = v.reshape(-1)                   # [groups*T0]\n",
    "        new_audio.append(a_cat)\n",
    "        new_targets.append(t_cat)\n",
    "        new_vad.append(v_cat)\n",
    "\n",
    "    if not new_audio:\n",
    "        return dataset  # fallback\n",
    "\n",
    "    audio_packed = torch.stack([x for x in new_audio], dim=0)                 # [N_new, T_new, F]\n",
    "    targets_packed = torch.stack([x for x in new_targets], dim=0)             # [N_new, T_new, 59]\n",
    "    vad_packed = torch.stack([x for x in new_vad], dim=0)                     # [N_new, T_new]\n",
    "\n",
    "    return {\n",
    "        'audio': audio_packed.contiguous(),\n",
    "        'targets': targets_packed.contiguous(),\n",
    "        'vad': vad_packed.contiguous(),\n",
    "        'metadata': dataset.get('metadata', {})\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # ‚Äî‚Äî CHANGE THIS LINE to switch datasets ‚Äî‚Äî\n",
    "    DATA_DIR = Path(\"data/train/\")   # <-- for MAIN: Path(\"data/train/\")\n",
    "\n",
    "    # Load data first so we know input feature size\n",
    "    tmp_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dummy_trainer = TCNTrainer(model=nn.Identity(), device=tmp_device, enable_visualization=True)\n",
    "    dataset = dummy_trainer.load_dataset(DATA_DIR)\n",
    "\n",
    "    in_features = dataset['audio'].shape[-1]\n",
    "\n",
    "    # Build the 10-layer model (10s lookback) explicitly\n",
    "    model = create_model(in_features=in_features, out_features=59, seq_len=None)  # use 10-layer config\n",
    "\n",
    "    # Ensure sequences are long enough for model receptive field (~10s)\n",
    "    required_len = model.get_model_info()['receptive_field_frames']\n",
    "    dataset = pack_sequences(dataset, target_len_frames=required_len, stride_groups=None)\n",
    "\n",
    "    # Instantiate trainer and train\n",
    "    trainer = TCNTrainer(model, enable_visualization=True)\n",
    "    trainer.criterion.enable_temporal_loss(0.05)\n",
    "    trainer.criterion.enable_pose_loss(0.02)\n",
    "\n",
    "    # Longer sequences ‚Üí reduce batch size for memory\n",
    "    history = trainer.train(dataset, num_epochs=50 , batch_size=8, validation_split=0.2)\n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # If running in Jupyter, call main automatically\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45860bd9",
   "metadata": {},
   "source": [
    "### Step 3 Do inference \n",
    "\n",
    "üìö Simple Audio-to-Blendshapes Inference Usage:\n",
    "\n",
    "Basic usage:\n",
    "```\n",
    "inference = SimpleAudioInference(\"models/best_tcn_model.pth\")\n",
    "results = inference.infer(\"path/to/audio.wav\")\n",
    "```\n",
    "\n",
    "With options:\n",
    "```\n",
    "results = inference.infer(\n",
    "    \"path/to/audio.mp3\",\n",
    "    max_duration=30,      # Process first 30 seconds\n",
    "    target_fps=30,        # Output at 30fps\n",
    "    save_json=\"output.json\"  # Save results\n",
    ")\n",
    "```\n",
    "\n",
    "Visualize results:\n",
    "```\n",
    "inference.visualize_results(results)\n",
    "inference.get_sample_values(results)\n",
    "```\n",
    "\n",
    "Quick one-liner:\n",
    "```\n",
    "results = quick_infer(\"audio.wav\")\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6902dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import librosa\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Model architecture (inline)\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation // 2\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.norm1 = nn.BatchNorm1d(out_ch)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.norm2 = nn.BatchNorm1d(out_ch)\n",
    "        self.act2 = nn.GELU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.res = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.norm1(y)\n",
    "        y = self.act1(y)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.norm2(y)\n",
    "        y = self.act2(y)\n",
    "        y = self.dropout2(y)\n",
    "        return y + self.res(x)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int = 59,\n",
    "                 hidden: int = 128, levels: int = 4, kernel_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        chans = [in_features] + [hidden] * levels\n",
    "        blocks = []\n",
    "        for i in range(levels):\n",
    "            dilation = 2 ** i\n",
    "            blocks.append(TemporalBlock(chans[i], chans[i + 1], kernel_size=kernel_size, dilation=dilation, dropout=dropout))\n",
    "        self.tcn = nn.Sequential(*blocks)\n",
    "        self.head = nn.Conv1d(hidden, out_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = self.tcn(x)\n",
    "        y = self.head(y)\n",
    "        y = y.permute(0, 2, 1)\n",
    "        return y\n",
    "\n",
    "# Simple inference class\n",
    "class AudioInference:\n",
    "    def __init__(self, model_path, device='auto'):\n",
    "        if device == 'auto':\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.device = device\n",
    "        self.model_path = model_path\n",
    "        self.sample_rate = 16000\n",
    "        self.n_mels = 80\n",
    "        self.hop_length = 160\n",
    "        self.win_length = 400\n",
    "        self.n_fft = 512\n",
    "        \n",
    "        self.model = self._load_model()\n",
    "        \n",
    "        self.blendshape_names = [\n",
    "            '_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft', \n",
    "            'browOuterUpRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft', \n",
    "            'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft', 'eyeLookInRight', \n",
    "            'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft', \n",
    "            'eyeSquintRight', 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen', \n",
    "            'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight', 'mouthFrownLeft', \n",
    "            'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight', \n",
    "            'mouthPressLeft', 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower', \n",
    "            'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft', 'mouthSmileRight', \n",
    "            'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight', \n",
    "            'noseSneerLeft', 'noseSneerRight'\n",
    "        ]\n",
    "        \n",
    "    def _load_model(self):\n",
    "        checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "        \n",
    "        if 'model_config' in checkpoint:\n",
    "            in_features = checkpoint['model_config'].get('in_features', 80)\n",
    "        else:\n",
    "            in_features = 80\n",
    "            \n",
    "        model = TCNModel(in_features=in_features, out_features=59)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def infer(self, audio_path, max_duration=None, target_fps=30, save_json=None):\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True, duration=max_duration)\n",
    "        \n",
    "        # Extract mel features\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sample_rate, n_mels=self.n_mels,\n",
    "            hop_length=self.hop_length, win_length=self.win_length, n_fft=self.n_fft\n",
    "        )\n",
    "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_features = log_mel.T\n",
    "        \n",
    "        # Normalize\n",
    "        mel_mean = mel_features.mean(axis=0, keepdims=True)\n",
    "        mel_std = mel_features.std(axis=0, keepdims=True) + 1e-6\n",
    "        mel_features = (mel_features - mel_mean) / mel_std\n",
    "        \n",
    "        # Run inference\n",
    "        mel_tensor = torch.FloatTensor(mel_features).to(self.device)\n",
    "        batch_size = 32\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(mel_tensor), batch_size):\n",
    "                end_idx = min(i + batch_size, len(mel_tensor))\n",
    "                batch_mel = mel_tensor[i:end_idx].unsqueeze(0)\n",
    "                batch_pred = self.model(batch_mel)\n",
    "                all_predictions.append(batch_pred.squeeze(0).cpu().numpy())\n",
    "        \n",
    "        predictions = np.concatenate(all_predictions, axis=0)\n",
    "        \n",
    "        # Downsample to target fps\n",
    "        original_fps = self.sample_rate / self.hop_length\n",
    "        if target_fps < original_fps:\n",
    "            downsample_ratio = original_fps / target_fps\n",
    "            num_output_frames = int(len(predictions) / downsample_ratio)\n",
    "            indices = np.linspace(0, len(predictions) - 1, num_output_frames, dtype=int)\n",
    "            predictions = predictions[indices]\n",
    "        \n",
    "        # Process results\n",
    "        blendshapes = predictions[:, :52]\n",
    "        head_pose = predictions[:, 52:]\n",
    "        frame_duration = 1.0 / target_fps\n",
    "        timestamps = np.arange(len(predictions)) * frame_duration\n",
    "        \n",
    "        results = {\n",
    "            'num_frames': len(predictions),\n",
    "            'fps': target_fps,\n",
    "            'duration': timestamps[-1] if len(timestamps) > 0 else 0,\n",
    "            'timestamps': timestamps,\n",
    "            'blendshapes': blendshapes,\n",
    "            'head_pose': head_pose,\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "        \n",
    "        # Save JSON if requested\n",
    "        if save_json:\n",
    "            frames = []\n",
    "            for i in range(results['num_frames']):\n",
    "                blendshapes_dict = {}\n",
    "                for j, name in enumerate(self.blendshape_names):\n",
    "                    blendshapes_dict[name] = float(results['blendshapes'][i][j])\n",
    "                \n",
    "                frame_data = {\n",
    "                    'frame_index': i,\n",
    "                    'timestamp': int(results['timestamps'][i] * 1000),\n",
    "                    'blendshapes': blendshapes_dict,\n",
    "                    'headPosition': {\n",
    "                        'x': float(results['head_pose'][i][0]),\n",
    "                        'y': float(results['head_pose'][i][1]),\n",
    "                        'z': float(results['head_pose'][i][2])\n",
    "                    },\n",
    "                    'headRotation': {\n",
    "                        'w': float(results['head_pose'][i][3]),\n",
    "                        'x': float(results['head_pose'][i][4]),\n",
    "                        'y': float(results['head_pose'][i][5]),\n",
    "                        'z': float(results['head_pose'][i][6])\n",
    "                    },\n",
    "                    'has_face': True\n",
    "                }\n",
    "                frames.append(frame_data)\n",
    "            \n",
    "            json_output = {\n",
    "                'sessionInfo': {\n",
    "                    'sessionId': f\"inference_{hash(results['audio_path']) % 10000}\",\n",
    "                    'targetFPS': results['fps'],\n",
    "                    'audioPath': results['audio_path']\n",
    "                },\n",
    "                'frameCount': results['num_frames'],\n",
    "                'failedFrames': 0,\n",
    "                'failureRate': 0.0,\n",
    "                'frames': frames\n",
    "            }\n",
    "            \n",
    "            Path(save_json).parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(save_json, 'w') as f:\n",
    "                json.dump(json_output, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"sample_audio/sample2.wav\"\n",
    "model_file = \"models/best_tcn_model_train_50.pth\"\n",
    "\n",
    "inference = AudioInference(model_file)\n",
    "results = inference.infer(\n",
    "    audio_path=audio_file,\n",
    "    # max_duration=10,\n",
    "    target_fps=30,\n",
    "    save_json=\"output/inference_results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27947c2",
   "metadata": {},
   "source": [
    "### Step 4 ONNX conversion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c904552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install dependecies for step 4 first\n",
    "# uncomment below if not installed and then run the cell\n",
    "# !pip install --quiet onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting TCN Model ONNX Conversion\n",
      "==================================================\n",
      "üîÑ Loading trained model from: models/best_tcn_model_train_50.pth\n",
      "üìã Model configuration:\n",
      "   in_features: 80\n",
      "   out_features: 59\n",
      "   hidden: 128\n",
      "   levels: 4\n",
      "   kernel_size: 3\n",
      "   dropout: 0.1\n",
      "‚úÖ Model loaded successfully!\n",
      "   Parameters: 395,835\n",
      "\n",
      "üîÑ Converting to ONNX format...\n",
      "üìä Input shape: torch.Size([1, 100, 80])\n",
      "   Batch size: 1\n",
      "   Sequence length: 100\n",
      "   Input features: 80\n",
      "‚úÖ ONNX model exported to: models/best_tcn_model_train_50.onnx\n",
      "\n",
      "üîç Verifying ONNX model...\n",
      "‚úÖ ONNX model structure is valid\n",
      "üìä ONNX Runtime Info:\n",
      "   Input name: audio_features\n",
      "   Input shape: ['batch_size', 'sequence_length', 80]\n",
      "   Output name: blendshapes\n",
      "   Output shape: ['batch_size', 'sequence_length', 59]\n",
      "‚úÖ ONNX Runtime inference successful!\n",
      "   Test input shape: (1, 50, 80)\n",
      "   Test output shape: (1, 50, 59)\n",
      "   Expected output features: 59\n",
      "‚úÖ Output dimensions match expected (59: 52 blendshapes + 7 pose)\n",
      "\n",
      "üîç Comparing PyTorch vs ONNX outputs...\n",
      "üìä Output comparison:\n",
      "   PyTorch output shape: (1, 50, 59)\n",
      "   ONNX output shape: (1, 50, 59)\n",
      "   Max absolute difference: 0.00000048\n",
      "   Mean absolute difference: 0.00000002\n",
      "‚úÖ Outputs match within tolerance (1e-05)\n",
      "\n",
      "üöÄ Optimizing ONNX model...\n",
      "‚úÖ Optimized ONNX model saved: models/best_tcn_model_train_50_optimized.onnx\n",
      "üìÑ Conversion info saved: models/best_tcn_model_train_50.json\n",
      "\n",
      "üéâ ONNX Conversion Complete!\n",
      "==================================================\n",
      "üìÅ Files created:\n",
      "   - ONNX model: models/best_tcn_model_train_50.onnx\n",
      "   - Optimized: models/best_tcn_model_train_50_optimized.onnx\n",
      "   - Info: models/best_tcn_model_train_50.json\n",
      "\n",
      "üìä Model size: 1.5 MB\n",
      "\n",
      "üöÄ Usage:\n",
      "   import onnxruntime as ort\n",
      "   session = ort.InferenceSession('models/best_tcn_model_train_50.onnx')\n",
      "   outputs = session.run(None, {'audio_features': your_input})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-09-11 13:02:25.967456 [W:onnxruntime:, inference_session.cc:2212 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# step 4 convert to ONNX format\n",
    "# please take the model from best_tcn_model.pth file and convert it to onnx format\n",
    "# use the architecture and do it proprely please\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Step 4: Convert trained TCN model (.pth) to ONNX format\n",
    "Converts the best_tcn_model.pth to optimized ONNX runtime format\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Suppress ONNX warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def load_trained_model(model_path=\"models/best_tcn_model.pth\"):\n",
    "    \"\"\"\n",
    "    Load the trained TCN model from .pth file\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Loading trained model from: {model_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model_config = checkpoint['model_config']\n",
    "    \n",
    "    print(f\"üìã Model configuration:\")\n",
    "    for key, value in model_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Recreate model with saved config\n",
    "    model = TCNModel(**model_config)\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model, model_config\n",
    "\n",
    "def convert_to_onnx(model, model_config, onnx_path=\"models/tcn_model.onnx\"):\n",
    "    \"\"\"\n",
    "    Convert PyTorch TCN model to ONNX format\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Converting to ONNX format...\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input based on model config\n",
    "    batch_size = 1\n",
    "    sequence_length = 100  # Reasonable sequence length for inference\n",
    "    input_features = model_config['in_features']\n",
    "    \n",
    "    # Create dummy input tensor (batch, time, features)\n",
    "    dummy_input = torch.randn(batch_size, sequence_length, input_features)\n",
    "    \n",
    "    print(f\"üìä Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Sequence length: {sequence_length}\")\n",
    "    print(f\"   Input features: {input_features}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(onnx_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model,                          # PyTorch model\n",
    "            dummy_input,                    # Model input\n",
    "            onnx_path,                      # Output path\n",
    "            export_params=True,             # Store trained parameters\n",
    "            opset_version=11,               # ONNX version\n",
    "            do_constant_folding=True,       # Optimize constant folding\n",
    "            input_names=['audio_features'], # Input tensor name\n",
    "            output_names=['blendshapes'],   # Output tensor name\n",
    "            dynamic_axes={                  # Dynamic dimensions\n",
    "                'audio_features': {\n",
    "                    0: 'batch_size',\n",
    "                    1: 'sequence_length'\n",
    "                },\n",
    "                'blendshapes': {\n",
    "                    0: 'batch_size', \n",
    "                    1: 'sequence_length'\n",
    "                }\n",
    "            },\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ ONNX model exported to: {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def verify_onnx_model(onnx_path, model_config):\n",
    "    \"\"\"\n",
    "    Verify ONNX model validity and test inference\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Verifying ONNX model...\")\n",
    "    \n",
    "    # Load and check ONNX model\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"‚úÖ ONNX model structure is valid\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ONNX model validation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test ONNX Runtime inference\n",
    "    try:\n",
    "        # Create ONNX Runtime session\n",
    "        ort_session = ort.InferenceSession(onnx_path)\n",
    "        \n",
    "        # Get input/output info\n",
    "        input_info = ort_session.get_inputs()[0]\n",
    "        output_info = ort_session.get_outputs()[0]\n",
    "        \n",
    "        print(f\"üìä ONNX Runtime Info:\")\n",
    "        print(f\"   Input name: {input_info.name}\")\n",
    "        print(f\"   Input shape: {input_info.shape}\")\n",
    "        print(f\"   Output name: {output_info.name}\")\n",
    "        print(f\"   Output shape: {output_info.shape}\")\n",
    "        \n",
    "        # Test inference with dummy data\n",
    "        test_input = np.random.randn(1, 50, model_config['in_features']).astype(np.float32)\n",
    "        \n",
    "        ort_outputs = ort_session.run(None, {input_info.name: test_input})\n",
    "        output_shape = ort_outputs[0].shape\n",
    "        \n",
    "        print(f\"‚úÖ ONNX Runtime inference successful!\")\n",
    "        print(f\"   Test input shape: {test_input.shape}\")\n",
    "        print(f\"   Test output shape: {output_shape}\")\n",
    "        print(f\"   Expected output features: {model_config['out_features']}\")\n",
    "        \n",
    "        # Verify output dimensions\n",
    "        if output_shape[-1] == model_config['out_features']:\n",
    "            print(\"‚úÖ Output dimensions match expected (59: 52 blendshapes + 7 pose)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Output dimension mismatch: got {output_shape[-1]}, expected {model_config['out_features']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ONNX Runtime test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def compare_pytorch_vs_onnx(pytorch_model, onnx_path, model_config):\n",
    "    \"\"\"\n",
    "    Compare PyTorch and ONNX model outputs for consistency\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Comparing PyTorch vs ONNX outputs...\")\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = torch.randn(1, 50, model_config['in_features'])\n",
    "    \n",
    "    # PyTorch inference\n",
    "    pytorch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = pytorch_model(test_input).numpy()\n",
    "    \n",
    "    # ONNX inference\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    onnx_output = ort_session.run(None, {'audio_features': test_input.numpy()})[0]\n",
    "    \n",
    "    # Compare outputs\n",
    "    max_diff = np.max(np.abs(pytorch_output - onnx_output))\n",
    "    mean_diff = np.mean(np.abs(pytorch_output - onnx_output))\n",
    "    \n",
    "    print(f\"üìä Output comparison:\")\n",
    "    print(f\"   PyTorch output shape: {pytorch_output.shape}\")\n",
    "    print(f\"   ONNX output shape: {onnx_output.shape}\")\n",
    "    print(f\"   Max absolute difference: {max_diff:.8f}\")\n",
    "    print(f\"   Mean absolute difference: {mean_diff:.8f}\")\n",
    "    \n",
    "    # Check if outputs are close (allowing for small numerical differences)\n",
    "    tolerance = 1e-5\n",
    "    if max_diff < tolerance:\n",
    "        print(f\"‚úÖ Outputs match within tolerance ({tolerance})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Outputs differ by more than tolerance ({tolerance})\")\n",
    "        return False\n",
    "\n",
    "def optimize_onnx_model(onnx_path):\n",
    "    \"\"\"\n",
    "    Optimize ONNX model for better performance\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Optimizing ONNX model...\")\n",
    "    \n",
    "    try:\n",
    "        # Create optimized session options\n",
    "        sess_options = ort.SessionOptions()\n",
    "        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        sess_options.optimized_model_filepath = onnx_path.replace('.onnx', '_optimized.onnx')\n",
    "        \n",
    "        # Create session with optimization\n",
    "        ort_session = ort.InferenceSession(onnx_path, sess_options)\n",
    "        \n",
    "        print(f\"‚úÖ Optimized ONNX model saved: {sess_options.optimized_model_filepath}\")\n",
    "        return sess_options.optimized_model_filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Optimization failed: {e}\")\n",
    "        return onnx_path\n",
    "\n",
    "def save_conversion_info(model_config, onnx_path, pytorch_path=\"models/best_tcn_model.pth\"):\n",
    "    \"\"\"\n",
    "    Save conversion metadata and usage instructions\n",
    "    \"\"\"\n",
    "    info_path = str(Path(onnx_path).with_suffix('.json'))\n",
    "    \n",
    "    conversion_info = {\n",
    "        \"model_type\": \"TCN_Audio_to_Blendshapes\",\n",
    "        \"pytorch_source\": pytorch_path,\n",
    "        \"onnx_path\": onnx_path,\n",
    "        \"input_shape\": [\"batch_size\", \"sequence_length\", model_config['in_features']],\n",
    "        \"output_shape\": [\"batch_size\", \"sequence_length\", model_config['out_features']],\n",
    "        \"input_name\": \"audio_features\",\n",
    "        \"output_name\": \"blendshapes\",\n",
    "        \"description\": {\n",
    "            \"input\": f\"{model_config['in_features']} mel-spectrogram features\",\n",
    "            \"output\": \"59 values: 52 blendshapes [0,1] + 7 head pose [-0.2,0.2]\"\n",
    "        },\n",
    "        \"usage_example\": {\n",
    "            \"python\": \"\"\"\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "session = ort.InferenceSession('tcn_model.onnx')\n",
    "\n",
    "# Prepare input (batch_size, sequence_length, 80)\n",
    "audio_features = np.random.randn(1, 100, 80).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {'audio_features': audio_features})\n",
    "blendshapes = outputs[0]  # Shape: (1, 100, 59)\n",
    "\"\"\"\n",
    "        },\n",
    "        \"model_config\": model_config\n",
    "    }\n",
    "    \n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(conversion_info, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Conversion info saved: {info_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main ONNX conversion pipeline\n",
    "    \"\"\"\n",
    "    print(\"üéØ Starting TCN Model ONNX Conversion\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Paths\n",
    "    pytorch_model_path = \"models/best_tcn_model_train_50.pth\"\n",
    "    onnx_model_path = \"models/best_tcn_model_train_50.onnx\"\n",
    "    \n",
    "    # Check if PyTorch model exists\n",
    "    if not Path(pytorch_model_path).exists():\n",
    "        print(f\"‚ùå PyTorch model not found: {pytorch_model_path}\")\n",
    "        print(\"   Please ensure you have trained and saved the model first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load trained model\n",
    "        model, model_config = load_trained_model(pytorch_model_path)\n",
    "        \n",
    "        # Step 2: Convert to ONNX\n",
    "        onnx_path = convert_to_onnx(model, model_config, onnx_model_path)\n",
    "        \n",
    "        # Step 3: Verify ONNX model\n",
    "        if not verify_onnx_model(onnx_path, model_config):\n",
    "            print(\"‚ùå ONNX verification failed!\")\n",
    "            return\n",
    "        \n",
    "        # Step 4: Compare outputs\n",
    "        if not compare_pytorch_vs_onnx(model, onnx_path, model_config):\n",
    "            print(\"‚ö†Ô∏è  Output comparison shows differences (may still be usable)\")\n",
    "        \n",
    "        # Step 5: Optimize model\n",
    "        optimized_path = optimize_onnx_model(onnx_path)\n",
    "        \n",
    "        # Step 6: Save conversion info\n",
    "        save_conversion_info(model_config, onnx_path, pytorch_model_path)\n",
    "        \n",
    "        print(\"\\nüéâ ONNX Conversion Complete!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìÅ Files created:\")\n",
    "        print(f\"   - ONNX model: {onnx_path}\")\n",
    "        if optimized_path != onnx_path:\n",
    "            print(f\"   - Optimized: {optimized_path}\")\n",
    "        print(f\"   - Info: {Path(onnx_path).with_suffix('.json')}\")\n",
    "        \n",
    "        # Get file sizes\n",
    "        onnx_size = Path(onnx_path).stat().st_size / (1024*1024)\n",
    "        print(f\"\\nüìä Model size: {onnx_size:.1f} MB\")\n",
    "        \n",
    "        print(f\"\\nüöÄ Usage:\")\n",
    "        print(f\"   import onnxruntime as ort\")\n",
    "        print(f\"   session = ort.InferenceSession('{onnx_path}')\")\n",
    "        print(f\"   outputs = session.run(None, {{'audio_features': your_input}})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Conversion failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the conversion\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # If running in Jupyter, call main automatically\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b15a0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TCN ‚Üí ONNX (B,T,F ‚Üí B,T,D)\n",
      "============================================================\n",
      "üîÑ Loading trained model from: models/best_tcn_model_train_50.pth\n",
      "‚úÖ Model loaded. Params: 395,835\n",
      "\n",
      "üîÑ Converting to ONNX format...\n",
      "üîé Core output layout detected: BTD\n",
      "‚úÖ ONNX exported ‚Üí models/best_tcn_model_train_50.onnx\n",
      "\n",
      "üîç Verifying ONNX graph...\n",
      "‚úÖ Structure OK\n",
      "\n",
      "üß™ ONNX Runtime quick test...\n",
      "   Input  : name=audio_features, shape=['batch_size', 'sequence_length', 80]\n",
      "   Output : name=blendshapes, shape=['batch_size', 'sequence_length', 59]\n",
      "   Test in : (1, 50, 80) ; out: (1, 50, 59)\n",
      "\n",
      "üìä Comparing PyTorch vs ONNX on identical input...\n",
      "   Torch out: (1, 50, 59) | ONNX out: (1, 50, 59)\n",
      "   Max abs diff  : 4.768372e-07\n",
      "   Mean abs diff : 1.565447e-08\n",
      "‚úÖ Close within tolerance (1e-05)\n",
      "üìÑ Saved conversion info ‚Üí models/best_tcn_model_train_50.json\n",
      "\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Step 4: Convert trained TCN model (.pth) to ONNX format\n",
    "\n",
    "Contract guaranteed:\n",
    "- ONNX input : [B, T, F]  (batch, time, features)\n",
    "- ONNX output: [B, T, D]  (D = 59)\n",
    "\n",
    "It validates ONNX vs PyTorch on identical inputs.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---- Your model implementation should be importable here ----\n",
    "# from your_module import TCNModel\n",
    "\n",
    "def load_trained_model(model_path=\"models/best_tcn_model_train_50.pth\"):\n",
    "    print(f\"üîÑ Loading trained model from: {model_path}\")\n",
    "    ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "    model_config = ckpt[\"model_config\"]\n",
    "    model = TCNModel(**model_config)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Model loaded. Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    return model, model_config\n",
    "\n",
    "def _infer_core_output_layout(model: torch.nn.Module, in_features: int) -> str:\n",
    "    \"\"\"\n",
    "    Probe the model with the layout it actually expects ([B,T,F] for your TCNModel).\n",
    "    Detect whether the core returns [B,T,D] (BTD) or [B,D,T] (BDT).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1, 16, in_features)  # [B,T,F]  <-- important\n",
    "        out = model(z)\n",
    "    if out.dim() == 3:\n",
    "        if out.shape[1] == 16:  # time at dim=1\n",
    "            return \"BTD\"\n",
    "        elif out.shape[2] == 16:  # time at dim=2\n",
    "            return \"BDT\"\n",
    "    return \"BTD\"\n",
    "\n",
    "class _Wrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Export-time wrapper that:\n",
    "    - Accepts [B,T,F] (what your model expects)\n",
    "    - Calls your core model directly\n",
    "    - If the core returns [B,D,T], transpose to [B,T,D]\n",
    "    \"\"\"\n",
    "    def __init__(self, core: torch.nn.Module, core_layout: str):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.core_layout = core_layout\n",
    "\n",
    "    def forward(self, x_btf: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.core(x_btf)  # your TCNModel expects [B,T,F]\n",
    "        if self.core_layout == \"BDT\":\n",
    "            y = y.transpose(1, 2).contiguous()  # -> [B,T,D]\n",
    "        return y\n",
    "\n",
    "def convert_to_onnx(\n",
    "    model: torch.nn.Module,\n",
    "    model_config: dict,\n",
    "    onnx_path=\"models/best_tcn_model_train_50.onnx\",\n",
    "    opset: int = 17,\n",
    "    fixed_seq_len: int | None = None,\n",
    "):\n",
    "    print(\"\\nüîÑ Converting to ONNX format...\")\n",
    "    core_layout = _infer_core_output_layout(model, model_config[\"in_features\"])\n",
    "    print(f\"üîé Core output layout detected: {core_layout}\")\n",
    "\n",
    "    wrapped = _Wrapper(model, core_layout).eval()\n",
    "\n",
    "    B = 1\n",
    "    T = fixed_seq_len if fixed_seq_len is not None else 100\n",
    "    F = model_config[\"in_features\"]\n",
    "    dummy = torch.randn(B, T, F, dtype=torch.float32)  # [B,T,F]\n",
    "\n",
    "    Path(onnx_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dynamic_axes = {\n",
    "        \"audio_features\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"blendshapes\":    {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    }\n",
    "    if fixed_seq_len is not None:\n",
    "        dynamic_axes = {\n",
    "            \"audio_features\": {0: \"batch_size\"},\n",
    "            \"blendshapes\":    {0: \"batch_size\"},\n",
    "        }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            wrapped,\n",
    "            dummy,\n",
    "            onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=opset,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\"audio_features\"],\n",
    "            output_names=[\"blendshapes\"],\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ ONNX exported ‚Üí {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def verify_onnx(onnx_path: str):\n",
    "    print(\"\\nüîç Verifying ONNX graph...\")\n",
    "    m = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(m)\n",
    "    print(\"‚úÖ Structure OK\")\n",
    "\n",
    "def test_runtime(onnx_path: str, in_features: int, out_features: int, seq_len: int = 50):\n",
    "    print(\"\\nüß™ ONNX Runtime quick test...\")\n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    sess = ort.InferenceSession(onnx_path, so, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "    inp = sess.get_inputs()[0]\n",
    "    out = sess.get_outputs()[0]\n",
    "    print(f\"   Input  : name={inp.name}, shape={inp.shape}\")\n",
    "    print(f\"   Output : name={out.name}, shape={out.shape}\")\n",
    "\n",
    "    x = np.random.randn(1, seq_len, in_features).astype(np.float32)  # [B,T,F]\n",
    "    y = sess.run(None, {inp.name: np.ascontiguousarray(x)})[0]\n",
    "    print(f\"   Test in : {x.shape} ; out: {y.shape}\")\n",
    "    if y.shape[-1] != out_features:\n",
    "        print(f\"‚ö†Ô∏è  Expected last dim {out_features}, got {y.shape[-1]}\")\n",
    "\n",
    "def compare_torch_vs_onnx(model, onnx_path, in_features: int, seq_len: int, device=\"cpu\"):\n",
    "    print(\"\\nüìä Comparing PyTorch vs ONNX on identical input...\")\n",
    "    model.eval()\n",
    "    x = torch.randn(1, seq_len, in_features, dtype=torch.float32, device=device)  # [B,T,F]\n",
    "    with torch.no_grad():\n",
    "        y_torch = model(x).cpu().numpy()  # your TCNModel returns [B,T,D]\n",
    "    so = ort.SessionOptions()\n",
    "    sess = ort.InferenceSession(onnx_path, so, providers=[\"CPUExecutionProvider\"])\n",
    "    y_onnx = sess.run(None, {\"audio_features\": x.cpu().numpy()})[0]\n",
    "\n",
    "    max_diff = np.max(np.abs(y_torch - y_onnx))\n",
    "    mean_diff = np.mean(np.abs(y_torch - y_onnx))\n",
    "    print(f\"   Torch out: {y_torch.shape} | ONNX out: {y_onnx.shape}\")\n",
    "    print(f\"   Max abs diff  : {max_diff:.6e}\")\n",
    "    print(f\"   Mean abs diff : {mean_diff:.6e}\")\n",
    "    tol = 1e-5\n",
    "    if max_diff < tol:\n",
    "        print(f\"‚úÖ Close within tolerance ({tol})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Larger than tolerance ({tol}). If shapes match, check mel/log/normalization parity in the browser.\")\n",
    "\n",
    "def save_info(model_config, onnx_path, pytorch_path):\n",
    "    info_path = str(Path(onnx_path).with_suffix(\".json\"))\n",
    "    meta = {\n",
    "        \"model_type\": \"TCN_Audio_to_Blendshapes\",\n",
    "        \"pytorch_source\": pytorch_path,\n",
    "        \"onnx_path\": onnx_path,\n",
    "        \"input_name\": \"audio_features\",\n",
    "        \"output_name\": \"blendshapes\",\n",
    "        \"input_shape\": [\"batch_size\", \"sequence_length\", model_config[\"in_features\"]],\n",
    "        \"output_shape\": [\"batch_size\", \"sequence_length\", model_config[\"out_features\"]],\n",
    "        \"expects_layout\": {\"input\": \"B,T,F\", \"output\": \"B,T,D\"},\n",
    "    }\n",
    "    with open(info_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"üìÑ Saved conversion info ‚Üí {info_path}\")\n",
    "\n",
    "def main():\n",
    "    print(\"üéØ TCN ‚Üí ONNX (B,T,F ‚Üí B,T,D)\"); print(\"=\"*60)\n",
    "    pytorch_model_path = \"models/best_tcn_model_train_50.pth\"\n",
    "    onnx_model_path    = \"models/best_tcn_model_train_50.onnx\"\n",
    "    if not Path(pytorch_model_path).exists():\n",
    "        print(f\"‚ùå Missing {pytorch_model_path}\")\n",
    "        return\n",
    "\n",
    "    model, cfg = load_trained_model(pytorch_model_path)\n",
    "    onnx_path = convert_to_onnx(model, cfg, onnx_model_path, opset=17, fixed_seq_len=None)\n",
    "    verify_onnx(onnx_path)\n",
    "    test_runtime(onnx_path, cfg[\"in_features\"], cfg[\"out_features\"], seq_len=50)\n",
    "    compare_torch_vs_onnx(model, onnx_path, cfg[\"in_features\"], seq_len=50)\n",
    "    save_info(cfg, onnx_path, pytorch_model_path)\n",
    "    print(\"\\n‚úÖ Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec066f4",
   "metadata": {},
   "source": [
    "### Finally finished !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f71d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
