model
<!DOCTYPE html>
<html lang="en">

<head>
    <title>ONNX Audio to Blendshapes Inference - Fixed</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/fft-js@0.0.12/lib/fft.js"></script>
    <!-- Import map to resolve Three.js modules -->
    <script type="importmap">
    {
        "imports": {
            "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
        }
    }
    </script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 300;
        }

        .section {
            margin: 30px 0;
            padding: 20px;
            border-radius: 15px;
            background: rgba(102, 126, 234, 0.05);
            border: 1px solid rgba(102, 126, 234, 0.1);
        }

        .section h3 {
            margin-top: 0;
            color: #667eea;
            font-size: 1.4em;
        }

        .file-input-wrapper {
            position: relative;
            display: inline-block;
            width: 100%;
        }

        .file-input {
            width: 100%;
            padding: 15px;
            border: 2px dashed #667eea;
            border-radius: 10px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            background: white;
        }

        .file-input:hover {
            border-color: #764ba2;
            background: rgba(102, 126, 234, 0.05);
        }

        input[type="file"] {
            display: none;
        }

        button {
            background: linear-gradient(45deg, #667eea, #764ba2);
            border: none;
            color: white;
            padding: 15px 30px;
            margin: 10px 5px;
            border-radius: 50px;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: #f0f0f0;
            border-radius: 4px;
            overflow: hidden;
            margin: 20px 0;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(45deg, #667eea, #764ba2);
            width: 0%;
            transition: width 0.3s ease;
        }

        .status {
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: 500;
        }

        .status.info {
            background: rgba(52, 152, 219, 0.1);
            color: #3498db;
            border: 2px solid rgba(52, 152, 219, 0.3);
        }

        .status.success {
            background: rgba(46, 213, 115, 0.1);
            color: #2ed573;
            border: 2px solid rgba(46, 213, 115, 0.3);
        }

        .status.error {
            background: rgba(255, 87, 87, 0.1);
            color: #ff5757;
            border: 2px solid rgba(255, 87, 87, 0.3);
        }

        .audio-preview {
            margin: 20px 0;
            width: 100%;
        }

        audio {
            width: 100%;
            border-radius: 10px;
        }

        .results-section {
            display: none;
            margin-top: 30px;
        }

        .download-link {
            display: inline-block;
            background: linear-gradient(45deg, #2ed573, #17c0eb);
            color: white;
            text-decoration: none;
            padding: 15px 30px;
            border-radius: 50px;
            font-weight: 500;
            margin: 10px 5px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(46, 213, 115, 0.3);
        }

        .download-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(46, 213, 115, 0.4);
        }

        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .info-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            text-align: center;
        }

        .info-card .value {
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }

        .info-card .label {
            color: #666;
            margin-top: 5px;
        }

        .spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: #fff;
            animation: spin 1s ease-in-out infinite;
            margin-right: 10px;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        /* Recording-specific styles */
        .recording-section,
        .upload-section {
            background: rgba(255, 255, 255, 0.5);
            border-radius: 15px;
            padding: 20px;
            margin: 15px 0;
        }

        .recording-section h4,
        .upload-section h4 {
            margin-top: 0;
            color: #667eea;
            font-size: 1.2em;
            text-align: center;
        }

        .recording-controls {
            text-align: center;
        }

        .timer {
            font-size: 24px;
            font-weight: bold;
            margin: 20px 0;
            color: #333;
            font-family: 'Courier New', monospace;
        }

        .wave-animation {
            display: none;
            align-items: center;
            justify-content: center;
            margin: 20px 0;
        }

        .wave-animation.active {
            display: flex;
        }

        .wave {
            width: 4px;
            height: 20px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            margin: 0 2px;
            border-radius: 2px;
            animation: wave 1s infinite ease-in-out;
        }

        .wave:nth-child(2) {
            animation-delay: 0.1s;
        }

        .wave:nth-child(3) {
            animation-delay: 0.2s;
        }

        .wave:nth-child(4) {
            animation-delay: 0.3s;
        }

        .wave:nth-child(5) {
            animation-delay: 0.4s;
        }

        @keyframes wave {

            0%,
            40%,
            100% {
                transform: scaleY(0.4);
            }

            20% {
                transform: scaleY(1);
            }
        }

        .divider {
            text-align: center;
            margin: 30px 0;
            position: relative;
        }

        .divider::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 0;
            right: 0;
            height: 1px;
            background: linear-gradient(90deg, transparent, #667eea, transparent);
        }

        .divider span {
            background: rgba(255, 255, 255, 0.95);
            padding: 0 20px;
            color: #667eea;
            font-weight: 600;
        }

        .controls {
            margin: 20px 0;
            text-align: center;
        }

        .status.recording {
            background: rgba(255, 87, 87, 0.1);
            color: #ff5757;
            border: 2px solid rgba(255, 87, 87, 0.3);
        }

        .status.ready {
            background: rgba(46, 213, 115, 0.1);
            color: #2ed573;
            border: 2px solid rgba(46, 213, 115, 0.3);
        }

        .status.idle {
            background: rgba(116, 125, 136, 0.1);
            color: #747d8c;
            border: 2px solid rgba(116, 125, 136, 0.3);
        }

        .audio-info {
            margin-top: 10px;
            padding: 10px;
            background: rgba(102, 126, 234, 0.1);
            border-radius: 8px;
            font-size: 14px;
            color: #667eea;
        }

        /* 3D Scene Styles */
        .scene-container {
            height: 400px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
            border: 2px solid rgba(102, 126, 234, 0.3);
            border-radius: 15px;
            position: relative;
            overflow: hidden;
            margin: 20px 0;
        }

        .scene-loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            color: #667eea;
        }

        .scene-loading .spinner {
            border: 3px solid rgba(102, 126, 234, 0.3);
            border-top: 3px solid #667eea;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 15px;
        }

        .main-grid {
            display: grid;
            grid-template-columns: 1fr 350px;
            gap: 30px;
            margin-top: 20px;
        }

        .left-panel {
            display: flex;
            flex-direction: column;
        }

        .right-panel {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            padding: 20px;
            height: fit-content;
        }

        .blendshapes-panel {
            background: rgba(102, 126, 234, 0.05);
            border-radius: 10px;
            padding: 15px;
            height: 350px;
            overflow-y: auto;
            border: 1px solid rgba(102, 126, 234, 0.2);
        }

        .blendshape-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
            font-size: 12px;
        }

        .blendshape-name {
            flex: 1;
            margin-right: 10px;
            color: #667eea;
            font-weight: 500;
        }

        .blendshape-value {
            color: #333;
            font-family: 'Courier New', monospace;
            font-weight: bold;
            min-width: 45px;
            text-align: right;
        }

        .blendshape-bar {
            width: 100%;
            height: 4px;
            background: rgba(102, 126, 234, 0.2);
            border-radius: 2px;
            overflow: hidden;
            margin-top: 2px;
        }

        .blendshape-fill {
            height: 100%;
            transition: width 0.1s ease;
            border-radius: 2px;
        }

        .blendshape-fill.high {
            background: linear-gradient(45deg, #22c55e, #16a34a);
        }

        .blendshape-fill.medium {
            background: linear-gradient(45deg, #eab308, #d97706);
        }

        .blendshape-fill.low {
            background: linear-gradient(45deg, #667eea, #764ba2);
        }

        .playback-controls {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .playback-controls button {
            flex: 1;
            min-width: 100px;
        }

        .frame-scrubber {
            margin: 15px 0;
        }

        .frame-scrubber input {
            width: 100%;
            height: 8px;
            background: rgba(102, 126, 234, 0.2);
            border: none;
            border-radius: 4px;
            outline: none;
        }

        .frame-scrubber input::-webkit-slider-thumb {
            appearance: none;
            width: 20px;
            height: 20px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            border-radius: 50%;
            cursor: pointer;
        }

        .frame-label {
            display: block;
            margin-bottom: 5px;
            color: #667eea;
            font-weight: 500;
        }

        @media (max-width: 1200px) {
            .main-grid {
                grid-template-columns: 1fr;
                gap: 20px;
            }

            .right-panel {
                order: -1;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üéôÔ∏è ONNX Audio to Blendshapes (Fixed)</h1>

        <!-- Model Loading Section -->
        <div class="section">
            <h3>1. Load ONNX Model</h3>
            <div class="file-input-wrapper">
                <div class="file-input" onclick="document.getElementById('modelFile').click()">
                    <span id="modelFileName">Click to select ONNX model file (.onnx)</span>
                </div>
                <input type="file" id="modelFile" accept=".onnx" onchange="handleModelFile(this)">
            </div>
            <div id="modelStatus" class="status info" style="display: none;">
                Model not loaded
            </div>
        </div>

        <!-- Audio Input Section -->
        <div class="section">
            <h3>2. Audio Input</h3>

            <!-- Recording Section -->
            <div class="recording-section">
                <h4>üéôÔ∏è Record Audio</h4>
                <div class="recording-controls">
                    <div class="status" id="recordingStatus">Ready to record</div>
                    <div class="timer" id="recordingTimer">00:00</div>

                    <div class="wave-animation" id="waveAnimation">
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                    </div>

                    <div class="controls">
                        <button id="startRecordBtn" onclick="startRecording()">Start Recording</button>
                        <button id="stopRecordBtn" onclick="stopRecording()" disabled>Stop Recording</button>
                        <button id="clearRecordBtn" onclick="clearRecording()" disabled>Clear Recording</button>
                    </div>
                </div>
            </div>

            <!-- Divider -->
            <div class="divider">
                <span>OR</span>
            </div>

            <!-- File Upload Section -->
            <div class="upload-section">
                <h4>üìÅ Upload Audio File</h4>
                <div class="file-input-wrapper">
                    <div class="file-input" onclick="document.getElementById('audioFile').click()">
                        <span id="audioFileName">Click to select audio file (.wav, .mp3, .m4a)</span>
                    </div>
                    <input type="file" id="audioFile" accept=".wav,.mp3,.m4a,.webm" onchange="handleAudioFile(this)">
                </div>
            </div>

            <!-- Audio Preview -->
            <div class="audio-preview" id="audioPreview" style="display: none;">
                <audio controls id="audioPlayer"></audio>
                <div class="audio-info" id="audioInfo"></div>
            </div>
        </div>

        <!-- Inference Section -->
        <div class="section">
            <h3>3. Run Inference</h3>
            <div>
                <label>
                    Target FPS:
                    <select id="targetFps">
                        <option value="24">24 FPS</option>
                        <option value="30" selected>30 FPS</option>
                        <option value="60">60 FPS</option>
                    </select>
                </label>
            </div>
            <br>
            <button id="runInference" onclick="runInference()" disabled>
                Run Inference
            </button>
            <div class="progress-bar" id="progressBar" style="display: none;">
                <div class="progress-fill" id="progressFill"></div>
            </div>
            <div id="inferenceStatus" class="status info" style="display: none;">
                Ready to run inference
            </div>
        </div>

        <!-- Results Section -->
        <div class="section results-section" id="resultsSection">
            <h3>4. Results</h3>
            <div class="info-grid" id="resultsInfo">
                <!-- Results will be populated here -->
            </div>
            <div>
                <a href="#" id="downloadJson" class="download-link" style="display: none;">
                    üì• Download JSON Results
                </a>
                <a href="#" id="downloadCsv" class="download-link" style="display: none;">
                    üìä Download CSV Results
                </a>
            </div>
        </div>

        <!-- 3D Visualization Section -->
        <div class="section results-section" id="visualizationSection" style="display: none;">
            <h3>5. ü¶ù 3D Raccoon Visualization</h3>

            <!-- Main Grid Layout -->
            <div class="main-grid">
                <!-- Left Panel - 3D Scene -->
                <div class="left-panel">
                    <div id="sceneContainer" class="scene-container">
                        <div class="scene-loading" id="sceneLoading">
                            <div class="spinner"></div>
                            <p>Loading Raccoon Model...</p>
                        </div>
                    </div>

                    <!-- Playback Controls -->
                    <div class="playback-controls" id="playbackControls" style="display: none;">
                        <button id="playBtn" class="btn-play">‚ñ∂Ô∏è Play</button>
                        <button id="pauseBtn" class="btn-pause" disabled>‚è∏Ô∏è Pause</button>
                        <button id="stopBtn" class="btn-stop" disabled>‚èπÔ∏è Stop</button>
                        <button id="resetBtn" class="btn-reset">ü¶ù Reset</button>
                    </div>

                    <!-- Frame Scrubber -->
                    <div class="frame-scrubber" id="frameScrubber" style="display: none;">
                        <label class="frame-label" id="frameLabel">Frame: 1 / 1</label>
                        <input type="range" id="frameSlider" min="0" max="0" value="0">
                    </div>
                </div>

                <!-- Right Panel - Blendshapes -->
                <div class="right-panel">
                    <h4>üé≠ Live Blendshapes</h4>
                    <div id="blendshapesPanel" class="blendshapes-panel">
                        <div style="text-align: center; color: #667eea; margin-top: 50px;">
                            <p>No inference results</p>
                            <p style="font-size: 12px;">Run inference to see blendshapes</p>
                        </div>
                    </div>

                    <!-- Audio Player -->
                    <div style="margin-top: 15px;">
                        <audio id="audioPlayerViz" controls style="width: 100%; display: none;"></audio>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // Import Three.js and modules
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // Global variables
        let onnxSession = null;
        let audioBuffer = null;
        let inferenceResults = null;

        // Recording variables
        let mediaRecorder = null;
        let audioChunks = [];
        let recordingStream = null;
        let recordingStartTime = 0;
        let recordingTimerInterval = null;

        // 3D Scene variables
        let scene, camera, renderer, controls;
        let gltfScene, morphTargetMeshes = [];
        let currentFrame = 0;
        let isPlaying = false;
        let currentBlendshapes = new Map();
        let modelLoaded = false;
        let animationId = null;
        let startTime = 0;
        let targetFPS = 30;

        // Blendshape names (52 blendshapes + 7 head pose values)
        const blendshapeNames = [
            '_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft',
            'browOuterUpRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft',
            'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft', 'eyeLookInRight',
            'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft',
            'eyeSquintRight', 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen',
            'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight', 'mouthFrownLeft',
            'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight',
            'mouthPressLeft', 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower',
            'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft', 'mouthSmileRight',
            'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight',
            'noseSneerLeft', 'noseSneerRight'
        ];

        // Audio processing parameters - FIXED to match PyTorch training
        const audioConfig = {
            sampleRate: 16000,
            nMels: 80,
            hopLength: 160,
            winLength: 400,
            nFFT: 512,
            fMin: 0,
            fMax: 8000,
            // Mel filter bank parameters
            melFilterMatrix: null
        };

        // Initialize mel filter bank
        function initializeMelFilterBank() {
            const nFFT = audioConfig.nFFT;
            const sampleRate = audioConfig.sampleRate;
            const nMels = audioConfig.nMels;
            const fMin = audioConfig.fMin;
            const fMax = audioConfig.fMax;

            // Convert frequencies to mel scale
            const melMin = 2595 * Math.log10(1 + fMin / 700);
            const melMax = 2595 * Math.log10(1 + fMax / 700);

            // Create mel points
            const melPoints = [];
            for (let i = 0; i <= nMels + 1; i++) {
                const mel = melMin + (melMax - melMin) * i / (nMels + 1);
                const freq = 700 * (Math.pow(10, mel / 2595) - 1);
                melPoints.push(freq);
            }

            // Convert to FFT bin numbers
            const fftFreqs = [];
            for (let i = 0; i <= nFFT / 2; i++) {
                fftFreqs.push(i * sampleRate / nFFT);
            }

            // Create filter bank
            const filterBank = [];
            for (let m = 1; m <= nMels; m++) {
                const filter = new Float32Array(nFFT / 2 + 1);

                for (let k = 0; k < filter.length; k++) {
                    const freq = fftFreqs[k];

                    if (freq < melPoints[m - 1] || freq > melPoints[m + 1]) {
                        filter[k] = 0;
                    } else if (freq <= melPoints[m]) {
                        filter[k] = (freq - melPoints[m - 1]) / (melPoints[m] - melPoints[m - 1]);
                    } else {
                        filter[k] = (melPoints[m + 1] - freq) / (melPoints[m + 1] - melPoints[m]);
                    }
                }
                filterBank.push(filter);
            }

            audioConfig.melFilterMatrix = filterBank;
        }

        // FIXED: Proper FFT implementation
        function computeFFT(signal) {
            const N = signal.length;
            const output = new Array(N);

            for (let k = 0; k < N; k++) {
                let sumReal = 0;
                let sumImag = 0;

                for (let n = 0; n < N; n++) {
                    const angle = -2 * Math.PI * k * n / N;
                    sumReal += signal[n] * Math.cos(angle);
                    sumImag += signal[n] * Math.sin(angle);
                }

                output[k] = { real: sumReal, imag: sumImag };
            }

            return output;
        }

        // FIXED: Proper STFT implementation
        function computeSTFT(audioData, winLength, hopLength, nFFT) {
            const numFrames = Math.floor((audioData.length - winLength) / hopLength) + 1;
            const stft = [];

            // Create Hanning window
            const window = new Float32Array(winLength);
            for (let i = 0; i < winLength; i++) {
                window[i] = 0.5 - 0.5 * Math.cos(2 * Math.PI * i / (winLength - 1));
            }

            for (let frame = 0; frame < numFrames; frame++) {
                const startIdx = frame * hopLength;
                const frameData = new Float32Array(nFFT);

                // Apply window and zero-pad
                for (let i = 0; i < winLength && startIdx + i < audioData.length; i++) {
                    frameData[i] = audioData[startIdx + i] * window[i];
                }

                // Compute FFT
                const fft = computeFFT(frameData);

                // Convert to magnitude spectrum
                const magnitudes = new Float32Array(nFFT / 2 + 1);
                for (let i = 0; i < magnitudes.length; i++) {
                    magnitudes[i] = Math.sqrt(fft[i].real * fft[i].real + fft[i].imag * fft[i].imag);
                }

                stft.push(magnitudes);
            }

            return stft;
        }

        // FIXED: Proper mel-spectrogram computation
        async function computeMelSpectrogram(audioData, sampleRate) {
            console.log('üîä Computing mel-spectrogram with proper implementation...');

            // Initialize mel filter bank if not done
            if (!audioConfig.melFilterMatrix) {
                initializeMelFilterBank();
            }

            const { hopLength, winLength, nFFT, nMels } = audioConfig;

            // Compute STFT
            const stft = computeSTFT(audioData, winLength, hopLength, nFFT);
            const numFrames = stft.length;

            // Apply mel filter bank
            const melSpectrogram = new Float32Array(numFrames * nMels);

            for (let frame = 0; frame < numFrames; frame++) {
                const spectrum = stft[frame];

                for (let mel = 0; mel < nMels; mel++) {
                    let melValue = 0;
                    const filter = audioConfig.melFilterMatrix[mel];

                    for (let bin = 0; bin < spectrum.length; bin++) {
                        melValue += spectrum[bin] * filter[bin];
                    }

                    // Apply log and clamp to prevent -inf
                    melSpectrogram[frame * nMels + mel] = Math.log(Math.max(melValue, 1e-10));
                }
            }

            // FIXED: Proper normalization to match PyTorch training
            const mean = melSpectrogram.reduce((sum, val) => sum + val, 0) / melSpectrogram.length;
            const variance = melSpectrogram.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / melSpectrogram.length;
            const std = Math.sqrt(variance) + 1e-8;

            for (let i = 0; i < melSpectrogram.length; i++) {
                melSpectrogram[i] = (melSpectrogram[i] - mean) / std;
            }

            return {
                data: melSpectrogram,
                frames: numFrames,
                originalDuration: audioData.length / sampleRate
            };
        }

        // Initialize the 3D scene
        async function initializeScene() {
            console.log('üöÄ Initializing 3D scene...');

            const container = document.getElementById('sceneContainer');
            const width = container.clientWidth;
            const height = container.clientHeight;

            // Scene setup
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0x1a1a1a);

            // Camera setup
            camera = new THREE.PerspectiveCamera(60, width / height, 0.1, 100);
            camera.position.set(0, 0, 4);

            // Renderer setup
            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: false });
            renderer.setSize(width, height);
            renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
            renderer.shadowMap.enabled = true;
            renderer.shadowMap.type = THREE.PCFSoftShadowMap;

            // Remove loading indicator and add renderer
            const loading = document.getElementById('sceneLoading');
            if (loading) loading.remove();
            container.appendChild(renderer.domElement);

            // Lighting setup
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.8);
            scene.add(ambientLight);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 1.0);
            directionalLight.position.set(2, 2, 3);
            directionalLight.castShadow = true;
            scene.add(directionalLight);

            const fillLight = new THREE.DirectionalLight(0xffffff, 0.4);
            fillLight.position.set(-2, 1, 2);
            scene.add(fillLight);

            const rimLight = new THREE.DirectionalLight(0x4488ff, 0.3);
            rimLight.position.set(0, 2, -2);
            scene.add(rimLight);

            // Controls setup
            controls = new OrbitControls(camera, renderer.domElement);
            controls.target.set(0, 0, 0);
            controls.enableDamping = true;
            controls.dampingFactor = 0.05;
            controls.minDistance = 2;
            controls.maxDistance = 8;
            controls.update();

            // Load the raccoon model
            await loadRaccoonModel();

            // Start render loop
            animate3D();
        }

        // Load the raccoon model
        async function loadRaccoonModel() {
            return new Promise((resolve, reject) => {
                const loader = new GLTFLoader();
                loader.load(
                    'https://storage.googleapis.com/mediapipe-tasks/face_landmarker/raccoon_head.glb',
                    (gltf) => {
                        console.log('‚úÖ Raccoon head model loaded');

                        // Remove any existing model
                        if (gltfScene) {
                            scene.remove(gltfScene);
                            morphTargetMeshes = [];
                        }

                        gltfScene = gltf.scene;
                        scene.add(gltfScene);

                        // Scale and position the model
                        const box = new THREE.Box3().setFromObject(gltfScene);
                        const size = box.getSize(new THREE.Vector3());
                        const center = box.getCenter(new THREE.Vector3());

                        gltfScene.position.set(0, 0, 0);
                        gltfScene.position.sub(center);

                        const maxDimension = Math.max(size.x, size.y, size.z);
                        const scale = 1.5 / maxDimension;
                        gltfScene.scale.setScalar(scale);
                        gltfScene.position.y += size.y * scale * 0.05;

                        // Find meshes with morph targets
                        gltfScene.traverse((object) => {
                            if (object.isMesh) {
                                const mesh = object;
                                mesh.frustumCulled = false;

                                if (mesh.morphTargetDictionary && mesh.morphTargetInfluences) {
                                    morphTargetMeshes.push(mesh);
                                    console.log('ü¶ù Added morph target mesh:', mesh.name, 'targets:', Object.keys(mesh.morphTargetDictionary).length);
                                }
                            }
                        });

                        console.log(`ü¶ù Total morph target meshes found: ${morphTargetMeshes.length}`);

                        modelLoaded = true;
                        resolve();
                    },
                    (progress) => {
                        const percent = (progress.loaded / progress.total) * 100;
                        console.log(`Loading model: ${percent.toFixed(1)}%`);
                    },
                    (error) => {
                        console.error('‚ùå Model load error:', error);
                        showStatus('inferenceStatus', 'error', 'Failed to load raccoon model');
                        reject(error);
                    }
                );
            });
        }

        // Animation loop for 3D scene
        function animate3D() {
            requestAnimationFrame(animate3D);
            if (controls) controls.update();
            if (renderer && scene && camera) renderer.render(scene, camera);
        }

        // Update raccoon with frame data
        function updateRaccoonFromFrame(frame) {
            if (!gltfScene) {
                console.warn('ü¶ù No GLTF model loaded yet');
                return;
            }

            // Apply head rotation and position if available
            if (frame.headPosition && frame.headRotation) {
                const quaternion = new THREE.Quaternion(
                    frame.headRotation.x,
                    frame.headRotation.y,
                    frame.headRotation.z,
                    frame.headRotation.w
                );
                gltfScene.quaternion.slerp(quaternion, 0.15);

                const position = new THREE.Vector3(
                    frame.headPosition.x * 0.5,
                    frame.headPosition.y * 0.5,
                    frame.headPosition.z * 0.5
                );
                gltfScene.position.lerp(position, 0.1);
            }

            // Apply blendshapes
            const blendshapes = processBlendshapes(frame.blendshapes);
            currentBlendshapes = blendshapes;
            updateBlendshapes(blendshapes);
            updateBlendshapesDisplay();
        }

        // Process blendshapes with enhancements
        function processBlendshapes(blendshapes) {
            const coefsMap = new Map();

            for (const [name, value] of Object.entries(blendshapes)) {
                let score = value;

                // Enhance certain expressions
                switch (name) {
                    case 'eyeBlinkLeft':
                    case 'eyeBlinkRight':
                        score *= 1.5;
                        break;
                    case 'browOuterUpLeft':
                    case 'browOuterUpRight':
                        score *= 1.3;
                        break;
                    case 'mouthSmileLeft':
                    case 'mouthSmileRight':
                        score *= 1.2;
                        break;
                    default:
                        score *= 1.0;
                }

                coefsMap.set(name, Math.min(score, 1.0));
            }

            return coefsMap;
        }

        // Update blendshapes on meshes
        function updateBlendshapes(blendshapes) {
            for (const mesh of morphTargetMeshes) {
                if (!mesh.morphTargetDictionary || !mesh.morphTargetInfluences) continue;

                for (const [name, value] of blendshapes) {
                    if (name in mesh.morphTargetDictionary) {
                        const index = mesh.morphTargetDictionary[name];
                        if (mesh.morphTargetInfluences[index] !== undefined) {
                            mesh.morphTargetInfluences[index] = THREE.MathUtils.lerp(
                                mesh.morphTargetInfluences[index],
                                value,
                                0.3
                            );
                        }
                    }
                }
            }
        }

        // Update blendshapes display panel
        function updateBlendshapesDisplay() {
            const panel = document.getElementById('blendshapesPanel');

            if (currentBlendshapes.size === 0) {
                panel.innerHTML = `
                    <div style="text-align: center; color: #667eea; margin-top: 50px;">
                        <p>No blendshapes data</p>
                        <p style="font-size: 12px;">Run inference to see live blendshapes</p>
                    </div>
                `;
                return;
            }

            // Sort blendshapes by value descending
            const sortedBlendshapes = Array.from(currentBlendshapes.entries())
                .sort(([, a], [, b]) => b - a);

            let html = '';
            for (const [name, value] of sortedBlendshapes) {
                const percentage = (value * 100).toFixed(1);
                const barWidth = Math.max(value * 100, 2);
                const colorClass = value > 0.1 ? 'high' : value > 0.05 ? 'medium' : 'low';

                html += `
                    <div class="blendshape-item">
                        <div style="display: flex; justify-content: space-between; width: 100%;">
                            <span class="blendshape-name">${name}</span>
                            <span class="blendshape-value">${percentage}%</span>
                        </div>
                        <div class="blendshape-bar">
                            <div class="blendshape-fill ${colorClass}" style="width: ${barWidth}%"></div>
                        </div>
                    </div>
                `;
            }
            panel.innerHTML = html;
        }

        async function handleModelFile(input) {
            const file = input.files[0];
            if (!file) return;

            document.getElementById('modelFileName').textContent = file.name;
            showStatus('modelStatus', 'info', 'Loading model...');

            try {
                const arrayBuffer = await file.arrayBuffer();
                onnxSession = await ort.InferenceSession.create(arrayBuffer);

                showStatus('modelStatus', 'success', `Model loaded successfully! Input: ${JSON.stringify(onnxSession.inputNames)}, Output: ${JSON.stringify(onnxSession.outputNames)}`);
                updateInferenceButton();
            } catch (error) {
                console.error('Error loading model:', error);
                showStatus('modelStatus', 'error', `Error loading model: ${error.message}`);
                onnxSession = null;
            }
        }

        // Recording Functions
        async function startRecording() {
            try {
                // Request microphone access
                recordingStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 44100,
                        channelCount: 2,
                        volume: 1.0
                    }
                });

                // Create MediaRecorder
                mediaRecorder = new MediaRecorder(recordingStream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                audioChunks = [];
                recordingStartTime = Date.now();

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    await processRecording();
                };

                // Start recording
                mediaRecorder.start(100); // Collect data every 100ms

                // Update UI
                updateRecordingStatus('recording', 'Recording...');
                document.getElementById('startRecordBtn').disabled = true;
                document.getElementById('stopRecordBtn').disabled = false;
                document.getElementById('clearRecordBtn').disabled = true;
                document.getElementById('waveAnimation').classList.add('active');

                // Start timer
                startRecordingTimer();

            } catch (error) {
                console.error('Error accessing microphone:', error);
                updateRecordingStatus('idle', 'Error: Could not access microphone');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();

                // Stop all tracks
                if (recordingStream) {
                    recordingStream.getTracks().forEach(track => track.stop());
                }

                // Update UI
                updateRecordingStatus('ready', 'Processing...');
                document.getElementById('startRecordBtn').disabled = false;
                document.getElementById('stopRecordBtn').disabled = true;
                document.getElementById('waveAnimation').classList.remove('active');

                // Stop timer
                stopRecordingTimer();
            }
        }

        async function processRecording() {
            try {
                // Create blob from chunks
                const webmBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });

                // Convert to WAV
                const wavBlob = await convertToWav(webmBlob);

                // Create audio URL for preview
                const audioUrl = URL.createObjectURL(wavBlob);
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                document.getElementById('audioPreview').style.display = 'block';

                // Load and decode audio for inference
                const arrayBuffer = await wavBlob.arrayBuffer();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                // Update UI
                updateRecordingStatus('ready', 'Recording ready!');
                document.getElementById('clearRecordBtn').disabled = false;

                // Show audio info
                const duration = audioBuffer.duration.toFixed(2);
                const sampleRate = audioBuffer.sampleRate;
                document.getElementById('audioInfo').innerHTML = `Duration: ${duration}s | Sample Rate: ${sampleRate}Hz | Source: Recording`;

                updateInferenceButton();

            } catch (error) {
                console.error('Error processing recording:', error);
                updateRecordingStatus('idle', 'Error processing recording');
            }
        }

        function clearRecording() {
            // Reset audio players
            const audioPlayer = document.getElementById('audioPlayer');
            const audioPlayerPreview = document.getElementById('audioPlayer');

            if (audioPlayer && audioPlayer.src) {
                URL.revokeObjectURL(audioPlayer.src);
                audioPlayer.src = '';
            }

            document.getElementById('audioPreview').style.display = 'none';

            // Reset audio buffer
            audioBuffer = null;
            inferenceResults = null;

            // Reset UI
            updateRecordingStatus('idle', 'Ready to record');
            document.getElementById('clearRecordBtn').disabled = true;
            document.getElementById('recordingTimer').textContent = '00:00';
            document.getElementById('audioInfo').innerHTML = '';

            // Hide and reset 3D visualization
            const visualizationSection = document.getElementById('visualizationSection');
            const resultsSection = document.getElementById('resultsSection');

            visualizationSection.style.display = 'none';
            resultsSection.style.display = 'none';

            // Reset 3D scene state
            resetVisualizationState();

            updateInferenceButton();
        }

        function resetVisualizationState() {
            // Stop any ongoing playback
            if (isPlaying) {
                pauseVisualizationPlayback();
            }

            // Reset playback state
            currentFrame = 0;
            isPlaying = false;
            window.playbackData = null;

            // Reset blendshapes
            currentBlendshapes.clear();

            // Reset raccoon position if model is loaded
            if (gltfScene) {
                gltfScene.position.set(0, 0, 0);
                gltfScene.rotation.set(0, 0, 0);

                // Reset all morph targets to 0
                for (const mesh of morphTargetMeshes) {
                    if (mesh.morphTargetInfluences) {
                        for (let i = 0; i < mesh.morphTargetInfluences.length; i++) {
                            mesh.morphTargetInfluences[i] = 0;
                        }
                    }
                }
            }

            // Clear blendshapes panel
            const panel = document.getElementById('blendshapesPanel');
            if (panel) {
                panel.innerHTML = `
                    <div style="text-align: center; color: #667eea; margin-top: 50px;">
                        <p>No inference results</p>
                        <p style="font-size: 12px;">Run inference to see blendshapes</p>
                    </div>
                `;
            }

            console.log('ü¶ù Visualization state reset');
        }

        function startRecordingTimer() {
            recordingTimerInterval = setInterval(() => {
                const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
                const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0');
                const seconds = (elapsed % 60).toString().padStart(2, '0');
                document.getElementById('recordingTimer').textContent = `${minutes}:${seconds}`;
            }, 1000);
        }

        function stopRecordingTimer() {
            if (recordingTimerInterval) {
                clearInterval(recordingTimerInterval);
                recordingTimerInterval = null;
            }
        }

        function updateRecordingStatus(className, text) {
            const status = document.getElementById('recordingStatus');
            status.className = `status ${className}`;
            status.textContent = text;
        }

        async function convertToWav(webmBlob) {
            // Create audio context
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();

            // Convert blob to array buffer
            const arrayBuffer = await webmBlob.arrayBuffer();

            // Decode audio data
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

            // Convert to WAV
            const wavBuffer = audioBufferToWav(audioBuffer);

            return new Blob([wavBuffer], { type: 'audio/wav' });
        }

        function audioBufferToWav(buffer) {
            const length = buffer.length;
            const numberOfChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const arrayBuffer = new ArrayBuffer(44 + length * numberOfChannels * 2);
            const view = new DataView(arrayBuffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + length * numberOfChannels * 2, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numberOfChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numberOfChannels * 2, true);
            view.setUint16(32, numberOfChannels * 2, true);
            view.setUint16(34, 16, true);
            writeString(36, 'data');
            view.setUint32(40, length * numberOfChannels * 2, true);

            // Convert float samples to 16-bit PCM
            let offset = 44;
            for (let i = 0; i < length; i++) {
                for (let channel = 0; channel < numberOfChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
                    view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                    offset += 2;
                }
            }

            return arrayBuffer;
        }

        async function handleAudioFile(input) {
            const file = input.files[0];
            if (!file) return;

            document.getElementById('audioFileName').textContent = file.name;

            try {
                // Create audio URL for preview
                const audioUrl = URL.createObjectURL(file);
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                document.getElementById('audioPreview').style.display = 'block';

                // Load and decode audio
                const arrayBuffer = await file.arrayBuffer();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                // Show audio info
                const duration = audioBuffer.duration.toFixed(2);
                const sampleRate = audioBuffer.sampleRate;
                document.getElementById('audioInfo').innerHTML = `Duration: ${duration}s | Sample Rate: ${sampleRate}Hz | Source: Upload`;

                // Clear recording status
                updateRecordingStatus('idle', 'Ready to record');
                document.getElementById('clearRecordBtn').disabled = true;
                document.getElementById('recordingTimer').textContent = '00:00';

                updateInferenceButton();
            } catch (error) {
                console.error('Error loading audio:', error);
                showStatus('inferenceStatus', 'error', `Error loading audio: ${error.message}`);
                audioBuffer = null;
            }
        }

        function updateInferenceButton() {
            const runButton = document.getElementById('runInference');
            runButton.disabled = !(onnxSession && audioBuffer);
        }

        // FIXED: Main inference function with proper audio preprocessing
        async function runInference() {
            if (!onnxSession || !audioBuffer) {
                showStatus('inferenceStatus', 'error', 'Please load both model and audio file');
                return;
            }

            const runButton = document.getElementById('runInference');
            runButton.disabled = true;
            runButton.innerHTML = '<span class="spinner"></span>Running inference...';

            showStatus('inferenceStatus', 'info', 'Processing audio...');
            document.getElementById('progressBar').style.display = 'block';

            try {
                // FIXED: Extract mel-spectrogram features with proper implementation
                updateProgress(20);
                showStatus('inferenceStatus', 'info', 'Extracting mel-spectrogram features...');

                const melFeatures = await extractMelFeatures(audioBuffer);
                console.log('üîä Mel features shape:', [1, melFeatures.frames, audioConfig.nMels]);

                updateProgress(40);
                showStatus('inferenceStatus', 'info', 'Running ONNX inference...');

                // FIXED: Create input tensor with correct shape [B, T, F]
                const inputTensor = new ort.Tensor('float32', melFeatures.data, [1, melFeatures.frames, audioConfig.nMels]);
                const feeds = {};
                feeds[onnxSession.inputNames[0]] = inputTensor;

                console.log('ü§ñ Running ONNX inference with input shape:', inputTensor.dims);
                const results = await onnxSession.run(feeds);
                const outputData = results[onnxSession.outputNames[0]].data;
                const outputShape = results[onnxSession.outputNames[0]].dims;

                console.log('üéØ ONNX output shape:', outputShape);
                console.log('üéØ Output data length:', outputData.length);

                updateProgress(80);
                showStatus('inferenceStatus', 'info', 'Processing results...');

                // Process results
                const targetFps = parseInt(document.getElementById('targetFps').value);
                inferenceResults = processInferenceResults(outputData, outputShape, targetFps, melFeatures.originalDuration);

                updateProgress(100);
                showStatus('inferenceStatus', 'success', `Inference completed! Generated ${inferenceResults.frameCount} frames`);

                // Show results
                displayResults(inferenceResults);

            } catch (error) {
                console.error('Inference error:', error);
                showStatus('inferenceStatus', 'error', `Inference failed: ${error.message}`);
            } finally {
                runButton.disabled = false;
                runButton.innerHTML = 'Run Inference';
                setTimeout(() => {
                    document.getElementById('progressBar').style.display = 'none';
                }, 2000);
            }
        }

        // FIXED: Proper audio feature extraction
        async function extractMelFeatures(audioBuffer) {
            console.log('üîä Extracting mel features...');

            // FIXED: Resample audio to target sample rate properly
            const targetSampleRate = audioConfig.sampleRate;
            let audioData;

            if (audioBuffer.sampleRate !== targetSampleRate) {
                console.log(`üîÑ Resampling from ${audioBuffer.sampleRate}Hz to ${targetSampleRate}Hz`);

                // Proper resampling using Web Audio API
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const offlineContext = new OfflineAudioContext(1,
                    Math.floor(audioBuffer.duration * targetSampleRate),
                    targetSampleRate);

                const source = offlineContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(offlineContext.destination);
                source.start();

                const resampledBuffer = await offlineContext.startRendering();
                audioData = resampledBuffer.getChannelData(0);
            } else {
                audioData = audioBuffer.getChannelData(0);
            }

            console.log('üîä Audio data length:', audioData.length, 'Sample rate:', targetSampleRate);

            // Compute mel-spectrogram using the fixed implementation
            const melFeatures = await computeMelSpectrogram(audioData, targetSampleRate);

            console.log('‚úÖ Mel features extracted:', melFeatures.frames, 'frames,', audioConfig.nMels, 'mels');

            return melFeatures;
        }

        // FIXED: Process inference results with correct shape handling
        function processInferenceResults(outputData, outputShape, targetFps, originalDuration) {
            console.log('üéØ Processing inference results...');
            console.log('üìä Output shape:', outputShape);
            console.log('üìä Output data length:', outputData.length);

            // Handle different possible output shapes
            let numFrames, outputFeatures;

            if (outputShape.length === 3) {
                // [B, T, D] format
                numFrames = outputShape[1];
                outputFeatures = outputShape[2];
            } else if (outputShape.length === 2) {
                // [T, D] format
                numFrames = outputShape[0];
                outputFeatures = outputShape[1];
            } else {
                throw new Error(`Unexpected output shape: ${outputShape}`);
            }

            console.log(`üìä Frames: ${numFrames}, Features: ${outputFeatures}`);

            if (outputFeatures !== 59) {
                console.warn(`‚ö†Ô∏è Expected 59 output features, got ${outputFeatures}`);
            }

            // No downsampling for now - use original timing
            const finalFrames = numFrames;
            const finalData = outputData;

            // Create frame data
            const frames = [];
            for (let i = 0; i < finalFrames; i++) {
                const blendshapes = {};

                // Extract blendshapes (first 52 features)
                for (let j = 0; j < Math.min(52, outputFeatures); j++) {
                    const value = finalData[i * outputFeatures + j];
                    // Apply sigmoid activation to ensure [0,1] range
                    blendshapes[blendshapeNames[j]] = 1 / (1 + Math.exp(-value));
                }

                // Extract head pose (last 7 features if available)
                let headPosition = { x: 0, y: 0, z: 0 };
                let headRotation = { w: 1, x: 0, y: 0, z: 0 };

                if (outputFeatures >= 59) {
                    headPosition = {
                        x: finalData[i * outputFeatures + 52],
                        y: finalData[i * outputFeatures + 53],
                        z: finalData[i * outputFeatures + 54]
                    };
                    headRotation = {
                        w: finalData[i * outputFeatures + 55],
                        x: finalData[i * outputFeatures + 56],
                        y: finalData[i * outputFeatures + 57],
                        z: finalData[i * outputFeatures + 58]
                    };
                }

                const frame = {
                    frame_index: i,
                    timestamp: Math.round((i / targetFps) * 1000), // milliseconds
                    blendshapes: blendshapes,
                    headPosition: headPosition,
                    headRotation: headRotation,
                    has_face: true
                };
                frames.push(frame);
            }

            return {
                sessionInfo: {
                    sessionId: `web_inference_${Date.now()}`,
                    targetFPS: targetFps,
                    audioPath: "uploaded_audio"
                },
                frameCount: finalFrames,
                failedFrames: 0,
                failureRate: 0.0,
                duration: finalFrames / targetFps,
                frames: frames
            };
        }

        function displayResults(results) {
            const resultsSection = document.getElementById('resultsSection');
            const resultsInfo = document.getElementById('resultsInfo');
            const visualizationSection = document.getElementById('visualizationSection');

            resultsInfo.innerHTML = `
                <div class="info-card">
                    <div class="value">${results.frameCount}</div>
                    <div class="label">Total Frames</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.sessionInfo.targetFPS}</div>
                    <div class="label">FPS</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.duration.toFixed(2)}s</div>
                    <div class="label">Duration</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.failureRate.toFixed(1)}%</div>
                    <div class="label">Failure Rate</div>
                </div>
            `;

            // Setup download links
            setupDownloads(results);

            // Show results section
            resultsSection.style.display = 'block';

            // Initialize and show 3D visualization
            visualizationSection.style.display = 'block';

            // Only initialize scene if not already initialized
            if (!modelLoaded || !scene) {
                console.log('ü¶ù Initializing 3D scene for the first time...');
                initializeScene().then(() => {
                    setupPlaybackControls(results);

                    // Show the first frame
                    if (results.frames.length > 0) {
                        updateRaccoonFromFrame(results.frames[0]);
                    }
                });
            } else {
                console.log('ü¶ù Using existing 3D scene...');
                // Scene already exists, just setup playback with new data
                resetVisualizationState(); // Reset to clean state
                setupPlaybackControls(results);

                // Show the first frame
                if (results.frames.length > 0) {
                    updateRaccoonFromFrame(results.frames[0]);
                }
            }
        }

        // Setup playback controls for the inference results
        function setupPlaybackControls(results) {
            const playbackControls = document.getElementById('playbackControls');
            const frameScrubber = document.getElementById('frameScrubber');
            const frameSlider = document.getElementById('frameSlider');
            const frameLabel = document.getElementById('frameLabel');
            const audioPlayerViz = document.getElementById('audioPlayerViz');

            // Store results globally for playback
            window.playbackData = results;
            targetFPS = results.sessionInfo.targetFPS;

            // Setup frame slider
            frameSlider.max = results.frameCount - 1;
            frameSlider.value = 0;
            frameLabel.textContent = `Frame: 1 / ${results.frameCount}`;

            // Setup audio if available
            if (audioBuffer) {
                const previewPlayer = document.getElementById('audioPlayer');
                if (previewPlayer && previewPlayer.src) {
                    audioPlayerViz.src = previewPlayer.src;
                    audioPlayerViz.style.display = 'block';
                }
            }

            // Show controls
            playbackControls.style.display = 'flex';
            frameScrubber.style.display = 'block';

            // Add event listeners
            setupPlaybackEventListeners();
        }

        // Setup event listeners for playback controls
        function setupPlaybackEventListeners() {
            const playBtn = document.getElementById('playBtn');
            const pauseBtn = document.getElementById('pauseBtn');
            const stopBtn = document.getElementById('stopBtn');
            const resetBtn = document.getElementById('resetBtn');
            const frameSlider = document.getElementById('frameSlider');

            // Remove existing listeners
            playBtn.replaceWith(playBtn.cloneNode(true));
            pauseBtn.replaceWith(pauseBtn.cloneNode(true));
            stopBtn.replaceWith(stopBtn.cloneNode(true));
            resetBtn.replaceWith(resetBtn.cloneNode(true));
            frameSlider.replaceWith(frameSlider.cloneNode(true));

            // Get fresh references
            const newPlayBtn = document.getElementById('playBtn');
            const newPauseBtn = document.getElementById('pauseBtn');
            const newStopBtn = document.getElementById('stopBtn');
            const newResetBtn = document.getElementById('resetBtn');
            const newFrameSlider = document.getElementById('frameSlider');

            newPlayBtn.addEventListener('click', startVisualizationPlayback);
            newPauseBtn.addEventListener('click', pauseVisualizationPlayback);
            newStopBtn.addEventListener('click', stopVisualizationPlayback);
            newResetBtn.addEventListener('click', resetRaccoonPosition);
            newFrameSlider.addEventListener('input', (e) => {
                seekToFrame(parseInt(e.target.value));
            });
        }

        // Playback control functions
        function startVisualizationPlayback() {
            if (!window.playbackData || isPlaying) return;

            console.log('üé¨ Starting visualization playback');
            isPlaying = true;
            startTime = performance.now() - (currentFrame / targetFPS * 1000);

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz.src) {
                audioPlayerViz.currentTime = currentFrame / targetFPS;
                audioPlayerViz.play().catch(error => console.warn('Audio playback failed:', error));
            }

            visualizationAnimate();
            updatePlaybackButtons();
        }

        function pauseVisualizationPlayback() {
            isPlaying = false;
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz) {
                audioPlayerViz.pause();
            }

            updatePlaybackButtons();
        }

        function stopVisualizationPlayback() {
            pauseVisualizationPlayback();
            currentFrame = 0;

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz) {
                audioPlayerViz.pause();
                audioPlayerViz.currentTime = 0;
            }

            if (window.playbackData && window.playbackData.frames.length > 0) {
                updateRaccoonFromFrame(window.playbackData.frames[0]);
                updateFrameDisplay();
            }
        }

        function seekToFrame(frameIndex) {
            if (!window.playbackData) return;

            currentFrame = Math.max(0, Math.min(frameIndex, window.playbackData.frameCount - 1));
            const frame = window.playbackData.frames[currentFrame];
            updateRaccoonFromFrame(frame);
            updateFrameDisplay();

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz && audioPlayerViz.src) {
                const frameTime = currentFrame / targetFPS;
                audioPlayerViz.currentTime = frameTime;
            }
        }

        function resetRaccoonPosition() {
            if (gltfScene) {
                gltfScene.position.set(0, 0, 0);
                gltfScene.rotation.set(0, 0, 0);
                console.log('ü¶ù Raccoon position reset');
            }
        }

        // Animation loop for playback
        function visualizationAnimate() {
            if (!isPlaying) return;

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            let elapsedTime;

            if (audioPlayerViz && audioPlayerViz.src && !audioPlayerViz.paused) {
                elapsedTime = audioPlayerViz.currentTime;
            } else {
                const currentTime = performance.now();
                elapsedTime = (currentTime - startTime) / 1000;
            }

            const frameInterval = 1 / targetFPS;
            const expectedFrameIndex = Math.floor(elapsedTime / frameInterval);

            if (expectedFrameIndex >= window.playbackData.frameCount) {
                // Playback complete
                pauseVisualizationPlayback();
                return;
            }

            if (expectedFrameIndex !== currentFrame && expectedFrameIndex < window.playbackData.frameCount) {
                currentFrame = expectedFrameIndex;
                const frame = window.playbackData.frames[currentFrame];
                updateRaccoonFromFrame(frame);
                updateFrameDisplay();
            }

            animationId = requestAnimationFrame(visualizationAnimate);
        }

        // Update frame display
        function updateFrameDisplay() {
            if (window.playbackData) {
                const frameLabel = document.getElementById('frameLabel');
                const frameSlider = document.getElementById('frameSlider');

                frameLabel.textContent = `Frame: ${currentFrame + 1} / ${window.playbackData.frameCount}`;
                frameSlider.value = currentFrame;
            }
        }

        // Update playback button states
        function updatePlaybackButtons() {
            const playBtn = document.getElementById('playBtn');
            const pauseBtn = document.getElementById('pauseBtn');
            const stopBtn = document.getElementById('stopBtn');
            const frameSlider = document.getElementById('frameSlider');

            if (playBtn) playBtn.disabled = !window.playbackData || !modelLoaded || isPlaying;
            if (pauseBtn) pauseBtn.disabled = !isPlaying;
            if (stopBtn) stopBtn.disabled = !window.playbackData;
            if (frameSlider) frameSlider.disabled = isPlaying;
        }

        function setupDownloads(results) {
            const downloadJson = document.getElementById('downloadJson');
            const downloadCsv = document.getElementById('downloadCsv');

            // JSON download
            const jsonBlob = new Blob([JSON.stringify(results, null, 2)], { type: 'application/json' });
            const jsonUrl = URL.createObjectURL(jsonBlob);
            downloadJson.href = jsonUrl;
            downloadJson.download = `inference_results_${Date.now()}.json`;
            downloadJson.style.display = 'inline-block';

            // CSV download
            const csvContent = generateCSV(results);
            const csvBlob = new Blob([csvContent], { type: 'text/csv' });
            const csvUrl = URL.createObjectURL(csvBlob);
            downloadCsv.href = csvUrl;
            downloadCsv.download = `inference_results_${Date.now()}.csv`;
            downloadCsv.style.display = 'inline-block';
        }

        function generateCSV(results) {
            const headers = ['frame_index', 'timestamp', ...blendshapeNames, 'head_pos_x', 'head_pos_y', 'head_pos_z', 'head_rot_w', 'head_rot_x', 'head_rot_y', 'head_rot_z'];
            let csv = headers.join(',') + '\n';

            results.frames.forEach(frame => {
                const row = [
                    frame.frame_index,
                    frame.timestamp,
                    ...blendshapeNames.map(name => frame.blendshapes[name]),
                    frame.headPosition.x,
                    frame.headPosition.y,
                    frame.headPosition.z,
                    frame.headRotation.w,
                    frame.headRotation.x,
                    frame.headRotation.y,
                    frame.headRotation.z
                ];
                csv += row.join(',') + '\n';
            });

            return csv;
        }

        function showStatus(elementId, type, message) {
            const element = document.getElementById(elementId);
            element.className = `status ${type}`;
            element.textContent = message;
            element.style.display = 'block';
        }

        function updateProgress(percentage) {
            const progressFill = document.getElementById('progressFill');
            progressFill.style.width = percentage + '%';
        }

        // Handle window resize for 3D scene
        window.addEventListener('resize', function () {
            if (renderer && camera) {
                const container = document.getElementById('sceneContainer');
                const width = container.clientWidth;
                const height = container.clientHeight;

                camera.aspect = width / height;
                camera.updateProjectionMatrix();
                renderer.setSize(width, height);
            }
        });

        // Initialize ONNX runtime
        ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/';

        // Make functions globally accessible for HTML onclick handlers
        window.startRecording = startRecording;
        window.stopRecording = stopRecording;
        window.clearRecording = clearRecording;
        window.handleModelFile = handleModelFile;
        window.handleAudioFile = handleAudioFile;
        window.runInference = runInference;
    </script>
</body>

</html>