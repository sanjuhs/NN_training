<!DOCTYPE html>
<html lang="en">

<head>
    <title>ONNX Audio to Blendshapes Inference</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/ort.min.js"></script>
    <!-- Import map to resolve Three.js modules -->
    <script type="importmap">
    {
        "imports": {
            "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
        }
    }
    </script>
    <script src="https://cdn.jsdelivr.net/npm/fft.js@0.3.0/dist/fft.min.js"></script>

    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(10px);
        }

        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 30px;
            font-size: 2.5em;
            font-weight: 300;
        }

        .section {
            margin: 30px 0;
            padding: 20px;
            border-radius: 15px;
            background: rgba(102, 126, 234, 0.05);
            border: 1px solid rgba(102, 126, 234, 0.1);
        }

        .section h3 {
            margin-top: 0;
            color: #667eea;
            font-size: 1.4em;
        }

        .file-input-wrapper {
            position: relative;
            display: inline-block;
            width: 100%;
        }

        .file-input {
            width: 100%;
            padding: 15px;
            border: 2px dashed #667eea;
            border-radius: 10px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            background: white;
        }

        .file-input:hover {
            border-color: #764ba2;
            background: rgba(102, 126, 234, 0.05);
        }

        input[type="file"] {
            display: none;
        }

        button {
            background: linear-gradient(45deg, #667eea, #764ba2);
            border: none;
            color: white;
            padding: 15px 30px;
            margin: 10px 5px;
            border-radius: 50px;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
        }

        button:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: #f0f0f0;
            border-radius: 4px;
            overflow: hidden;
            margin: 20px 0;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(45deg, #667eea, #764ba2);
            width: 0%;
            transition: width 0.3s ease;
        }

        .status {
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
            font-weight: 500;
        }

        .status.info {
            background: rgba(52, 152, 219, 0.1);
            color: #3498db;
            border: 2px solid rgba(52, 152, 219, 0.3);
        }

        .status.success {
            background: rgba(46, 213, 115, 0.1);
            color: #2ed573;
            border: 2px solid rgba(46, 213, 115, 0.3);
        }

        .status.error {
            background: rgba(255, 87, 87, 0.1);
            color: #ff5757;
            border: 2px solid rgba(255, 87, 87, 0.3);
        }

        .audio-preview {
            margin: 20px 0;
            width: 100%;
        }

        audio {
            width: 100%;
            border-radius: 10px;
        }

        .results-section {
            display: none;
            margin-top: 30px;
        }

        .download-link {
            display: inline-block;
            background: linear-gradient(45deg, #2ed573, #17c0eb);
            color: white;
            text-decoration: none;
            padding: 15px 30px;
            border-radius: 50px;
            font-weight: 500;
            margin: 10px 5px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(46, 213, 115, 0.3);
        }

        .download-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(46, 213, 115, 0.4);
        }

        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .info-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            text-align: center;
        }

        .info-card .value {
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }

        .info-card .label {
            color: #666;
            margin-top: 5px;
        }

        .spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: #fff;
            animation: spin 1s ease-in-out infinite;
            margin-right: 10px;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        /* Recording-specific styles */
        .recording-section,
        .upload-section {
            background: rgba(255, 255, 255, 0.5);
            border-radius: 15px;
            padding: 20px;
            margin: 15px 0;
        }

        .recording-section h4,
        .upload-section h4 {
            margin-top: 0;
            color: #667eea;
            font-size: 1.2em;
            text-align: center;
        }

        .recording-controls {
            text-align: center;
        }

        .timer {
            font-size: 24px;
            font-weight: bold;
            margin: 20px 0;
            color: #333;
            font-family: 'Courier New', monospace;
        }

        .wave-animation {
            display: none;
            align-items: center;
            justify-content: center;
            margin: 20px 0;
        }

        .wave-animation.active {
            display: flex;
        }

        .wave {
            width: 4px;
            height: 20px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            margin: 0 2px;
            border-radius: 2px;
            animation: wave 1s infinite ease-in-out;
        }

        .wave:nth-child(2) {
            animation-delay: 0.1s;
        }

        .wave:nth-child(3) {
            animation-delay: 0.2s;
        }

        .wave:nth-child(4) {
            animation-delay: 0.3s;
        }

        .wave:nth-child(5) {
            animation-delay: 0.4s;
        }

        @keyframes wave {

            0%,
            40%,
            100% {
                transform: scaleY(0.4);
            }

            20% {
                transform: scaleY(1);
            }
        }

        .divider {
            text-align: center;
            margin: 30px 0;
            position: relative;
        }

        .divider::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 0;
            right: 0;
            height: 1px;
            background: linear-gradient(90deg, transparent, #667eea, transparent);
        }

        .divider span {
            background: rgba(255, 255, 255, 0.95);
            padding: 0 20px;
            color: #667eea;
            font-weight: 600;
        }

        .controls {
            margin: 20px 0;
            text-align: center;
        }

        .status.recording {
            background: rgba(255, 87, 87, 0.1);
            color: #ff5757;
            border: 2px solid rgba(255, 87, 87, 0.3);
        }

        .status.ready {
            background: rgba(46, 213, 115, 0.1);
            color: #2ed573;
            border: 2px solid rgba(46, 213, 115, 0.3);
        }

        .status.idle {
            background: rgba(116, 125, 136, 0.1);
            color: #747d8c;
            border: 2px solid rgba(116, 125, 136, 0.3);
        }

        .audio-info {
            margin-top: 10px;
            padding: 10px;
            background: rgba(102, 126, 234, 0.1);
            border-radius: 8px;
            font-size: 14px;
            color: #667eea;
        }

        /* 3D Scene Styles */
        .scene-container {
            height: 400px;
            background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
            border: 2px solid rgba(102, 126, 234, 0.3);
            border-radius: 15px;
            position: relative;
            overflow: hidden;
            margin: 20px 0;
        }

        .scene-loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            color: #667eea;
        }

        .scene-loading .spinner {
            border: 3px solid rgba(102, 126, 234, 0.3);
            border-top: 3px solid #667eea;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto 15px;
        }

        .main-grid {
            display: grid;
            grid-template-columns: 1fr 350px;
            gap: 30px;
            margin-top: 20px;
        }

        .left-panel {
            display: flex;
            flex-direction: column;
        }

        .right-panel {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            padding: 20px;
            height: fit-content;
        }

        .blendshapes-panel {
            background: rgba(102, 126, 234, 0.05);
            border-radius: 10px;
            padding: 15px;
            height: 350px;
            overflow-y: auto;
            border: 1px solid rgba(102, 126, 234, 0.2);
        }

        .blendshape-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
            font-size: 12px;
        }

        .blendshape-name {
            flex: 1;
            margin-right: 10px;
            color: #667eea;
            font-weight: 500;
        }

        .blendshape-value {
            color: #333;
            font-family: 'Courier New', monospace;
            font-weight: bold;
            min-width: 45px;
            text-align: right;
        }

        .blendshape-bar {
            width: 100%;
            height: 4px;
            background: rgba(102, 126, 234, 0.2);
            border-radius: 2px;
            overflow: hidden;
            margin-top: 2px;
        }

        .blendshape-fill {
            height: 100%;
            transition: width 0.1s ease;
            border-radius: 2px;
        }

        .blendshape-fill.high {
            background: linear-gradient(45deg, #22c55e, #16a34a);
        }

        .blendshape-fill.medium {
            background: linear-gradient(45deg, #eab308, #d97706);
        }

        .blendshape-fill.low {
            background: linear-gradient(45deg, #667eea, #764ba2);
        }

        .playback-controls {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .playback-controls button {
            flex: 1;
            min-width: 100px;
        }

        .frame-scrubber {
            margin: 15px 0;
        }

        .frame-scrubber input {
            width: 100%;
            height: 8px;
            background: rgba(102, 126, 234, 0.2);
            border: none;
            border-radius: 4px;
            outline: none;
        }

        .frame-scrubber input::-webkit-slider-thumb {
            appearance: none;
            width: 20px;
            height: 20px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            border-radius: 50%;
            cursor: pointer;
        }

        .frame-label {
            display: block;
            margin-bottom: 5px;
            color: #667eea;
            font-weight: 500;
        }

        @media (max-width: 1200px) {
            .main-grid {
                grid-template-columns: 1fr;
                gap: 20px;
            }

            .right-panel {
                order: -1;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>🎙️ ONNX Audio to Blendshapes</h1>

        <!-- Model Loading Section -->
        <div class="section">
            <h3>1. Load ONNX Model</h3>
            <div class="file-input-wrapper">
                <div class="file-input" onclick="document.getElementById('modelFile').click()">
                    <span id="modelFileName">Click to select ONNX model file (.onnx)</span>
                </div>
                <input type="file" id="modelFile" accept=".onnx" onchange="handleModelFile(this)">
            </div>
            <div id="modelStatus" class="status info" style="display: none;">
                Model not loaded
            </div>
        </div>

        <!-- Audio Input Section -->
        <div class="section">
            <h3>2. Audio Input</h3>

            <!-- Recording Section -->
            <div class="recording-section">
                <h4>🎙️ Record Audio</h4>
                <div class="recording-controls">
                    <div class="status" id="recordingStatus">Ready to record</div>
                    <div class="timer" id="recordingTimer">00:00</div>

                    <div class="wave-animation" id="waveAnimation">
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                    </div>

                    <div class="controls">
                        <button id="startRecordBtn" onclick="startRecording()">Start Recording</button>
                        <button id="stopRecordBtn" onclick="stopRecording()" disabled>Stop Recording</button>
                        <button id="clearRecordBtn" onclick="clearRecording()" disabled>Clear Recording</button>
                    </div>
                </div>
            </div>

            <!-- Divider -->
            <div class="divider">
                <span>OR</span>
            </div>

            <!-- File Upload Section -->
            <div class="upload-section">
                <h4>📁 Upload Audio File</h4>
                <div class="file-input-wrapper">
                    <div class="file-input" onclick="document.getElementById('audioFile').click()">
                        <span id="audioFileName">Click to select audio file (.wav, .mp3, .m4a)</span>
                    </div>
                    <input type="file" id="audioFile" accept=".wav,.mp3,.m4a,.webm" onchange="handleAudioFile(this)">
                </div>
            </div>

            <!-- Audio Preview -->
            <div class="audio-preview" id="audioPreview" style="display: none;">
                <audio controls id="audioPlayer"></audio>
                <div class="audio-info" id="audioInfo"></div>
                <a href="#" id="downloadAudioLink" class="download-link" style="display: none;">🎧 Download Audio</a>
            </div>
        </div>

        <!-- Inference Section -->
        <div class="section">
            <h3>3. Run Inference</h3>
            <div>
                <label>
                    Target FPS:
                    <select id="targetFps">
                        <option value="24">24 FPS</option>
                        <option value="30" selected>30 FPS</option>
                        <option value="60">60 FPS</option>
                    </select>
                </label>
            </div>
            <br>
            <button id="runInference" onclick="runInference()" disabled>
                Run Inference
            </button>
            <div class="progress-bar" id="progressBar" style="display: none;">
                <div class="progress-fill" id="progressFill"></div>
            </div>
            <div id="inferenceStatus" class="status info" style="display: none;">
                Ready to run inference
            </div>
        </div>

        <!-- Results Section -->
        <div class="section results-section" id="resultsSection">
            <h3>4. Results</h3>
            <div class="info-grid" id="resultsInfo">
                <!-- Results will be populated here -->
            </div>
            <div>
                <a href="#" id="downloadJson" class="download-link" style="display: none;">
                    📥 Download JSON Results
                </a>
                <a href="#" id="downloadCsv" class="download-link" style="display: none;">
                    📊 Download CSV Results
                </a>
            </div>
        </div>

        <!-- 3D Visualization Section -->
        <div class="section results-section" id="visualizationSection" style="display: none;">
            <h3>5. 🦝 3D Raccoon Visualization</h3>

            <!-- Main Grid Layout -->
            <div class="main-grid">
                <!-- Left Panel - 3D Scene -->
                <div class="left-panel">
                    <div id="sceneContainer" class="scene-container">
                        <div class="scene-loading" id="sceneLoading">
                            <div class="spinner"></div>
                            <p>Loading Raccoon Model...</p>
                        </div>
                    </div>

                    <!-- Playback Controls -->
                    <div class="playback-controls" id="playbackControls" style="display: none;">
                        <button id="playBtn" class="btn-play">▶️ Play</button>
                        <button id="pauseBtn" class="btn-pause" disabled>⏸️ Pause</button>
                        <button id="stopBtn" class="btn-stop" disabled>⏹️ Stop</button>
                        <button id="resetBtn" class="btn-reset">🦝 Reset</button>
                    </div>

                    <!-- Frame Scrubber -->
                    <div class="frame-scrubber" id="frameScrubber" style="display: none;">
                        <label class="frame-label" id="frameLabel">Frame: 1 / 1</label>
                        <input type="range" id="frameSlider" min="0" max="0" value="0">
                    </div>
                </div>

                <!-- Right Panel - Blendshapes -->
                <div class="right-panel">
                    <h4>🎭 Live Blendshapes</h4>
                    <div id="blendshapesPanel" class="blendshapes-panel">
                        <div style="text-align: center; color: #667eea; margin-top: 50px;">
                            <p>No inference results</p>
                            <p style="font-size: 12px;">Run inference to see blendshapes</p>
                        </div>
                    </div>

                    <!-- Audio Player -->
                    <div style="margin-top: 15px;">
                        <audio id="audioPlayerViz" controls style="width: 100%; display: none;"></audio>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">

        // FFT fallback (in case CDN fails). Provides minimal compatible API with fft.js used below.
        class FallbackFFT {
            constructor(size) {
                if ((size & (size - 1)) !== 0) {
                    throw new Error('FFT size must be power of two');
                }
                this.size = size;
                const half = size >>> 1;
                this.cosTable = new Float32Array(half);
                this.sinTable = new Float32Array(half);
                for (let i = 0; i < half; i++) {
                    const angle = -2 * Math.PI * i / size;
                    this.cosTable[i] = Math.cos(angle);
                    this.sinTable[i] = Math.sin(angle);
                }
                this.bitReversed = FallbackFFT._bitReverseIndices(size);
            }
            createComplexArray() {
                return new Float32Array(this.size * 2);
            }
            // Computes complex FFT of real input (imag parts assumed 0). Output is interleaved complex.
            realTransform(out, input) {
                const n = this.size;
                const real = new Float32Array(n);
                const imag = new Float32Array(n);
                const len = Math.min(input.length, n);
                for (let i = 0; i < len; i++) real[i] = input[i];

                // Bit-reversal permutation
                for (let i = 0; i < n; i++) {
                    const j = this.bitReversed[i];
                    if (j > i) {
                        let tr = real[i]; real[i] = real[j]; real[j] = tr;
                        let ti = imag[i]; imag[i] = imag[j]; imag[j] = ti;
                    }
                }

                // Iterative Cooley–Tukey decimation-in-time FFT
                for (let step = 1; step < n; step <<= 1) {
                    const jump = step << 1;
                    const tableStep = n / jump;
                    for (let i = 0; i < n; i += jump) {
                        for (let j = 0; j < step; j++) {
                            const k = (j * tableStep) | 0;
                            const wr = this.cosTable[k];
                            const wi = this.sinTable[k];
                            const ir = real[i + j + step];
                            const ii = imag[i + j + step];
                            const tr = wr * ir - wi * ii;
                            const ti = wr * ii + wi * ir;
                            real[i + j + step] = real[i + j] - tr;
                            imag[i + j + step] = imag[i + j] - ti;
                            real[i + j] += tr;
                            imag[i + j] += ti;
                        }
                    }
                }

                for (let i = 0; i < n; i++) {
                    out[2 * i] = real[i];
                    out[2 * i + 1] = imag[i];
                }
            }
            // No-op: spectrum is already complete for our purposes
            completeSpectrum(_) { }
            static _bitReverseIndices(n) {
                const result = new Uint32Array(n);
                const bits = Math.log2(n) | 0;
                for (let i = 0; i < n; i++) {
                    let x = i, y = 0;
                    for (let b = 0; b < bits; b++) { y = (y << 1) | (x & 1); x >>= 1; }
                    result[i] = y >>> 0;
                }
                return result;
            }
        }

        function getFFTClass() {
            return window.FFT || globalThis.FFT || FallbackFFT;
        }


        // Lazy-load Three.js modules when visualization is initialized
        let THREE; // Namespace for three.js
        let GLTFLoader; // Class
        let OrbitControls; // Class

        // Global variables
        let onnxSession = null;
        let audioBuffer = null;
        let inferenceResults = null;
        let latestAudioBlob = null;
        let latestAudioFilename = null;
        let currentAudioObjectUrl = null;

        // Recording variables
        let mediaRecorder = null;
        let audioChunks = [];
        let recordingStream = null;
        let recordingStartTime = 0;
        let recordingTimerInterval = null;

        // 3D Scene variables
        let scene, camera, renderer, controls;
        let gltfScene, morphTargetMeshes = [];
        let currentFrame = 0;
        let isPlaying = false;
        let currentBlendshapes = new Map();
        let modelLoaded = false;
        let animationId = null;
        let startTime = 0;
        let targetFPS = 30;

        // Blendshape names (52 blendshapes + 7 head pose values)
        const blendshapeNames = [
            '_neutral', 'browDownLeft', 'browDownRight', 'browInnerUp', 'browOuterUpLeft',
            'browOuterUpRight', 'cheekPuff', 'cheekSquintLeft', 'cheekSquintRight', 'eyeBlinkLeft',
            'eyeBlinkRight', 'eyeLookDownLeft', 'eyeLookDownRight', 'eyeLookInLeft', 'eyeLookInRight',
            'eyeLookOutLeft', 'eyeLookOutRight', 'eyeLookUpLeft', 'eyeLookUpRight', 'eyeSquintLeft',
            'eyeSquintRight', 'eyeWideLeft', 'eyeWideRight', 'jawForward', 'jawLeft', 'jawOpen',
            'jawRight', 'mouthClose', 'mouthDimpleLeft', 'mouthDimpleRight', 'mouthFrownLeft',
            'mouthFrownRight', 'mouthFunnel', 'mouthLeft', 'mouthLowerDownLeft', 'mouthLowerDownRight',
            'mouthPressLeft', 'mouthPressRight', 'mouthPucker', 'mouthRight', 'mouthRollLower',
            'mouthRollUpper', 'mouthShrugLower', 'mouthShrugUpper', 'mouthSmileLeft', 'mouthSmileRight',
            'mouthStretchLeft', 'mouthStretchRight', 'mouthUpperUpLeft', 'mouthUpperUpRight',
            'noseSneerLeft', 'noseSneerRight'
        ];

        // Audio processing parameters
        const audioConfig = {
            sampleRate: 16000,
            nMels: 80,
            hopLength: 160,
            winLength: 400,
            nFFT: 512
        };

        // Initialize the 3D scene
        async function initializeScene() {
            console.log('🚀 Initializing 3D scene...');

            // Dynamically import Three.js modules if not already loaded
            if (!THREE || !GLTFLoader || !OrbitControls) {
                try {
                    THREE = await import('three');
                    ({ GLTFLoader } = await import('three/addons/loaders/GLTFLoader.js'));
                    ({ OrbitControls } = await import('three/addons/controls/OrbitControls.js'));
                } catch (err) {
                    console.error('❌ Failed to load Three.js modules:', err);
                    showStatus('inferenceStatus', 'error', 'Failed to load 3D modules. Check your internet connection.');
                    return;
                }
            }

            const container = document.getElementById('sceneContainer');
            const width = container.clientWidth;
            const height = container.clientHeight;

            // Scene setup
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0x1a1a1a);

            // Camera setup
            camera = new THREE.PerspectiveCamera(60, width / height, 0.1, 100);
            camera.position.set(0, 0, 4);

            // Renderer setup
            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: false });
            renderer.setSize(width, height);
            renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
            renderer.shadowMap.enabled = true;
            renderer.shadowMap.type = THREE.PCFSoftShadowMap;

            // Remove loading indicator and add renderer
            const loading = document.getElementById('sceneLoading');
            if (loading) loading.remove();
            container.appendChild(renderer.domElement);

            // Lighting setup
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.8);
            scene.add(ambientLight);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 1.0);
            directionalLight.position.set(2, 2, 3);
            directionalLight.castShadow = true;
            scene.add(directionalLight);

            const fillLight = new THREE.DirectionalLight(0xffffff, 0.4);
            fillLight.position.set(-2, 1, 2);
            scene.add(fillLight);

            const rimLight = new THREE.DirectionalLight(0x4488ff, 0.3);
            rimLight.position.set(0, 2, -2);
            scene.add(rimLight);

            // Controls setup
            controls = new OrbitControls(camera, renderer.domElement);
            controls.target.set(0, 0, 0);
            controls.enableDamping = true;
            controls.dampingFactor = 0.05;
            controls.minDistance = 2;
            controls.maxDistance = 8;
            controls.update();

            // Load the raccoon model
            await loadRaccoonModel();

            // Start render loop
            animate3D();
        }

        // Load the raccoon model
        async function loadRaccoonModel() {
            return new Promise((resolve, reject) => {
                const loader = new GLTFLoader();
                loader.load(
                    'https://storage.googleapis.com/mediapipe-tasks/face_landmarker/raccoon_head.glb',
                    (gltf) => {
                        console.log('✅ Raccoon head model loaded');

                        // Remove any existing model
                        if (gltfScene) {
                            scene.remove(gltfScene);
                            morphTargetMeshes = [];
                        }

                        gltfScene = gltf.scene;
                        scene.add(gltfScene);

                        // Scale and position the model
                        const box = new THREE.Box3().setFromObject(gltfScene);
                        const size = box.getSize(new THREE.Vector3());
                        const center = box.getCenter(new THREE.Vector3());

                        gltfScene.position.set(0, 0, 0);
                        gltfScene.position.sub(center);

                        const maxDimension = Math.max(size.x, size.y, size.z);
                        const scale = 1.5 / maxDimension;
                        gltfScene.scale.setScalar(scale);
                        gltfScene.position.y += size.y * scale * 0.05;

                        // Find meshes with morph targets
                        gltfScene.traverse((object) => {
                            if (object.isMesh) {
                                const mesh = object;
                                mesh.frustumCulled = false;

                                if (mesh.morphTargetDictionary && mesh.morphTargetInfluences) {
                                    morphTargetMeshes.push(mesh);
                                    console.log('🦝 Added morph target mesh:', mesh.name, 'targets:', Object.keys(mesh.morphTargetDictionary).length);
                                }
                            }
                        });

                        console.log(`🦝 Total morph target meshes found: ${morphTargetMeshes.length}`);

                        modelLoaded = true;
                        resolve();
                    },
                    (progress) => {
                        const percent = (progress.loaded / progress.total) * 100;
                        console.log(`Loading model: ${percent.toFixed(1)}%`);
                    },
                    (error) => {
                        console.error('❌ Model load error:', error);
                        showStatus('inferenceStatus', 'error', 'Failed to load raccoon model');
                        reject(error);
                    }
                );
            });
        }

        // Animation loop for 3D scene
        function animate3D() {
            requestAnimationFrame(animate3D);
            if (controls) controls.update();
            if (renderer && scene && camera) renderer.render(scene, camera);
        }

        // Update raccoon with frame data
        function updateRaccoonFromFrame(frame) {
            if (!gltfScene) {
                console.warn('🦝 No GLTF model loaded yet');
                return;
            }

            // Apply head rotation and position if available
            if (frame.headPosition && frame.headRotation) {
                const quaternion = new THREE.Quaternion(
                    frame.headRotation.x,
                    frame.headRotation.y,
                    frame.headRotation.z,
                    frame.headRotation.w
                );
                gltfScene.quaternion.slerp(quaternion, 0.15);

                const position = new THREE.Vector3(
                    frame.headPosition.x * 0.5,
                    frame.headPosition.y * 0.5,
                    frame.headPosition.z * 0.5
                );
                gltfScene.position.lerp(position, 0.1);
            }

            // Apply blendshapes
            const blendshapes = processBlendshapes(frame.blendshapes);
            currentBlendshapes = blendshapes;
            updateBlendshapes(blendshapes);
            updateBlendshapesDisplay();
        }

        // Process blendshapes with enhancements
        function processBlendshapes(blendshapes) {
            const coefsMap = new Map();

            for (const [name, value] of Object.entries(blendshapes)) {
                let score = value;

                // // Enhance certain expressions
                // switch (name) {
                //     case 'eyeBlinkLeft':
                //     case 'eyeBlinkRight':
                //         score *= 1.5;
                //         break;
                //     case 'browOuterUpLeft':
                //     case 'browOuterUpRight':
                //         score *= 1.3;
                //         break;
                //     case 'mouthSmileLeft':
                //     case 'mouthSmileRight':
                //         score *= 1.2;
                //         break;
                //     default:
                //         score *= 1.0;

                // }

                coefsMap.set(name, Math.min(score, 1.0));
            }

            return coefsMap;
        }

        // Update blendshapes on meshes
        function updateBlendshapes(blendshapes) {
            for (const mesh of morphTargetMeshes) {
                if (!mesh.morphTargetDictionary || !mesh.morphTargetInfluences) continue;

                for (const [name, value] of blendshapes) {
                    if (name in mesh.morphTargetDictionary) {
                        const index = mesh.morphTargetDictionary[name];
                        if (mesh.morphTargetInfluences[index] !== undefined) {
                            mesh.morphTargetInfluences[index] = THREE.MathUtils.lerp(
                                mesh.morphTargetInfluences[index],
                                value,
                                0.3
                            );
                        }
                    }
                }
            }
        }

        // Update blendshapes display panel
        function updateBlendshapesDisplay() {
            const panel = document.getElementById('blendshapesPanel');

            if (currentBlendshapes.size === 0) {
                panel.innerHTML = `
                    <div style="text-align: center; color: #667eea; margin-top: 50px;">
                        <p>No blendshapes data</p>
                        <p style="font-size: 12px;">Run inference to see live blendshapes</p>
                    </div>
                `;
                return;
            }

            // Sort blendshapes by value descending
            const sortedBlendshapes = Array.from(currentBlendshapes.entries())
                .sort(([, a], [, b]) => b - a);

            let html = '';
            for (const [name, value] of sortedBlendshapes) {
                const percentage = (value * 100).toFixed(1);
                const barWidth = Math.max(value * 100, 2);
                const colorClass = value > 0.1 ? 'high' : value > 0.05 ? 'medium' : 'low';

                html += `
                    <div class="blendshape-item">
                        <div style="display: flex; justify-content: space-between; width: 100%;">
                            <span class="blendshape-name">${name}</span>
                            <span class="blendshape-value">${percentage}%</span>
                        </div>
                        <div class="blendshape-bar">
                            <div class="blendshape-fill ${colorClass}" style="width: ${barWidth}%"></div>
                        </div>
                    </div>
                `;
            }
            panel.innerHTML = html;
        }

        async function handleModelFile(input) {
            const file = input.files[0];
            if (!file) return;

            document.getElementById('modelFileName').textContent = file.name;
            showStatus('modelStatus', 'info', 'Loading model...');

            try {
                const arrayBuffer = await file.arrayBuffer();
                if (!window.ort) {
                    throw new Error('ONNX Runtime failed to load. Check network and CDN.');
                }
                onnxSession = await window.ort.InferenceSession.create(arrayBuffer);

                showStatus('modelStatus', 'success', `Model loaded successfully! Input: ${JSON.stringify(onnxSession.inputNames)}, Output: ${JSON.stringify(onnxSession.outputNames)}`);
                updateInferenceButton();
            } catch (error) {
                console.error('Error loading model:', error);
                showStatus('modelStatus', 'error', `Error loading model: ${error.message}`);
                onnxSession = null;
            }
        }

        // Recording Functions
        async function startRecording() {
            try {
                // Request microphone access
                recordingStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 44100,
                        channelCount: 2,
                        volume: 1.0
                    }
                });

                // Create MediaRecorder
                let mimeType = '';
                const candidates = [
                    'audio/webm;codecs=opus',
                    'audio/webm',
                    'audio/mp4;codecs=mp4a.40.2',
                    'audio/mpeg'
                ];
                for (const c of candidates) {
                    if (window.MediaRecorder && MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported(c)) {
                        mimeType = c; break;
                    }
                }
                mediaRecorder = new MediaRecorder(recordingStream, mimeType ? { mimeType } : undefined);

                audioChunks = [];
                recordingStartTime = Date.now();

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    await processRecording();
                };

                // Start recording
                mediaRecorder.start(100); // Collect data every 100ms

                // Update UI
                updateRecordingStatus('recording', 'Recording...');
                document.getElementById('startRecordBtn').disabled = true;
                document.getElementById('stopRecordBtn').disabled = false;
                document.getElementById('clearRecordBtn').disabled = true;
                document.getElementById('waveAnimation').classList.add('active');

                // Start timer
                startRecordingTimer();

            } catch (error) {
                console.error('Error accessing microphone:', error);
                updateRecordingStatus('idle', 'Error: Could not access microphone');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();

                // Stop all tracks
                if (recordingStream) {
                    recordingStream.getTracks().forEach(track => track.stop());
                }

                // Update UI
                updateRecordingStatus('ready', 'Processing...');
                document.getElementById('startRecordBtn').disabled = false;
                document.getElementById('stopRecordBtn').disabled = true;
                document.getElementById('waveAnimation').classList.remove('active');

                // Stop timer
                stopRecordingTimer();
            }
        }

        async function processRecording() {
            try {
                // Create blob from chunks
                const webmBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });

                // Convert to WAV
                const wavBlob = await convertToWav(webmBlob);

                // Create audio URL for preview
                if (currentAudioObjectUrl) {
                    URL.revokeObjectURL(currentAudioObjectUrl);
                    currentAudioObjectUrl = null;
                }
                const audioUrl = URL.createObjectURL(wavBlob);
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                document.getElementById('audioPreview').style.display = 'block';

                // Load and decode audio for inference
                const arrayBuffer = await wavBlob.arrayBuffer();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                // Setup download link
                latestAudioBlob = wavBlob;
                latestAudioFilename = `recording_${Date.now()}.wav`;
                currentAudioObjectUrl = audioUrl;
                const dl = document.getElementById('downloadAudioLink');
                dl.href = audioUrl;
                dl.download = latestAudioFilename;
                dl.style.display = 'inline-block';

                // Update UI
                updateRecordingStatus('ready', 'Recording ready!');
                document.getElementById('clearRecordBtn').disabled = false;

                // Show audio info
                const duration = audioBuffer.duration.toFixed(2);
                const sampleRate = audioBuffer.sampleRate;
                document.getElementById('audioInfo').innerHTML = `Duration: ${duration}s | Sample Rate: ${sampleRate}Hz | Source: Recording`;

                updateInferenceButton();

            } catch (error) {
                console.error('Error processing recording:', error);
                updateRecordingStatus('idle', 'Error processing recording');
            }
        }

        function clearRecording() {
            // Reset audio players
            const audioPlayer = document.getElementById('audioPlayer');
            const audioPlayerPreview = document.getElementById('audioPlayer');

            if (audioPlayer && audioPlayer.src) {
                URL.revokeObjectURL(audioPlayer.src);
                audioPlayer.src = '';
            }

            document.getElementById('audioPreview').style.display = 'none';
            const dl = document.getElementById('downloadAudioLink');
            dl.style.display = 'none';
            dl.href = '#';
            dl.removeAttribute('download');
            if (currentAudioObjectUrl) {
                URL.revokeObjectURL(currentAudioObjectUrl);
                currentAudioObjectUrl = null;
            }

            // Reset audio buffer
            audioBuffer = null;
            inferenceResults = null;
            latestAudioBlob = null;
            latestAudioFilename = null;

            // Reset UI
            updateRecordingStatus('idle', 'Ready to record');
            document.getElementById('clearRecordBtn').disabled = true;
            document.getElementById('recordingTimer').textContent = '00:00';
            document.getElementById('audioInfo').innerHTML = '';

            // Hide and reset 3D visualization
            const visualizationSection = document.getElementById('visualizationSection');
            const resultsSection = document.getElementById('resultsSection');

            visualizationSection.style.display = 'none';
            resultsSection.style.display = 'none';

            // Reset 3D scene state
            resetVisualizationState();

            updateInferenceButton();
        }

        function resetVisualizationState() {
            // Stop any ongoing playback
            if (isPlaying) {
                pauseVisualizationPlayback();
            }

            // Reset playback state
            currentFrame = 0;
            isPlaying = false;
            window.playbackData = null;

            // Reset blendshapes
            currentBlendshapes.clear();

            // Reset raccoon position if model is loaded
            if (gltfScene) {
                gltfScene.position.set(0, 0, 0);
                gltfScene.rotation.set(0, 0, 0);

                // Reset all morph targets to 0
                for (const mesh of morphTargetMeshes) {
                    if (mesh.morphTargetInfluences) {
                        for (let i = 0; i < mesh.morphTargetInfluences.length; i++) {
                            mesh.morphTargetInfluences[i] = 0;
                        }
                    }
                }
            }

            // Clear blendshapes panel
            const panel = document.getElementById('blendshapesPanel');
            if (panel) {
                panel.innerHTML = `
                    <div style="text-align: center; color: #667eea; margin-top: 50px;">
                        <p>No inference results</p>
                        <p style="font-size: 12px;">Run inference to see blendshapes</p>
                    </div>
                `;
            }

            console.log('🦝 Visualization state reset');
        }

        function startRecordingTimer() {
            recordingTimerInterval = setInterval(() => {
                const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
                const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0');
                const seconds = (elapsed % 60).toString().padStart(2, '0');
                document.getElementById('recordingTimer').textContent = `${minutes}:${seconds}`;
            }, 1000);
        }

        function stopRecordingTimer() {
            if (recordingTimerInterval) {
                clearInterval(recordingTimerInterval);
                recordingTimerInterval = null;
            }
        }

        function updateRecordingStatus(className, text) {
            const status = document.getElementById('recordingStatus');
            status.className = `status ${className}`;
            status.textContent = text;
        }

        async function convertToWav(webmBlob) {
            // Create audio context
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();

            // Convert blob to array buffer
            const arrayBuffer = await webmBlob.arrayBuffer();

            // Decode audio data
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

            // Convert to WAV
            const wavBuffer = audioBufferToWav(audioBuffer);

            return new Blob([wavBuffer], { type: 'audio/wav' });
        }

        function audioBufferToWav(buffer) {
            const length = buffer.length;
            const numberOfChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const arrayBuffer = new ArrayBuffer(44 + length * numberOfChannels * 2);
            const view = new DataView(arrayBuffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + length * numberOfChannels * 2, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numberOfChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numberOfChannels * 2, true);
            view.setUint16(32, numberOfChannels * 2, true);
            view.setUint16(34, 16, true);
            writeString(36, 'data');
            view.setUint32(40, length * numberOfChannels * 2, true);

            // Convert float samples to 16-bit PCM
            let offset = 44;
            for (let i = 0; i < length; i++) {
                for (let channel = 0; channel < numberOfChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
                    view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                    offset += 2;
                }
            }

            return arrayBuffer;
        }

        async function handleAudioFile(input) {
            const file = input.files[0];
            if (!file) return;

            document.getElementById('audioFileName').textContent = file.name;

            try {
                // Create audio URL for preview
                if (currentAudioObjectUrl) {
                    URL.revokeObjectURL(currentAudioObjectUrl);
                    currentAudioObjectUrl = null;
                }
                const audioUrl = URL.createObjectURL(file);
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                document.getElementById('audioPreview').style.display = 'block';

                // Load and decode audio
                const arrayBuffer = await file.arrayBuffer();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                // Show audio info
                const duration = audioBuffer.duration.toFixed(2);
                const sampleRate = audioBuffer.sampleRate;
                document.getElementById('audioInfo').innerHTML = `Duration: ${duration}s | Sample Rate: ${sampleRate}Hz | Source: Upload`;

                // Setup download link
                latestAudioBlob = file;
                latestAudioFilename = file.name || `audio_${Date.now()}`;
                currentAudioObjectUrl = audioUrl;
                const dl = document.getElementById('downloadAudioLink');
                dl.href = audioUrl;
                dl.download = latestAudioFilename;
                dl.style.display = 'inline-block';

                // Clear recording status
                updateRecordingStatus('idle', 'Ready to record');
                document.getElementById('clearRecordBtn').disabled = true;
                document.getElementById('recordingTimer').textContent = '00:00';

                updateInferenceButton();
            } catch (error) {
                console.error('Error loading audio:', error);
                showStatus('inferenceStatus', 'error', `Error loading audio: ${error.message}`);
                audioBuffer = null;
            }
        }

        function updateInferenceButton() {
            const runButton = document.getElementById('runInference');
            runButton.disabled = !(onnxSession && audioBuffer);
        }

        async function runInference() {
            if (!onnxSession || !audioBuffer) {
                showStatus('inferenceStatus', 'error', 'Please load both model and audio file');
                return;
            }

            const runButton = document.getElementById('runInference');
            runButton.disabled = true;
            runButton.innerHTML = '<span class="spinner"></span>Running inference...';

            showStatus('inferenceStatus', 'info', 'Processing audio...');
            document.getElementById('progressBar').style.display = 'block';

            try {
                // Extract mel-spectrogram features
                updateProgress(20);
                showStatus('inferenceStatus', 'info', 'Extracting mel-spectrogram features...');

                const melFeatures = await extractMelFeatures(audioBuffer);

                updateProgress(40);
                showStatus('inferenceStatus', 'info', 'Running ONNX inference...');

                // Run ONNX inference
                if (!window.ort) throw new Error('ONNX runtime not available');
                const inputTensor = new window.ort.Tensor('float32', melFeatures.data, [1, melFeatures.frames, audioConfig.nMels]);
                const feeds = {};
                feeds[onnxSession.inputNames[0]] = inputTensor;

                const results = await onnxSession.run(feeds);
                const outputData = results[onnxSession.outputNames[0]].data;

                updateProgress(80);
                showStatus('inferenceStatus', 'info', 'Processing results...');

                // Process results
                const targetFps = parseInt(document.getElementById('targetFps').value);
                inferenceResults = processInferenceResults(outputData, melFeatures.frames, targetFps, melFeatures.originalDuration);

                updateProgress(100);
                showStatus('inferenceStatus', 'success', `Inference completed! Generated ${inferenceResults.frameCount} frames`);

                // Show results
                displayResults(inferenceResults);

            } catch (error) {
                console.error('Inference error:', error);
                showStatus('inferenceStatus', 'error', `Inference failed: ${error.message}`);
            } finally {
                runButton.disabled = false;
                runButton.innerHTML = 'Run Inference';
                setTimeout(() => {
                    document.getElementById('progressBar').style.display = 'none';
                }, 2000);
            }
        }

        // ---- Librosa-like preprocessing (matches sr=16k, n_fft=512, hop=160, win=400) ----
        const melCfg = {
            sr: 16000,
            nFFT: 512,
            hop: 160,
            win: 400,
            nMels: 80,
            fmin: 0,
            fmax: 8000,           // sr/2
            power: 2.0,           // power spectrogram (|stft|^2)
            amin: 1e-10,
            top_db: null,         // set if you used librosa.power_to_db(top_db=...)
            norm: "slaney",       // librosa default
            htk: false            // librosa default
        };

        // If you saved dataset stats during training, put them here (length = nMels)
        // Example: const FEATURE_MEAN = [...80 floats...]; const FEATURE_STD = [...80 floats...];
        const FEATURE_MEAN = null; // <- fill with training stats for best parity
        const FEATURE_STD = null; // <- fill with training stats for best parity

        function hzToMel(hz, htk = false) {
            if (htk) return 2595 * Math.log10(1 + hz / 700);
            const f = hz / 700;
            return 1127.01048 * Math.log(1 + f);
        }
        function melToHz(mel, htk = false) {
            if (htk) return 700 * (Math.pow(10, mel / 2595) - 1);
            return 700 * (Math.expm1(mel / 1127.01048));
        }

        function buildHann(win) {
            const w = new Float32Array(win);
            for (let i = 0; i < win; i++) { w[i] = 0.5 - 0.5 * Math.cos(2 * Math.PI * i / (win - 1)); }
            return w;
        }

        function buildMelFilterBank(sr, nFFT, nMels, fmin, fmax, { norm = "slaney", htk = false } = {}) {
            const nFreqs = Math.floor(nFFT / 2) + 1;
            const melMin = hzToMel(fmin, htk), melMax = hzToMel(fmax, htk);
            const mels = new Float32Array(nMels + 2);
            for (let i = 0; i < mels.length; i++) {
                mels[i] = melMin + (i * (melMax - melMin) / (nMels + 1));
            }
            const hz = new Float32Array(mels.length);
            for (let i = 0; i < hz.length; i++) hz[i] = melToHz(mels[i], htk);

            const fftFreqs = new Float32Array(nFreqs);
            for (let i = 0; i < nFreqs; i++) fftFreqs[i] = (sr * i) / nFFT;

            // Triangular filters
            const fb = Array.from({ length: nMels }, () => new Float32Array(nFreqs));
            for (let m = 0; m < nMels; m++) {
                const f_m_l = hz[m], f_m = hz[m + 1], f_m_h = hz[m + 2];
                for (let i = 0; i < nFreqs; i++) {
                    const f = fftFreqs[i];
                    let w = 0;
                    if (f >= f_m_l && f <= f_m) {
                        w = (f - f_m_l) / (f_m - f_m_l);
                    } else if (f > f_m && f <= f_m_h) {
                        w = (f_m_h - f) / (f_m_h - f_m);
                    }
                    fb[m][i] = Math.max(0, w);
                }
            }

            // Slaney area normalization
            if (norm === "slaney") {
                for (let m = 0; m < nMels; m++) {
                    let area = 0;
                    for (let i = 0; i < nFreqs; i++) area += fb[m][i];
                    if (area > 0) {
                        const scale = 2.0 / (fmax - fmin); // librosa approx
                        for (let i = 0; i < nFreqs; i++) fb[m][i] *= scale;
                    }
                }
            }
            return fb;
        }

        function linearResample(x, srcSr, dstSr) {
            if (srcSr === dstSr) return x.slice();
            const duration = x.length / srcSr;
            const nDst = Math.round(duration * dstSr);
            const y = new Float32Array(nDst);
            const ratio = (x.length - 1) / (nDst - 1);
            for (let i = 0; i < nDst; i++) {
                const t = i * ratio;
                const i0 = Math.floor(t), i1 = Math.min(i0 + 1, x.length - 1);
                const frac = t - i0;
                y[i] = (1 - frac) * x[i0] + frac * x[i1];
            }
            return y;
        }

        function stftPower(signal, sr, { nFFT, hop, win }, hann) {
            const FFTCtor = getFFTClass();
            if (!FFTCtor) {
                throw new Error('FFT library is not available. Please ensure fft.js is loaded.');
            }
            const nFreqs = Math.floor(nFFT / 2) + 1;
            const nFrames = Math.max(0, Math.floor((signal.length - win) / hop) + 1);
            const out = new Float32Array(nFrames * nFreqs);
            const f = new FFTCtor(nFFT);
            const complex = f.createComplexArray();
            const frameSig = new Float32Array(nFFT);

            for (let frame = 0; frame < nFrames; frame++) {
                const start = frame * hop;
                // windowing and zero-pad
                for (let i = 0; i < nFFT; i++) frameSig[i] = 0;
                for (let i = 0; i < win; i++) {
                    frameSig[i] = (signal[start + i] || 0) * hann[i];
                }
                // real FFT
                f.realTransform(complex, frameSig);
                if (typeof f.completeSpectrum === 'function') {
                    f.completeSpectrum(complex);
                }
                // power spectrum |X|^2 for positive frequencies
                for (let k = 0; k < nFreqs; k++) {
                    const rr = complex[2 * k];
                    const ii = complex[2 * k + 1];
                    out[frame * nFreqs + k] = rr * rr + ii * ii;
                }
            }
            return { data: out, nFrames, nFreqs };
        }

        function applyMel(fbank, powerSpec, nFrames, nFreqs, nMels) {
            const out = new Float32Array(nFrames * nMels);
            for (let t = 0; t < nFrames; t++) {
                const base = t * nFreqs;
                for (let m = 0; m < nMels; m++) {
                    let e = 0;
                    const filt = fbank[m];
                    for (let k = 0; k < nFreqs; k++) {
                        e += filt[k] * powerSpec[base + k];
                    }
                    out[t * nMels + m] = e;
                }
            }
            return out;
        }

        function powerToLogMel(mel, amin = 1e-10) {
            // log10 like librosa.power_to_db (offset handled upstream)
            const y = new Float32Array(mel.length);
            for (let i = 0; i < mel.length; i++) {
                y[i] = Math.log10(Math.max(mel[i], amin));
            }
            return y;
        }

        function normalizePerBin(melFrames, nFrames, nMels, mean = null, std = null) {
            // melFrames is [nFrames*nMels] row-major (t*m)
            const out = new Float32Array(melFrames.length);
            if (mean && std) {
                for (let t = 0; t < nFrames; t++) {
                    for (let m = 0; m < nMels; m++) {
                        const v = melFrames[t * nMels + m];
                        out[t * nMels + m] = (v - mean[m]) / (std[m] || 1e-6);
                    }
                }
            } else {
                // fallback: compute per-bin stats over time (closer to training than global z-score)
                const mu = new Float32Array(nMels);
                const sig = new Float32Array(nMels);
                // mean
                for (let m = 0; m < nMels; m++) {
                    let s = 0; for (let t = 0; t < nFrames; t++) s += melFrames[t * nMels + m];
                    mu[m] = s / nFrames;
                }
                // std
                for (let m = 0; m < nMels; m++) {
                    let s = 0; for (let t = 0; t < nFrames; t++) { const d = melFrames[t * nMels + m] - mu[m]; s += d * d; }
                    sig[m] = Math.sqrt(s / Math.max(1, nFrames - 1)) + 1e-6;
                }
                for (let t = 0; t < nFrames; t++) {
                    for (let m = 0; m < nMels; m++) {
                        const v = melFrames[t * nMels + m];
                        out[t * nMels + m] = (v - mu[m]) / sig[m];
                    }
                }
            }
            return out;
        }

        async function extractMelFeatures(audioBuffer) {
            // 1) mono + resample -> 16 kHz
            const ch0 = audioBuffer.getChannelData(0);
            const mono = ch0; // (you can average channels if stereo input)
            const srIn = audioBuffer.sampleRate;
            const x = linearResample(mono, srIn, melCfg.sr);

            // 2) STFT power
            const hann = buildHann(melCfg.win);
            const { data: P, nFrames, nFreqs } = stftPower(x, melCfg.sr, melCfg, hann); // [nFrames, nFreqs]

            // 3) Mel filter bank
            const fb = buildMelFilterBank(melCfg.sr, melCfg.nFFT, melCfg.nMels, melCfg.fmin, melCfg.fmax, { norm: melCfg.norm, htk: melCfg.htk });
            const mel = applyMel(fb, P, nFrames, nFreqs, melCfg.nMels); // [nFrames, nMels]

            // 4) Log compression (log10)
            const logmel = powerToLogMel(mel, melCfg.amin);

            // 5) Normalize per mel bin (prefer dataset mean/std if available)
            const norm = normalizePerBin(logmel, nFrames, melCfg.nMels, FEATURE_MEAN, FEATURE_STD);

            return {
                data: norm,               // flattened [nFrames*nMels]
                frames: nFrames,
                originalDuration: audioBuffer.duration
            };
        }


        // async function extractMelFeatures(audioBuffer) {
        //     // Resample audio to target sample rate
        //     const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        //     const targetSampleRate = audioConfig.sampleRate;

        //     let audioData;
        //     if (audioBuffer.sampleRate !== targetSampleRate) {
        //         // Simple resampling - in production, use a proper resampling library
        //         const ratio = audioBuffer.sampleRate / targetSampleRate;
        //         const newLength = Math.floor(audioBuffer.length / ratio);
        //         audioData = new Float32Array(newLength);

        //         for (let i = 0; i < newLength; i++) {
        //             const srcIndex = Math.floor(i * ratio);
        //             audioData[i] = audioBuffer.getChannelData(0)[srcIndex];
        //         }
        //     } else {
        //         audioData = audioBuffer.getChannelData(0);
        //     }

        //     // Extract mel-spectrogram using a simplified approach
        //     // Note: This is a basic implementation. For production, use a proper audio processing library
        //     const melFeatures = await computeMelSpectrogram(audioData, targetSampleRate);

        //     return {
        //         data: melFeatures,
        //         frames: Math.floor(melFeatures.length / audioConfig.nMels),
        //         originalDuration: audioBuffer.duration
        //     };
        // }

        // async function computeMelSpectrogram(audioData, sampleRate) {
        //     // This is a simplified mel-spectrogram computation
        //     // In a real implementation, you'd use a proper audio processing library

        //     const hopLength = audioConfig.hopLength;
        //     const winLength = audioConfig.winLength;
        //     const nFFT = audioConfig.nFFT;
        //     const nMels = audioConfig.nMels;

        //     const numFrames = Math.floor((audioData.length - winLength) / hopLength) + 1;
        //     const melFeatures = new Float32Array(numFrames * nMels);

        //     // Create a simple Hanning window
        //     const window = new Float32Array(winLength);
        //     for (let i = 0; i < winLength; i++) {
        //         window[i] = 0.5 - 0.5 * Math.cos(2 * Math.PI * i / (winLength - 1));
        //     }

        //     // Process each frame
        //     for (let frame = 0; frame < numFrames; frame++) {
        //         const startIdx = frame * hopLength;
        //         const frameData = new Float32Array(nFFT);

        //         // Apply window and zero-pad
        //         for (let i = 0; i < winLength && startIdx + i < audioData.length; i++) {
        //             frameData[i] = audioData[startIdx + i] * window[i];
        //         }

        //         // Compute FFT (simplified - using a basic magnitude spectrum)
        //         const spectrum = computeSpectrum(frameData);

        //         // Convert to mel scale (simplified)
        //         const melSpectrum = convertToMelScale(spectrum, sampleRate, nMels);

        //         // Normalize and convert to log scale
        //         for (let i = 0; i < nMels; i++) {
        //             const logMel = Math.log(Math.max(melSpectrum[i], 1e-10));
        //             melFeatures[frame * nMels + i] = logMel;
        //         }
        //     }

        //     // Normalize features
        //     const mean = melFeatures.reduce((sum, val) => sum + val, 0) / melFeatures.length;
        //     const variance = melFeatures.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / melFeatures.length;
        //     const std = Math.sqrt(variance) + 1e-6;

        //     for (let i = 0; i < melFeatures.length; i++) {
        //         melFeatures[i] = (melFeatures[i] - mean) / std;
        //     }

        //     return melFeatures;
        // }

        // function computeSpectrum(frameData) {
        //     // Simplified spectrum computation (magnitude only)
        //     const spectrum = new Float32Array(frameData.length / 2);

        //     for (let i = 0; i < spectrum.length; i++) {
        //         const real = frameData[i * 2] || 0;
        //         const imag = frameData[i * 2 + 1] || 0;
        //         spectrum[i] = Math.sqrt(real * real + imag * imag);
        //     }

        //     return spectrum;
        // }

        // function convertToMelScale(spectrum, sampleRate, nMels) {
        //     // Simplified mel scale conversion
        //     const melSpectrum = new Float32Array(nMels);
        //     const nyquist = sampleRate / 2;

        //     for (let i = 0; i < nMels; i++) {
        //         const melFreq = (i / (nMels - 1)) * 2595 * Math.log10(1 + nyquist / 700);
        //         const freq = 700 * (Math.pow(10, melFreq / 2595) - 1);
        //         const binIndex = Math.floor(freq * spectrum.length / nyquist);

        //         melSpectrum[i] = spectrum[Math.min(binIndex, spectrum.length - 1)] || 0;
        //     }

        //     return melSpectrum;
        // }

        function processInferenceResults(outputData, numFrames, targetFps, originalDuration) {
            // The output should be [batch, sequence, 59] flattened
            const outputFrames = numFrames;
            const outputFeatures = 59; // 52 blendshapes + 7 head pose

            // Downsample to target FPS if needed
            const originalFps = numFrames / originalDuration;
            let finalFrames = outputFrames;
            let finalData = outputData;

            if (targetFps < originalFps) {
                const downsampleRatio = originalFps / targetFps;
                finalFrames = Math.floor(outputFrames / downsampleRatio);
                finalData = new Float32Array(finalFrames * outputFeatures);

                for (let i = 0; i < finalFrames; i++) {
                    const srcIndex = Math.floor(i * downsampleRatio);
                    for (let j = 0; j < outputFeatures; j++) {
                        finalData[i * outputFeatures + j] = outputData[srcIndex * outputFeatures + j];
                    }
                }
            }

            // Create frame data
            const frames = [];
            for (let i = 0; i < finalFrames; i++) {
                const blendshapes = {};
                for (let j = 0; j < 52; j++) {
                    blendshapes[blendshapeNames[j]] = finalData[i * outputFeatures + j];
                }

                const frame = {
                    frame_index: i,
                    timestamp: Math.round((i / targetFps) * 1000), // milliseconds
                    blendshapes: blendshapes,
                    headPosition: {
                        x: finalData[i * outputFeatures + 52],
                        y: finalData[i * outputFeatures + 53],
                        z: finalData[i * outputFeatures + 54]
                    },
                    headRotation: {
                        w: finalData[i * outputFeatures + 55],
                        x: finalData[i * outputFeatures + 56],
                        y: finalData[i * outputFeatures + 57],
                        z: finalData[i * outputFeatures + 58]
                    },
                    has_face: true
                };
                frames.push(frame);
            }

            return {
                sessionInfo: {
                    sessionId: `web_inference_${Date.now()}`,
                    targetFPS: targetFps,
                    audioPath: "uploaded_audio"
                },
                frameCount: finalFrames,
                failedFrames: 0,
                failureRate: 0.0,
                duration: finalFrames / targetFps,
                frames: frames
            };
        }

        function displayResults(results) {
            const resultsSection = document.getElementById('resultsSection');
            const resultsInfo = document.getElementById('resultsInfo');
            const visualizationSection = document.getElementById('visualizationSection');

            resultsInfo.innerHTML = `
                <div class="info-card">
                    <div class="value">${results.frameCount}</div>
                    <div class="label">Total Frames</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.sessionInfo.targetFPS}</div>
                    <div class="label">FPS</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.duration.toFixed(2)}s</div>
                    <div class="label">Duration</div>
                </div>
                <div class="info-card">
                    <div class="value">${results.failureRate.toFixed(1)}%</div>
                    <div class="label">Failure Rate</div>
                </div>
            `;

            // Setup download links
            setupDownloads(results);

            // Show results section
            resultsSection.style.display = 'block';

            // Initialize and show 3D visualization
            visualizationSection.style.display = 'block';

            // Only initialize scene if not already initialized
            if (!modelLoaded || !scene) {
                console.log('🦝 Initializing 3D scene for the first time...');
                initializeScene().then(() => {
                    setupPlaybackControls(results);

                    // Show the first frame
                    if (results.frames.length > 0) {
                        updateRaccoonFromFrame(results.frames[0]);
                    }
                });
            } else {
                console.log('🦝 Using existing 3D scene...');
                // Scene already exists, just setup playback with new data
                resetVisualizationState(); // Reset to clean state
                setupPlaybackControls(results);

                // Show the first frame
                if (results.frames.length > 0) {
                    updateRaccoonFromFrame(results.frames[0]);
                }
            }
        }

        // Setup playback controls for the inference results
        function setupPlaybackControls(results) {
            const playbackControls = document.getElementById('playbackControls');
            const frameScrubber = document.getElementById('frameScrubber');
            const frameSlider = document.getElementById('frameSlider');
            const frameLabel = document.getElementById('frameLabel');
            const audioPlayerViz = document.getElementById('audioPlayerViz');

            // Store results globally for playback
            window.playbackData = results;
            targetFPS = results.sessionInfo.targetFPS;

            // Setup frame slider
            frameSlider.max = results.frameCount - 1;
            frameSlider.value = 0;
            frameLabel.textContent = `Frame: 1 / ${results.frameCount}`;

            // Setup audio if available
            if (audioBuffer) {
                const previewPlayer = document.getElementById('audioPlayer');
                if (previewPlayer && previewPlayer.src) {
                    audioPlayerViz.src = previewPlayer.src;
                    audioPlayerViz.style.display = 'block';
                }
            }

            // Show controls
            playbackControls.style.display = 'flex';
            frameScrubber.style.display = 'block';

            // Add event listeners
            setupPlaybackEventListeners();
        }

        // Setup event listeners for playback controls
        function setupPlaybackEventListeners() {
            const playBtn = document.getElementById('playBtn');
            const pauseBtn = document.getElementById('pauseBtn');
            const stopBtn = document.getElementById('stopBtn');
            const resetBtn = document.getElementById('resetBtn');
            const frameSlider = document.getElementById('frameSlider');

            // Remove existing listeners
            playBtn.replaceWith(playBtn.cloneNode(true));
            pauseBtn.replaceWith(pauseBtn.cloneNode(true));
            stopBtn.replaceWith(stopBtn.cloneNode(true));
            resetBtn.replaceWith(resetBtn.cloneNode(true));
            frameSlider.replaceWith(frameSlider.cloneNode(true));

            // Get fresh references
            const newPlayBtn = document.getElementById('playBtn');
            const newPauseBtn = document.getElementById('pauseBtn');
            const newStopBtn = document.getElementById('stopBtn');
            const newResetBtn = document.getElementById('resetBtn');
            const newFrameSlider = document.getElementById('frameSlider');

            newPlayBtn.addEventListener('click', startVisualizationPlayback);
            newPauseBtn.addEventListener('click', pauseVisualizationPlayback);
            newStopBtn.addEventListener('click', stopVisualizationPlayback);
            newResetBtn.addEventListener('click', resetRaccoonPosition);
            newFrameSlider.addEventListener('input', (e) => {
                seekToFrame(parseInt(e.target.value));
            });
        }

        // Playback control functions
        function startVisualizationPlayback() {
            if (!window.playbackData || isPlaying) return;

            console.log('🎬 Starting visualization playback');
            isPlaying = true;
            startTime = performance.now() - (currentFrame / targetFPS * 1000);

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz.src) {
                audioPlayerViz.currentTime = currentFrame / targetFPS;
                audioPlayerViz.play().catch(error => console.warn('Audio playback failed:', error));
            }

            visualizationAnimate();
            updatePlaybackButtons();
        }

        function pauseVisualizationPlayback() {
            isPlaying = false;
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz) {
                audioPlayerViz.pause();
            }

            updatePlaybackButtons();
        }

        function stopVisualizationPlayback() {
            pauseVisualizationPlayback();
            currentFrame = 0;

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz) {
                audioPlayerViz.pause();
                audioPlayerViz.currentTime = 0;
            }

            if (window.playbackData && window.playbackData.frames.length > 0) {
                updateRaccoonFromFrame(window.playbackData.frames[0]);
                updateFrameDisplay();
            }
        }

        function seekToFrame(frameIndex) {
            if (!window.playbackData) return;

            currentFrame = Math.max(0, Math.min(frameIndex, window.playbackData.frameCount - 1));
            const frame = window.playbackData.frames[currentFrame];
            updateRaccoonFromFrame(frame);
            updateFrameDisplay();

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            if (audioPlayerViz && audioPlayerViz.src) {
                const frameTime = currentFrame / targetFPS;
                audioPlayerViz.currentTime = frameTime;
            }
        }

        function resetRaccoonPosition() {
            if (gltfScene) {
                gltfScene.position.set(0, 0, 0);
                gltfScene.rotation.set(0, 0, 0);
                console.log('🦝 Raccoon position reset');
            }
        }

        // Animation loop for playback
        function visualizationAnimate() {
            if (!isPlaying) return;

            const audioPlayerViz = document.getElementById('audioPlayerViz');
            let elapsedTime;

            if (audioPlayerViz && audioPlayerViz.src && !audioPlayerViz.paused) {
                elapsedTime = audioPlayerViz.currentTime;
            } else {
                const currentTime = performance.now();
                elapsedTime = (currentTime - startTime) / 1000;
            }

            const frameInterval = 1 / targetFPS;
            const expectedFrameIndex = Math.floor(elapsedTime / frameInterval);

            if (expectedFrameIndex >= window.playbackData.frameCount) {
                // Playback complete
                pauseVisualizationPlayback();
                return;
            }

            if (expectedFrameIndex !== currentFrame && expectedFrameIndex < window.playbackData.frameCount) {
                currentFrame = expectedFrameIndex;
                const frame = window.playbackData.frames[currentFrame];
                updateRaccoonFromFrame(frame);
                updateFrameDisplay();
            }

            animationId = requestAnimationFrame(visualizationAnimate);
        }

        // Update frame display
        function updateFrameDisplay() {
            if (window.playbackData) {
                const frameLabel = document.getElementById('frameLabel');
                const frameSlider = document.getElementById('frameSlider');

                frameLabel.textContent = `Frame: ${currentFrame + 1} / ${window.playbackData.frameCount}`;
                frameSlider.value = currentFrame;
            }
        }

        // Update playback button states
        function updatePlaybackButtons() {
            const playBtn = document.getElementById('playBtn');
            const pauseBtn = document.getElementById('pauseBtn');
            const stopBtn = document.getElementById('stopBtn');
            const frameSlider = document.getElementById('frameSlider');

            if (playBtn) playBtn.disabled = !window.playbackData || !modelLoaded || isPlaying;
            if (pauseBtn) pauseBtn.disabled = !isPlaying;
            if (stopBtn) stopBtn.disabled = !window.playbackData;
            if (frameSlider) frameSlider.disabled = isPlaying;
        }

        function setupDownloads(results) {
            const downloadJson = document.getElementById('downloadJson');
            const downloadCsv = document.getElementById('downloadCsv');

            // JSON download
            const jsonBlob = new Blob([JSON.stringify(results, null, 2)], { type: 'application/json' });
            const jsonUrl = URL.createObjectURL(jsonBlob);
            downloadJson.href = jsonUrl;
            downloadJson.download = `inference_results_${Date.now()}.json`;
            downloadJson.style.display = 'inline-block';

            // CSV download
            const csvContent = generateCSV(results);
            const csvBlob = new Blob([csvContent], { type: 'text/csv' });
            const csvUrl = URL.createObjectURL(csvBlob);
            downloadCsv.href = csvUrl;
            downloadCsv.download = `inference_results_${Date.now()}.csv`;
            downloadCsv.style.display = 'inline-block';
        }

        function generateCSV(results) {
            const headers = ['frame_index', 'timestamp', ...blendshapeNames, 'head_pos_x', 'head_pos_y', 'head_pos_z', 'head_rot_w', 'head_rot_x', 'head_rot_y', 'head_rot_z'];
            let csv = headers.join(',') + '\n';

            results.frames.forEach(frame => {
                const row = [
                    frame.frame_index,
                    frame.timestamp,
                    ...blendshapeNames.map(name => frame.blendshapes[name]),
                    frame.headPosition.x,
                    frame.headPosition.y,
                    frame.headPosition.z,
                    frame.headRotation.w,
                    frame.headRotation.x,
                    frame.headRotation.y,
                    frame.headRotation.z
                ];
                csv += row.join(',') + '\n';
            });

            return csv;
        }

        function showStatus(elementId, type, message) {
            const element = document.getElementById(elementId);
            element.className = `status ${type}`;
            element.textContent = message;
            element.style.display = 'block';
        }

        function updateProgress(percentage) {
            const progressFill = document.getElementById('progressFill');
            progressFill.style.width = percentage + '%';
        }

        // Handle window resize for 3D scene
        window.addEventListener('resize', function () {
            if (renderer && camera) {
                const container = document.getElementById('sceneContainer');
                const width = container.clientWidth;
                const height = container.clientHeight;

                camera.aspect = width / height;
                camera.updateProjectionMatrix();
                renderer.setSize(width, height);
            }
        });

        // Initialize ONNX runtime
        if (window.ort && window.ort.env && window.ort.env.wasm) {
            window.ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/';
        }

        // Make functions globally accessible for HTML onclick handlers
        window.startRecording = startRecording;
        window.stopRecording = stopRecording;
        window.clearRecording = clearRecording;
        window.handleModelFile = handleModelFile;
        window.handleAudioFile = handleAudioFile;
        window.runInference = runInference;
    </script>
</body>

</html>